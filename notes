A graph is a digraph of transformations, each of which can have multiple inputs and outputs
and is polymorphic.

The inputs and outputs can be of a number of different types, and these form a hierarchy:
for example, an image filter might produce RGB888 images and some other transformation might
accept any kind of image.

The nodes in the graph are transformations, not data. The glyph for a transformation should
contain its name and type data.

Because of how Python does class attributes, I'm going to differentiate between transformations
and their type with singletons. Each type of transformation has a singleton object and a
factory for generating that transformation. The polymorphism takes place in the transformation
types, the transformations are of a single type with a reference to the singleton (this is 
also how I did it with Stumpy, and how Angort types work).

transformation type object attributes:
    name
    input connection names and types (tuple of (name,type))
    output connection names and types
    others
    
transformation attributes:
    input connections (i.e. references to outputs of transformations)
    output connections (i.e. references to inputs of transformations) (yes, we do it both ways)
            (connections are a tuple of (object,index) )
    products generated by the transformation, one for each output, of the appropriate type
    private data    

Type management is hairy, but really we only care if a datum is an image or not. So if we just
have an "is image" test, then we're fine. So let's say types are just names, and start "img" if they
are images. But try to keep this tight so we can refine it later if we need to.

What about drawing? To draw things, the graph has to be converted into a Qt scene graph.
Probably the most straightforward way to do this is to have a function just do it statically when
required, rather than somehow keeping two graph structures in sync.

Macros:

Macros are stored as a graph inside the .json file, where each macro
acts as a "prototype" - the main graph is stored under GRAPH, macros
are stored in a MACROS entry containing a dict of name->graph.

When a macro is instantiated, a new graph is created from the nodes
in the prototype (the entire graph is copied). A link to each macro
instance is added to the original macro prototype.
The reason is that each macro instance needs its own set of nodes
to ensure that multiple instances of a macro in a graph don't interfere
with each other.

When the macro instance graph runs, its IN components read their inputs
and copy them to their outputs. The graph is run. The OUT components read
their inputs and copy them to their outputs, which constitute the outputs
of the macro. There is some internal wiring handling the links between
the macro node's inputs and outputs and the internal IN/OUT components.

When a user opens a macro, they are viewing the prototype. Any change to
the prototype graph should be copied to all instance graphs.


A "sink" node in a macro will be displayed in the macro's canvas.

Macros can be duplicated from the macro editor, creating another macro
prototype.

When a node is edited the current "perform()" should be "changed()."
In a "normal" graph, this will cause the current perform action. In
a macro, it will cause the prototype to look up the corresponding
node in all the instances, and will run perform on them. This should
feed the outputs. It may then have to run perform() on the macro node,
to ensure the nodes downstream of the instance also run.

Need to find a way to avoid recursive macros, and to prevent
copy/paste of in/out nodes into "normal" graphs.


So:
    perform() in Tab and subclasses -> changed()
    perform() in XFormGraph -> changed()
    
    
Saving and loading:
    Macros can be saved in a library
    Macros can be imported from another file
    So I suppose a macro library is just another file with no main graph?

New imagecubes:
   Change pancamimages images to hold filter descriptors in an array
    and not a set; order is important.

Notes on channel mappings:
Many nodes need to view their output images, which may have any number of channels. Thus a node needs a way of mapping
the output image(s) onto RGB in a way that the user can modify. To this end, we have the ChannelMapping class which
essentially encapsulates a triple of indices (red,green,blue) into the source image channel. ImageCube.rgb() takes
this mapping. The mappings initialise in a null state. When ChannelMapping.ensureValid is called, we set up a default
mapping if one doesn't exist, or if the channels in the image (encoded into a string) have changed since last time.

Every node gets a ChannelMapping, even though it might not use it. ChannelMappings also persist.

While the mapping is stored in the node, the Canvas also gets a link to it.

Some notes I was working from:
* each output image (or possibly just monitoring image) needs a channel mapping
* updating channel mappings when the image is new, or its channels change
* getting an RGB image based on a channel mapping : covered by rgb() in ImageCube
* Storing one or more channel mappings inside a node
* generating a default channel mapping from an image
* linking a channel mapping into a Canvas:
    * transferring a channel mapping into the Canvas controls
    * transferring Canvas controls into a channel mapping

Rect is straightforward. Inset is more problematic, because we really want to compose the RGBs for the images
as output by the previous nodes. And now I suddenly realise a thing - the mappings don't really belong to nodes at all!
They belong to images! That would make inset easy, as well as multi-image nodes. But I need to see how well it fits
the pattern, since in a sense they *do* belong to nodes (since the canvas is part of the node).

OK, I think I see - nodes do own the channel mappings, but the channel mapping (or rather the RGB?) is stored in
the image too. This would let inset grab the RGBs for its inputs. There's a problem: what happens if an image ends up
with two mappings? Can an image end up with two mappings? Two ways around this:
* image.rgb(mapping) cannot run twice on an image with different mappings
* The mapping is assigned in the constructor
Both of these have a problem with ordering? Currently the mapping is done in the canvas! But if one of the outputs goes
to an inset, the inset would need to reperform, so the node needs to reperform!

I think I'm going to have to be more strict about this. The current sequence is:

Node performs.
onNodeChanged runs on open tabs.
    display is called on canvas with image
        ensurevalid runs on canvas' mapping with image (the canvas' mapping is set in tab's ctor), which may generate a new default mapping
        redisplay is called on canvas
            display is called on inner canvas with image and canvas mapping
                description obtained with image, window and mapping
                rgb obtained with image and mapping
                state changed
                update called

OK, the way it's going now, it looks like the the canvas, node and image might end up having references to the mapping.
Node and image is fair enough. Canvas also needs it because it needs to be able to modify it, but can't the canvas
just use the one which belongs to whichever image it gets (which will be the appropriate one in the node?)
This should deal with the problem that the current display() does the RGB magic.

Step 1: add a reference to the mapping to the image, passing it into the ctor and making rgb() use it (and cache the rgb image)

    remove parameters to rgb and rgbImage
    add parameters to imageCube

Step 2: remove the mapping passed into InnerCanvas.display(), we don't need to pass it into rgb()
Step 3: we're keeping the mapping reference in the canvas; sometimes the canvas might not have an image!

Step 5: we can now use rgb() to get the mapping from the images input into inset

Step 6: make sure ensureValid happens even when display() doesn't! This can be done by setting the mapping in the image
        with setMapping(), which is also done in the image ctor.

Test gradient - it's really messy. OK
Test inset, obviously. OK
Test ellipse detect. Not sure what the output format is and what mapping and sources should be. OK (once I'd remembered that drawing keypoints expands to rgb)
Add is hairy because of where it's getting the mapping from, but I think it works.

So here's the new idea:
Each node (XForm) holds a ChannelMapping for each image it outputs to a canvas, typically one - so helpfully we
create a default one. When we output an image, we create it with the appropriate mapping - typically like this:
            node.img = ImageCube(newimg, node.mapping, img.sources)
although we can also set the mapping with img.setMapping. This gives the image a reference to the same mapping - careful now!
This part isn't idea - when we display an image in a canvas, we do this:
        self.w.canvas.setMapping(node.mapping)
This tells the canvas to use the mapping - we have to do this, we can't just use the one in the image because there might
not be an image yet. The canvas will now know which mapping to edit, which will be the same one as in any image.



