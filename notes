A graph is a digraph of transformations, each of which can have multiple inputs and outputs
and is polymorphic.

The inputs and outputs can be of a number of different types, and these form a hierarchy:
for example, an image filter might produce RGB888 images and some other transformation might
accept any kind of image.

The nodes in the graph are transformations, not data. The glyph for a transformation should
contain its name and type data.

Because of how Python does class attributes, I'm going to differentiate between transformations
and their type with singletons. Each type of transformation has a singleton object and a
factory for generating that transformation. The polymorphism takes place in the transformation
types, the transformations are of a single type with a reference to the singleton (this is 
also how I did it with Stumpy, and how Angort types work).

transformation type object attributes:
    name
    input connection names and types (tuple of (name,type))
    output connection names and types
    others
    
transformation attributes:
    input connections (i.e. references to outputs of transformations)
    output connections (i.e. references to inputs of transformations) (yes, we do it both ways)
            (connections are a tuple of (object,index) )
    products generated by the transformation, one for each output, of the appropriate type
    private data    

Type management is hairy, but really we only care if a datum is an image or not. So if we just
have an "is image" test, then we're fine. So let's say types are just names, and start "img" if they
are images. But try to keep this tight so we can refine it later if we need to.

What about drawing? To draw things, the graph has to be converted into a Qt scene graph.
Probably the most straightforward way to do this is to have a function just do it statically when
required, rather than somehow keeping two graph structures in sync.

Macros:

Macros are stored as a graph inside the .json file, where each macro
acts as a "prototype" - the main graph is stored under GRAPH, macros
are stored in a MACROS entry containing a dict of name->graph.

When a macro is instantiated, a new graph is created from the nodes
in the prototype (the entire graph is copied). A link to each macro
instance is added to the original macro prototype.
The reason is that each macro instance needs its own set of nodes
to ensure that multiple instances of a macro in a graph don't interfere
with each other.

When the macro instance graph runs, its IN components read their inputs
and copy them to their outputs. The graph is run. The OUT components read
their inputs and copy them to their outputs, which constitute the outputs
of the macro. There is some internal wiring handling the links between
the macro node's inputs and outputs and the internal IN/OUT components.

When a user opens a macro, they are viewing the prototype. Any change to
the prototype graph should be copied to all instance graphs.


A "sink" node in a macro will be displayed in the macro's canvas.

Macros can be duplicated from the macro editor, creating another macro
prototype.

When a node is edited the current "perform()" should be "changed()."
In a "normal" graph, this will cause the current perform action. In
a macro, it will cause the prototype to look up the corresponding
node in all the instances, and will run perform on them. This should
feed the outputs. It may then have to run perform() on the macro node,
to ensure the nodes downstream of the instance also run.

Need to find a way to avoid recursive macros, and to prevent
copy/paste of in/out nodes into "normal" graphs.


So:
    perform() in Tab and subclasses -> changed()
    perform() in XFormGraph -> changed()
    
    
Saving and loading:
    Macros can be saved in a library
    Macros can be imported from another file
    So I suppose a macro library is just another file with no main graph?

New imagecubes:
   Change pancamimages images to hold filter descriptors in an array
    and not a set; order is important.

Notes on channel mappings:
Many nodes need to view their output images, which may have any number of channels. Thus a node needs a way of mapping
the output image(s) onto RGB in a way that the user can modify. To this end, we have the ChannelMapping class which
essentially encapsulates a triple of indices (red,green,blue) into the source image channel. ImageCube.rgb() takes
this mapping. The mappings initialise in a null state. When ChannelMapping.ensureValid is called, we set up a default
mapping if one doesn't exist, or if the channels in the image (encoded into a string) have changed since last time.

Every node gets a ChannelMapping, even though it might not use it. ChannelMappings also persist.

While the mapping is stored in the node, the Canvas also gets a link to it.

Some notes I was working from:
* each output image (or possibly just monitoring image) needs a channel mapping
* updating channel mappings when the image is new, or its channels change
* getting an RGB image based on a channel mapping : covered by rgb() in ImageCube
* Storing one or more channel mappings inside a node
* generating a default channel mapping from an image
* linking a channel mapping into a Canvas:
    * transferring a channel mapping into the Canvas controls
    * transferring Canvas controls into a channel mapping
