A graph is a digraph of transformations, each of which can have multiple inputs and outputs
and is polymorphic.

The inputs and outputs can be of a number of different types, and these form a hierarchy:
for example, an image filter might produce RGB888 images and some other transformation might
accept any kind of image.

The nodes in the graph are transformations, not data. The glyph for a transformation should
contain its name and type data.

Because of how Python does class attributes, I'm going to differentiate between transformations
and their type with singletons. Each type of transformation has a singleton object and a
factory for generating that transformation. The polymorphism takes place in the transformation
types, the transformations are of a single type with a reference to the singleton (this is 
also how I did it with Stumpy, and how Angort types work).

transformation type object attributes:
    name
    input connection names and types (tuple of (name,type))
    output connection names and types
    others
    
transformation attributes:
    input connections (i.e. references to outputs of transformations)
    output connections (i.e. references to inputs of transformations) (yes, we do it both ways)
            (connections are a tuple of (object,index) )
    products generated by the transformation, one for each output, of the appropriate type
    private data    

Type management is hairy, but really we only care if a datum is an image or not. So if we just
have an "is image" test, then we're fine. So let's say types are just names, and start "img" if they
are images. But try to keep this tight so we can refine it later if we need to.

What about drawing? To draw things, the graph has to be converted into a Qt scene graph.
Probably the most straightforward way to do this is to have a function just do it statically when
required, rather than somehow keeping two graph structures in sync.

Macros:

Macros are stored as a graph inside the .json file, where each macro
acts as a "prototype" - the main graph is stored under GRAPH, macros
are stored in a MACROS entry containing a dict of name->graph.

When a macro is instantiated, a new graph is created from the nodes
in the prototype (the entire graph is copied). A link to each macro
instance is added to the original macro prototype.
The reason is that each macro instance needs its own set of nodes
to ensure that multiple instances of a macro in a graph don't interfere
with each other.

When the macro instance graph runs, its IN components read their inputs
and copy them to their outputs. The graph is run. The OUT components read
their inputs and copy them to their outputs, which constitute the outputs
of the macro. There is some internal wiring handling the links between
the macro node's inputs and outputs and the internal IN/OUT components.

When a user opens a macro, they are viewing the prototype. Any change to
the prototype graph should be copied to all instance graphs.


A "sink" node in a macro will be displayed in the macro's canvas.

Macros can be duplicated from the macro editor, creating another macro
prototype.

When a node is edited the current "perform()" should be "changed()."
In a "normal" graph, this will cause the current perform action. In
a macro, it will cause the prototype to look up the corresponding
node in all the instances, and will run perform on them. This should
feed the outputs. It may then have to run perform() on the macro node,
to ensure the nodes downstream of the instance also run.

Need to find a way to avoid recursive macros, and to prevent
copy/paste of in/out nodes into "normal" graphs.


So:
    perform() in Tab and subclasses -> changed()
    perform() in XFormGraph -> changed()
    
    
Saving and loading:
    Macros can be saved in a library
    Macros can be imported from another file
    So I suppose a macro library is just another file with no main graph?

New imagecubes:
   Change pancamimages images to hold filter descriptors in an array
    and not a set; order is important.

Notes on channel mappings:
Many nodes need to view their output images, which may have any number of channels. Thus a node needs a way of mapping
the output image(s) onto RGB in a way that the user can modify. To this end, we have the ChannelMapping class which
essentially encapsulates a triple of indices (red,green,blue) into the source image channel. ImageCube.rgb() takes
this mapping. The mappings initialise in a null state. When ChannelMapping.ensureValid is called, we set up a default
mapping if one doesn't exist, or if the channels in the image (encoded into a string) have changed since last time.

Every node gets a ChannelMapping, even though it might not use it. ChannelMappings also persist.

While the mapping is stored in the node, the Canvas also gets a link to it.

Some notes I was working from:
* each output image (or possibly just monitoring image) needs a channel mapping
* updating channel mappings when the image is new, or its channels change
* getting an RGB image based on a channel mapping : covered by rgb() in ImageCube
* Storing one or more channel mappings inside a node
* generating a default channel mapping from an image
* linking a channel mapping into a Canvas:
    * transferring a channel mapping into the Canvas controls
    * transferring Canvas controls into a channel mapping

Rect is straightforward. Inset is more problematic, because we really want to compose the RGBs for the images
as output by the previous nodes. And now I suddenly realise a thing - the mappings don't really belong to nodes at all!
They belong to images! That would make inset easy, as well as multi-image nodes. But I need to see how well it fits
the pattern, since in a sense they *do* belong to nodes (since the canvas is part of the node).

OK, I think I see - nodes do own the channel mappings, but the channel mapping (or rather the RGB?) is stored in
the image too. This would let inset grab the RGBs for its inputs. There's a problem: what happens if an image ends up
with two mappings? Can an image end up with two mappings? Two ways around this:
* image.rgb(mapping) cannot run twice on an image with different mappings
* The mapping is assigned in the constructor
Both of these have a problem with ordering? Currently the mapping is done in the canvas! But if one of the outputs goes
to an inset, the inset would need to reperform, so the node needs to reperform!

I think I'm going to have to be more strict about this. The current sequence is:

Node performs.
onNodeChanged runs on open tabs.
    display is called on canvas with image
        ensurevalid runs on canvas' mapping with image (the canvas' mapping is set in tab's ctor), which may generate a new default mapping
        redisplay is called on canvas
            display is called on inner canvas with image and canvas mapping
                description obtained with image, window and mapping
                rgb obtained with image and mapping
                state changed
                update called

OK, the way it's going now, it looks like the the canvas, node and image might end up having references to the mapping.
Node and image is fair enough. Canvas also needs it because it needs to be able to modify it, but can't the canvas
just use the one which belongs to whichever image it gets (which will be the appropriate one in the node?)
This should deal with the problem that the current display() does the RGB magic.

Step 1: add a reference to the mapping to the image, passing it into the ctor and making rgb() use it (and cache the rgb image)

    remove parameters to rgb and rgbImage
    add parameters to imageCube

Step 2: remove the mapping passed into InnerCanvas.display(), we don't need to pass it into rgb()
Step 3: we're keeping the mapping reference in the canvas; sometimes the canvas might not have an image!

Step 5: we can now use rgb() to get the mapping from the images input into inset

Step 6: make sure ensureValid happens even when display() doesn't! This can be done by setting the mapping in the image
        with setMapping(), which is also done in the image ctor.

Test gradient - it's really messy. OK
Test inset, obviously. OK
Test ellipse detect. Not sure what the output format is and what mapping and sources should be. OK (once I'd remembered that drawing keypoints expands to rgb)
Add is hairy because of where it's getting the mapping from, but I think it works.

So here's the new idea:
Each node (XForm) holds a ChannelMapping for each image it outputs to a canvas, typically one - so helpfully we
create a default one. When we output an image, we create it with the appropriate mapping - typically like this:
            node.img = ImageCube(newimg, node.mapping, img.sources)
although we can also set the mapping with img.setMapping. This gives the image a reference to the same mapping - careful now!
This part isn't idea - when we display an image in a canvas, we do this:
        self.w.canvas.setMapping(node.mapping)
This tells the canvas to use the mapping - we have to do this, we can't just use the one in the image because there might
not be an image yet. The canvas will now know which mapping to edit, which will be the same one as in any image.


04/03
OK, here's what's going on. Some time ago I realised that images could be any depth, not just rgb/grey. 
That means we can't use the current type system - there can only be one "image" type, really. That means that
type checking has to be more dynamic, and that's why I've expanded the error handling system - to make errors
more visible.

We can also remove the output-matching thing, which will cause a lot of
current niggles to Go Away.


08/03
Over the weekend I thought about templates and ease of use. My idea
for templates is that for common operations the user can create an entire graph
from stored data in one or two clicks. This is great, but it can be difficult
to edit inputs. Consider the following story:
* user wants to do a Flange Wobble on an image. Creates template, has
  to locate and go into the input node to set the input.
* user now wants to run a Polarity Inversion on the same image. Now
  they have to create the template and edit the input *again*.
  
That seems like a pain. What I'd like is this:

* User wants to do a Flange Wobble on image Q.
* Loads PCOT. Clicks the button for Input 1 which opens either a dialog or tab
  and sets it up.
* Clicks the template for Flange Wobble. Done!
* Now wants to do a Polarity Inversion on the same image.
* Just click on a different template!
* Now wants to change image - just reopen the tab/dialog and change it!

So organisation of the code:

* Inputs are a Thing, and there's an array of them. I'm tempted to say 4.
  Each Input is one of a number of types, so it's tempted to do that
  as an interface. But that would cause problems switching input types
  (you'd lose data). So all input types need to exist simultaneously
  with only one being used. So inside each Input are a number of
  InputMethod objects, with one being "active" (selected by index).
  
* There will be an Input Manager owning all the inputs, itself owned by
  the main graph.
  
* Each Input may also have an InputWindow. This contains widgets for
  all the InputMethods, but only one is visible at any time (the active
  one).
  
* A main graph (NOT macro protos) owns an input manager, and loads/saves
  the input data. It is NOT loaded when a template is loaded, though
  (that's a key difference between templates and graph saves)
  

OK, what's going on with RGB on rect?

Consider that you're adding a rect to an RGB, and you modify 
the mapping. What you want to happen is this:

    Image is fetched, converted to RGB using node mapping.

    Image is then displayed in the canvas, showing that mapping,
    which can be edited.

What actually seems to happen is this:

    Image is fetched, converted to RGB using node mapping.

    Image is then sent to canvas where it is converted AGAIN
    
I've fixed this by adding an alreadyRGBMapped flag - if that's 
set, the incoming image is assumed to be RGB and won't be remapped.
There's a problem, though: the mapping is now happening inside the
corresponding node's perform, which means that changing the
comboboxes (which just triggers a canvas redisplay) won't cause
the mapping to change. Instead, in this case we need to reperform
the node when the canvas mapping changes.

Argh. I've tried to add a thing called "redisplayNode", which gets
called to perform a node in redisplay, but it's not working right.

