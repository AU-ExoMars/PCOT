{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PCOT - the PanCam Operations Toolkit PCOT is a Python program and library which allows users to manipulate multispectral images and other data from the ExoMars Rosalind Franklin rover. This is an alpha version with serious limitations. There is no calibration code of any kind (although the preliminaries are in place) PDS4 import capabilities are poor - we only support spec-rad products from the ExoMars PANCAM instrument - but we also support ENVI provided the images are 32-bit float BSQ RGB PNGs multispectral images made of multiple monochrome PNGs multispectral images made of monochrome raw files in various formats and adding new PDS4 formats should be relatively straightforward There are probably a lot of useful operations missing There are certainly a lot of bugs. Getting Started User Guide Dev Roadmap Automatically generated node and function docs Updating Existing PCOT installs can be updated by following the instructions here . Reporting bugs There are known issues which can stop PCOT running - see issues . If your problem isn't described there please create a new issue on Github or contact the Aberystwyth team. Release history Can be found here","title":"PCOT - the PanCam Operations Toolkit"},{"location":"#pcot-the-pancam-operations-toolkit","text":"PCOT is a Python program and library which allows users to manipulate multispectral images and other data from the ExoMars Rosalind Franklin rover. This is an alpha version with serious limitations. There is no calibration code of any kind (although the preliminaries are in place) PDS4 import capabilities are poor - we only support spec-rad products from the ExoMars PANCAM instrument - but we also support ENVI provided the images are 32-bit float BSQ RGB PNGs multispectral images made of multiple monochrome PNGs multispectral images made of monochrome raw files in various formats and adding new PDS4 formats should be relatively straightforward There are probably a lot of useful operations missing There are certainly a lot of bugs. Getting Started User Guide Dev Roadmap Automatically generated node and function docs","title":"PCOT - the PanCam Operations Toolkit"},{"location":"#updating","text":"Existing PCOT installs can be updated by following the instructions here .","title":"Updating"},{"location":"#reporting-bugs","text":"There are known issues which can stop PCOT running - see issues . If your problem isn't described there please create a new issue on Github or contact the Aberystwyth team.","title":"Reporting bugs"},{"location":"#release-history","text":"Can be found here","title":"Release history"},{"location":"releases/","text":"Releases Production releases None Beta releases None Alpha releases 0.10.0-alpha 2025-06-12 HALWYN ROUND I still miss Fenton Saurus. New system for storing camera data in PARC files Subcommand system with special commands (lscams, gencam etc.) Reflectance correction (given data from the camera and a calib target) Various objects no longer require Documents for deserialisation Colorchecker (i.e. Macbeth) calibration target locator Some new nodes and functions (roicull, getflags, reflectance..) Small changes to canvas DQ viewing Staying at Python 3.9 for now 0.9.0-alpha 2025-02-17 GODOLPHIN HILL I'll miss Fenton Saurus, that was a great name. New system for node parameters - the TaggedAggregate system. All nodes updated to this where it makes sense. Batch runner ( pcotbatch ) first draft, which allows a graph to be run from the command line with inputs and nodes able to be modified with a text file, and outputs able to be captured and saved. Complete rewrite of manual register node \"PARC\" input and output file format - allows multispectral images with uncertainty to be stored (and other data types too) Various bits of refactoring Issue fixes Yet more tests Nodes are now created by left-click dragging from the palette Nodes which have been renamed from their defaults have their name text shown in bold Forced to downgrade minimum Python version to 3.9 temporarily 0.8.0-alpha 2024-07-25 FENTON SAURUS Yes, really: Fenton Saurus More unit tests colour connector \"swatch\" generator script started work on dark/flat field generator Datum archives - a file format (.PARC) for saving Datum objects, with an associated input method and exporters. Required because we have no other way of saving images with uncertainty and DQ. DatumStore class wraps Archive objects so we can store Datum (this is used for the datum archives) expr uses a DataWidget, as does TabData. 1D vectors supported as a Datum.NUMBER type. Modifications made to datumfuncs and operators permit this. Notably, the semantics of mean , sd , sum , min and square bracket parsing in expressions generates vector-creation and vector-index instructions max have changed to operate band-wise and generate a vector when performed on images. Multiband extraction, e.g. a$[640,550,440] . .bands property generates a vector of wavelengths, so we can do a$b.bands , to get the bands in a that are also in b , in the same order as in b . .u property properties graph tests and QoL work for test building precedence adjustments in expressions getSelection in document can help get selected nodes in plugins serialiseFields does a deepcopy - fixes undo bugs fixes to roiexpr ; it no longer keeps UI data in the node so undo works better Cookbook in progress, but not part of the main repository to allow it to be updated more frequently First release for Zenodo 0.7.0-alpha 2024-05-03 EAST PENTIRE Very many more unit tests Bug fixes Complete rewrite of spectrum system, using the SpectrumSet object Multidot now does painted regions and floodfill Joseph's PCT detector outputs image with ROIs Dump removed and sink enhanced TabData shows sources Inputs decoupled from Sources - Sources now use composition, not inheritance Comment box for nodes removed (it was never used) Direct multifile loading Direct PDS4 loading - required refactoring of entire PDS4 layer Direct ENVI loading Raw file loading from mono images supporting lots of formats Loader presets for multifile Operator overloading on Datum objects The \"datumfunc\" system replacing hand-registration of functions flip and rotate functions (datumfuncs) String datum objects and strings usable in expr Docs on library usage Changes to nodes so that slow nodes can be disabled and very slow nodes start disabled. This functionality existed before, but was \"ad-hoc\" Document.changed() is now Document.run() and forces disabled nodes to run Most nodes now store data in their outputs rather than a \"node.out\" which is then written to an output Changes to multidot - doc improvements, UX and bug fixes 0.6.1-alpha 2023-10-04 DYNAS COVE (minor release) Multifile input can accept BMP files Better multifile documentation Filter specifications are no longer hardwired and are loaded from CSVs PANCAM and AUPE filters are default filter sets loaded in Others can be specified in a config file (and can override PANCAM and AUPE) Filter set no longer required by PDS4 input 0.6.0-alpha 2023-09-11 DRIFT STONES uncertainty and error bit propagation in expr and all nodes Testing quality and propagation rules (see Principles ) Test graphs for nodes and other high-level functionality Test nodes for those graphs Tabular output on spectrum and histogram nodes Gen node for test patterns Refactoring of Datum Utility nodes - e.g. roidq for generating an ROI from DQ bits Output enhancements Gradient node can export to PDF Annotations (e.g. text labels) are now drawn on the painter at high res, and have been refactored hugely Annotations use thickness zero by default (the Qt \"cosmetic\" thickness) PCT detector node ROI negation and refactoring of operators roiexpr node for composing ROIs using expressions Crude band depth node (needs work) A lot of bug fixes and regression fixes 0.5.0-alpha 2023-03-08 CARLENNO ROUND Data quality and bit viewing on canvas Palette and canvas interface with collapsable sections Annotations (ROIs, legends) are now drawn onto the canvas rather than the image Export to PDF, SVG and PNG with those hi-res annotations gradient is much simpler, can overlay onto the image and can draw a legend 0.4.0-alpha 2022-11-30 CAER BRAN Annotation system entirely rewritten PDF/PNG/SVG exporter Gradient legend annotation Doc updates 0.3.0-alpha 2022-10-27 BEACON HUT Open source! PDS4 importer with proctools Ad-hoc Spectrum viewer in canvas Significant rewrite of expression execution code, permitting custom types to have operations defined on them Direct input method for library use Improved default RGB mapping in canvas Testing Basics testing Testing of the operating principles (see Principles ) Source rules ROI rules rect node can now be edited numerically circle node can add circular ROIs, which can be edited numerically. 0.2.0-alpha 2022-04-21 ANJARDEN SPRING \"pixel scanning\" on canvases, shows spectrum of pixel when active custom cursor, pixel under cursor highlighted at high zooms text toggle button (currently unused) fixes to example plugin added macos.spec for pyinstaller archive system shows progress when loading each archive element Issue 1 fix (multiple tab closes when main window reinitialised) dynamic type determination for expr output can connect incompatible node outputs to inputs; indicated as red arrows infinite recursion in ROI nodes fix splash screen for Windows/Linux pyinstaller startup (not yet supported on MacOS pyinstaller) custom Datum and connection brush types now easy expr resizing regression fix multiple input buttons after load/resize fix status bar repaints on ui.msg, so it's updated in load and perform context menu on editable text caused a crash (bug in Qt). Workaround. comment boxes 0.1.0-alpha 2022-03-02 ALSIA WELL Initial alpha release outside Aberystwyth","title":"Releases"},{"location":"releases/#releases","text":"","title":"Releases"},{"location":"releases/#production-releases","text":"None","title":"Production releases"},{"location":"releases/#beta-releases","text":"None","title":"Beta releases"},{"location":"releases/#alpha-releases","text":"","title":"Alpha releases"},{"location":"releases/#0100-alpha-2025-06-12-halwyn-round","text":"I still miss Fenton Saurus. New system for storing camera data in PARC files Subcommand system with special commands (lscams, gencam etc.) Reflectance correction (given data from the camera and a calib target) Various objects no longer require Documents for deserialisation Colorchecker (i.e. Macbeth) calibration target locator Some new nodes and functions (roicull, getflags, reflectance..) Small changes to canvas DQ viewing Staying at Python 3.9 for now","title":"0.10.0-alpha 2025-06-12 HALWYN ROUND"},{"location":"releases/#090-alpha-2025-02-17-godolphin-hill","text":"I'll miss Fenton Saurus, that was a great name. New system for node parameters - the TaggedAggregate system. All nodes updated to this where it makes sense. Batch runner ( pcotbatch ) first draft, which allows a graph to be run from the command line with inputs and nodes able to be modified with a text file, and outputs able to be captured and saved. Complete rewrite of manual register node \"PARC\" input and output file format - allows multispectral images with uncertainty to be stored (and other data types too) Various bits of refactoring Issue fixes Yet more tests Nodes are now created by left-click dragging from the palette Nodes which have been renamed from their defaults have their name text shown in bold Forced to downgrade minimum Python version to 3.9 temporarily","title":"0.9.0-alpha 2025-02-17 GODOLPHIN HILL"},{"location":"releases/#080-alpha-2024-07-25-fenton-saurus","text":"Yes, really: Fenton Saurus More unit tests colour connector \"swatch\" generator script started work on dark/flat field generator Datum archives - a file format (.PARC) for saving Datum objects, with an associated input method and exporters. Required because we have no other way of saving images with uncertainty and DQ. DatumStore class wraps Archive objects so we can store Datum (this is used for the datum archives) expr uses a DataWidget, as does TabData. 1D vectors supported as a Datum.NUMBER type. Modifications made to datumfuncs and operators permit this. Notably, the semantics of mean , sd , sum , min and square bracket parsing in expressions generates vector-creation and vector-index instructions max have changed to operate band-wise and generate a vector when performed on images. Multiband extraction, e.g. a$[640,550,440] . .bands property generates a vector of wavelengths, so we can do a$b.bands , to get the bands in a that are also in b , in the same order as in b . .u property properties graph tests and QoL work for test building precedence adjustments in expressions getSelection in document can help get selected nodes in plugins serialiseFields does a deepcopy - fixes undo bugs fixes to roiexpr ; it no longer keeps UI data in the node so undo works better Cookbook in progress, but not part of the main repository to allow it to be updated more frequently First release for Zenodo","title":"0.8.0-alpha 2024-07-25 FENTON SAURUS"},{"location":"releases/#070-alpha-2024-05-03-east-pentire","text":"Very many more unit tests Bug fixes Complete rewrite of spectrum system, using the SpectrumSet object Multidot now does painted regions and floodfill Joseph's PCT detector outputs image with ROIs Dump removed and sink enhanced TabData shows sources Inputs decoupled from Sources - Sources now use composition, not inheritance Comment box for nodes removed (it was never used) Direct multifile loading Direct PDS4 loading - required refactoring of entire PDS4 layer Direct ENVI loading Raw file loading from mono images supporting lots of formats Loader presets for multifile Operator overloading on Datum objects The \"datumfunc\" system replacing hand-registration of functions flip and rotate functions (datumfuncs) String datum objects and strings usable in expr Docs on library usage Changes to nodes so that slow nodes can be disabled and very slow nodes start disabled. This functionality existed before, but was \"ad-hoc\" Document.changed() is now Document.run() and forces disabled nodes to run Most nodes now store data in their outputs rather than a \"node.out\" which is then written to an output Changes to multidot - doc improvements, UX and bug fixes","title":"0.7.0-alpha 2024-05-03 EAST PENTIRE"},{"location":"releases/#061-alpha-2023-10-04-dynas-cove-minor-release","text":"Multifile input can accept BMP files Better multifile documentation Filter specifications are no longer hardwired and are loaded from CSVs PANCAM and AUPE filters are default filter sets loaded in Others can be specified in a config file (and can override PANCAM and AUPE) Filter set no longer required by PDS4 input","title":"0.6.1-alpha 2023-10-04 DYNAS COVE (minor release)"},{"location":"releases/#060-alpha-2023-09-11-drift-stones","text":"uncertainty and error bit propagation in expr and all nodes Testing quality and propagation rules (see Principles ) Test graphs for nodes and other high-level functionality Test nodes for those graphs Tabular output on spectrum and histogram nodes Gen node for test patterns Refactoring of Datum Utility nodes - e.g. roidq for generating an ROI from DQ bits Output enhancements Gradient node can export to PDF Annotations (e.g. text labels) are now drawn on the painter at high res, and have been refactored hugely Annotations use thickness zero by default (the Qt \"cosmetic\" thickness) PCT detector node ROI negation and refactoring of operators roiexpr node for composing ROIs using expressions Crude band depth node (needs work) A lot of bug fixes and regression fixes","title":"0.6.0-alpha 2023-09-11 DRIFT STONES"},{"location":"releases/#050-alpha-2023-03-08-carlenno-round","text":"Data quality and bit viewing on canvas Palette and canvas interface with collapsable sections Annotations (ROIs, legends) are now drawn onto the canvas rather than the image Export to PDF, SVG and PNG with those hi-res annotations gradient is much simpler, can overlay onto the image and can draw a legend","title":"0.5.0-alpha 2023-03-08 CARLENNO ROUND"},{"location":"releases/#040-alpha-2022-11-30-caer-bran","text":"Annotation system entirely rewritten PDF/PNG/SVG exporter Gradient legend annotation Doc updates","title":"0.4.0-alpha 2022-11-30 CAER BRAN"},{"location":"releases/#030-alpha-2022-10-27-beacon-hut","text":"Open source! PDS4 importer with proctools Ad-hoc Spectrum viewer in canvas Significant rewrite of expression execution code, permitting custom types to have operations defined on them Direct input method for library use Improved default RGB mapping in canvas Testing Basics testing Testing of the operating principles (see Principles ) Source rules ROI rules rect node can now be edited numerically circle node can add circular ROIs, which can be edited numerically.","title":"0.3.0-alpha 2022-10-27 BEACON HUT"},{"location":"releases/#020-alpha-2022-04-21-anjarden-spring","text":"\"pixel scanning\" on canvases, shows spectrum of pixel when active custom cursor, pixel under cursor highlighted at high zooms text toggle button (currently unused) fixes to example plugin added macos.spec for pyinstaller archive system shows progress when loading each archive element Issue 1 fix (multiple tab closes when main window reinitialised) dynamic type determination for expr output can connect incompatible node outputs to inputs; indicated as red arrows infinite recursion in ROI nodes fix splash screen for Windows/Linux pyinstaller startup (not yet supported on MacOS pyinstaller) custom Datum and connection brush types now easy expr resizing regression fix multiple input buttons after load/resize fix status bar repaints on ui.msg, so it's updated in load and perform context menu on editable text caused a crash (bug in Qt). Workaround. comment boxes","title":"0.2.0-alpha 2022-04-21 ANJARDEN SPRING"},{"location":"releases/#010-alpha-2022-03-02-alsia-well","text":"Initial alpha release outside Aberystwyth","title":"0.1.0-alpha 2022-03-02 ALSIA WELL"},{"location":"roadmap/","text":"Development roadmap This is a rough guide, and things may change! Current release: 0.10.0 \"Camera pack\" files which hold filter and calibration data for particular setups in an extensible format Flatfield calibration code Reflectance calibration code Previous release 0.9.0 New node parameter system: makes the node self-documenting (node parameters appear as a table, both in the \"help\" window and in the on-line documentation provides better type and range checking when the node is modified in code provides support for batch files You now create nodes by dragging from the palette with the left mouse button batch runner allows a PCOT file to be loaded and its parameters (i.e. node settings, inputs and outputs) to be modifed with a \"batch file\" before running. The outputs can be saved to files. Batch files have a simple but flexible and powerful language for describing the changes to nodes and inputs, and how the results should be written to files. Future releases Reorganise the node palette Obtain user stories and feedback Filter aberration Filter aberration parameters need to be obtained and added to config Node (or func??) to convert aberration to image Calculate and process in canvas spectrum Calculate and process in spectrum node Obtain user stories for analysis of HK data (which could potentially get messy, as these are likely to be time series)","title":"Roadmap"},{"location":"roadmap/#development-roadmap","text":"This is a rough guide, and things may change!","title":"Development roadmap"},{"location":"roadmap/#current-release-0100","text":"\"Camera pack\" files which hold filter and calibration data for particular setups in an extensible format Flatfield calibration code Reflectance calibration code","title":"Current release: 0.10.0"},{"location":"roadmap/#previous-release-090","text":"New node parameter system: makes the node self-documenting (node parameters appear as a table, both in the \"help\" window and in the on-line documentation provides better type and range checking when the node is modified in code provides support for batch files You now create nodes by dragging from the palette with the left mouse button batch runner allows a PCOT file to be loaded and its parameters (i.e. node settings, inputs and outputs) to be modifed with a \"batch file\" before running. The outputs can be saved to files. Batch files have a simple but flexible and powerful language for describing the changes to nodes and inputs, and how the results should be written to files.","title":"Previous release 0.9.0"},{"location":"roadmap/#future-releases","text":"Reorganise the node palette Obtain user stories and feedback Filter aberration Filter aberration parameters need to be obtained and added to config Node (or func??) to convert aberration to image Calculate and process in canvas spectrum Calculate and process in spectrum node Obtain user stories for analysis of HK data (which could potentially get messy, as these are likely to be time series)","title":"Future releases"},{"location":"autodocs/","text":"Autodocs Below are automatically generated documents for certain entities in PCOT. The text in them is extracted from the Python source code, usually from \"docstring\" comments to classes or functions. They are generated by running the generate_autodocs.py script in the mkdocs directory. Nodes Nodes are the entities which make up a PCOT document's graph, taking inputs from various sources and manipulating them in various ways. PCT Patch Detection assignfilters banddepth circle colorchecker comment constant contrast stretch croproi crosscalib curve decorr stretch dqmod dummy edge errortest example expr gen gradient histequal histogram importroi in input 0 input 1 input 2 input 3 inset manual register mergetests multidot normimage offset out painted pct pixtest poly rect reflectance roicull roidq roiexpr scalartest sink spectrum stitch stringtest striproi tvl1 autoreg Expr functions Below are functions which can be used in the expression evaluation node, expr . name params opt. params (default in brackets) description abs a Calculate absolute value addroi img,r Add an ROI to an image's ROIs clamp img clamp all channels of an image to 0-1 cos a Calculate cosine of an angle in radians crop img,x,y,w,h Crop an image to a rectangle curve img mul (1),add (0) impose a sigmoid curve on an image, y=1/(1+e^-(m(x-0.5)+a))) where m and a are parameters. Note from that equation that x is biased, so that x=0.5 is the inflection point if c=0. dqset img,bits sets DQ bits flat val... Turns a list of vectors, images and numbers into a single vector, by flattening each into a 1D vector and concatenating them all together. Data with any of the dq.BAD bits is removed. fliph img Flip an image horizontally flipv img Flip an image vertically getflats img Given an image with single-channel bands from a known camera, return the flatfield images for each band as another image with the same band assignments as the input. The flats will come from the camera data for the camera. grey img opencv (0) Greyscale conversion. If the optional second argument is nonzero, and the image has 3 channels, we'll use CV's conversion equation rather than just the mean. However, this loses uncertainty information. Otherwise uncertainty is calculated by adding together the channels in quadrature and then dividing the number of channels. interp img,factor w (-1) Using trilinear interpolation, generate an image by interpolating between the bands of an existing image. If an ROI is attached, the image generated will be interpolated from the pixels in the ROI. The width of the image will be either given in an optional parameter, or will be the same as the input image. WARNING - IS VERY SLOW makeunc v1,u1,v2,u2 marksat img mn (0),mx (1.0) mark pixels outside a certain range as SAT or ERROR in the DQ bits. Pixels outside any ROI will be ignored, as will any pixels already marked as BAD. max val Find the maximum of a Datum. For a multiband image, returns a vector of the maximum of each band. For a single band image, a scalar, or a vector, returns a scalar. Pixels with \"bad\" DQ bits will be ignored. See also the mean val Find the mean\u00b1sd of a Datum. This does different things depending on what kind of Datum we are dealing with. For a scalar, it just returns the scalar. For a vector, it returns the mean and sd of the vector. For an image, it returns a vector of the means and sds of each channel. Pixels with \"bad\" DQ bits will be ignored. merge img1... merge a number of images into multiple bands of a single image. If the image has multiple bands they will all become bands in the new image. min val Find the minimum of a Datum. For a multiband image, returns a vector of the minimum value of each band. For a single band image, a scalar, or a vector, returns a scalar. Pixels with \"bad\" DQ bits will be ignored. See also the & (AND) operator, which will find the minimum of two values (or images, vectors etc). nominal d If input is an image, create an image made up of the nominal (mean) pixel values for all bands - i.e. an image with no uncertainty; if input is numeric, output the nominal value. Ignores ROIs. norm img splitchans (0) normalize all channels of an image to 0-1, operating on all channels combined (the default) or separately overlay img1,img2 Given a pair of images of the same dimensions and band count, replace all pixels in the first image with the second EXCEPT where the second image has the NODATA bit set. This is useful for combining two images where one image has \"holes\" (e.g. registration). reducecircles img,thresh useratio (0) Given an image with a set of circular ROIs, reduce all the radii until the maximum of the pooled SDs of the bands stops decreasing. The threshold value \"thresh\" is the value by which the SD must stop decreasing before we stop reducing the radius. This is useful for finding the smallest circle around a region of the same value. Alternatively, if an optional boolean is set, we just reduce every circle by a given ratio. resize img,width,height method (linear) Resize an image to a new size using OpenCV's resize function. The method is one of: \"nearest\", \"linear\", \"cubic\", \"area\", \"lanczos4\" mapping to the OpenCV constants cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS rgb img create a 3-channel image consisting of the current RGB mapping of the input image. NOTE: changing the mapping on a node does NOT cause the downstream nodes to run - you will have to click \"Run All\" to make an expr node with rgb() recalculate itself. roi img Extract a single combined ROI from all ROIs on the image. If no ROIs are present, will return a single rectangular ROI covering the entire image. rotate img,angle Rotate an image anti-clockwise by an angle in degrees. The angle must be a multiple of 90 degrees. sd val Find the SD of a Datum. This does different things depending on what kind of Datum we are dealing with. For a scalar, it just returns 0. For a vector or single-channel image, it returns a scalar. For an image, it returns a vector of the SDs of each channel. Because each individual value in the input set can have its own uncertainty, the uncertainty is pooled - the pooled variance is the mean of the variances plus the variance of the means (Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). For pooling, we make the assumption that the number of items in each input subset (e.g. each pixel) is the same. Pixels with \"bad\" DQ bits will be ignored. setcwl img,cwl Given a 1-band image, create a 'fake' filter with a given centre wavelength and assign it. The transmission of the filter is 1.0, and the fwhm is 30. The image itself is unchanged. This is used in testing only. sin a Calculate sine of an angle in radians sqrt a Calculate square root striproi img stripannots (0) Strip all regions of interest from an image stripsources d Strip all sources from a datum, leaving the data intact. This is sometimes necessary in certain processing pipelines e.g. to avoid bands with multiple sources in spectrum nodes. sum val Find the sum of a Datum. For a multiband image, returns a vector of the sums of each band. For a single band image, a scalar, or a vector, returns a scalar. The uncertainty is pooled differently as this is a sum. The variance will be the variance of the means plus the sum of the variances (still following Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). Pixels with \"bad\" DQ bits will be ignored. tan a Calculate tangent of an angle in radians testf arg1,arg2 This \"docstring\" is mandatory in datumfuncs, because it contains the description and argument types. The function calculates a+2*b, correctly combining sources and propagating uncertainty. testf2 a b (2) Calculates a+b and a-b, creating a custom object to store that data. testimg index usetestfilters (0) Load a test image. uncertainty d If input is an image, create an image made up of uncertainty data for all bands; if input is numeric, output the uncertainty. Ignores ROIs. v n,u dqbits (0) create a new value with uncertainty by combining two values. These can be either numbers or images. \" Ignores and discards ROIs. valuesbyfilter img Rather like the spectrum node, this gathers together all the pixel values for an ROI. However, it just outputs them in a table by filter, not by frequency. Useful for collating certain kinds of calibration data. vec s1... Create a 1D numeric vector from several scalars or vectors. Expr properties Below are properties which can be used in the expression evaluation node, expr . Properties are names which can be used as identifiers on the right hand side of a \".\" operator, such as a.w to get the width of an image a . name type of x desc x.w img give the width of an image in pixels (if there are ROIs, give the width of the BB of the ROI union) x.w roi give the width of an ROI in pixels x.h img give the height of an image in pixels (if there are ROIs, give the width of the BB of the ROI union) x.h roi give the width of an ROI in pixels x.n img give the area of an image in pixels (if there are ROIs, give the number of pixels in the ROI union) x.n roi give the number of pixels in an ROI x.n number give the number of items in a vector x.u img return an new image containing the uncertainties of the image's pixels x.u number return the uncertainty of the scalar or vector (if vector, individual uncertainties will be pooled using Rudmin 2010). DQ is ignored. x.bands img return a vector of the centre wavelengths of each band in the image Input and output parameters for the parameter file (batch) runner Below are links to pages describing parameters for setting the inputs and outputs in batch files. See batch mode for more details. Input parameters Output parameters Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Autodocs"},{"location":"autodocs/#autodocs","text":"Below are automatically generated documents for certain entities in PCOT. The text in them is extracted from the Python source code, usually from \"docstring\" comments to classes or functions. They are generated by running the generate_autodocs.py script in the mkdocs directory.","title":"Autodocs"},{"location":"autodocs/#nodes","text":"Nodes are the entities which make up a PCOT document's graph, taking inputs from various sources and manipulating them in various ways. PCT Patch Detection assignfilters banddepth circle colorchecker comment constant contrast stretch croproi crosscalib curve decorr stretch dqmod dummy edge errortest example expr gen gradient histequal histogram importroi in input 0 input 1 input 2 input 3 inset manual register mergetests multidot normimage offset out painted pct pixtest poly rect reflectance roicull roidq roiexpr scalartest sink spectrum stitch stringtest striproi tvl1 autoreg","title":"Nodes"},{"location":"autodocs/#expr-functions","text":"Below are functions which can be used in the expression evaluation node, expr . name params opt. params (default in brackets) description abs a Calculate absolute value addroi img,r Add an ROI to an image's ROIs clamp img clamp all channels of an image to 0-1 cos a Calculate cosine of an angle in radians crop img,x,y,w,h Crop an image to a rectangle curve img mul (1),add (0) impose a sigmoid curve on an image, y=1/(1+e^-(m(x-0.5)+a))) where m and a are parameters. Note from that equation that x is biased, so that x=0.5 is the inflection point if c=0. dqset img,bits sets DQ bits flat val... Turns a list of vectors, images and numbers into a single vector, by flattening each into a 1D vector and concatenating them all together. Data with any of the dq.BAD bits is removed. fliph img Flip an image horizontally flipv img Flip an image vertically getflats img Given an image with single-channel bands from a known camera, return the flatfield images for each band as another image with the same band assignments as the input. The flats will come from the camera data for the camera. grey img opencv (0) Greyscale conversion. If the optional second argument is nonzero, and the image has 3 channels, we'll use CV's conversion equation rather than just the mean. However, this loses uncertainty information. Otherwise uncertainty is calculated by adding together the channels in quadrature and then dividing the number of channels. interp img,factor w (-1) Using trilinear interpolation, generate an image by interpolating between the bands of an existing image. If an ROI is attached, the image generated will be interpolated from the pixels in the ROI. The width of the image will be either given in an optional parameter, or will be the same as the input image. WARNING - IS VERY SLOW makeunc v1,u1,v2,u2 marksat img mn (0),mx (1.0) mark pixels outside a certain range as SAT or ERROR in the DQ bits. Pixels outside any ROI will be ignored, as will any pixels already marked as BAD. max val Find the maximum of a Datum. For a multiband image, returns a vector of the maximum of each band. For a single band image, a scalar, or a vector, returns a scalar. Pixels with \"bad\" DQ bits will be ignored. See also the mean val Find the mean\u00b1sd of a Datum. This does different things depending on what kind of Datum we are dealing with. For a scalar, it just returns the scalar. For a vector, it returns the mean and sd of the vector. For an image, it returns a vector of the means and sds of each channel. Pixels with \"bad\" DQ bits will be ignored. merge img1... merge a number of images into multiple bands of a single image. If the image has multiple bands they will all become bands in the new image. min val Find the minimum of a Datum. For a multiband image, returns a vector of the minimum value of each band. For a single band image, a scalar, or a vector, returns a scalar. Pixels with \"bad\" DQ bits will be ignored. See also the & (AND) operator, which will find the minimum of two values (or images, vectors etc). nominal d If input is an image, create an image made up of the nominal (mean) pixel values for all bands - i.e. an image with no uncertainty; if input is numeric, output the nominal value. Ignores ROIs. norm img splitchans (0) normalize all channels of an image to 0-1, operating on all channels combined (the default) or separately overlay img1,img2 Given a pair of images of the same dimensions and band count, replace all pixels in the first image with the second EXCEPT where the second image has the NODATA bit set. This is useful for combining two images where one image has \"holes\" (e.g. registration). reducecircles img,thresh useratio (0) Given an image with a set of circular ROIs, reduce all the radii until the maximum of the pooled SDs of the bands stops decreasing. The threshold value \"thresh\" is the value by which the SD must stop decreasing before we stop reducing the radius. This is useful for finding the smallest circle around a region of the same value. Alternatively, if an optional boolean is set, we just reduce every circle by a given ratio. resize img,width,height method (linear) Resize an image to a new size using OpenCV's resize function. The method is one of: \"nearest\", \"linear\", \"cubic\", \"area\", \"lanczos4\" mapping to the OpenCV constants cv2.INTER_NEAREST, cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_LANCZOS rgb img create a 3-channel image consisting of the current RGB mapping of the input image. NOTE: changing the mapping on a node does NOT cause the downstream nodes to run - you will have to click \"Run All\" to make an expr node with rgb() recalculate itself. roi img Extract a single combined ROI from all ROIs on the image. If no ROIs are present, will return a single rectangular ROI covering the entire image. rotate img,angle Rotate an image anti-clockwise by an angle in degrees. The angle must be a multiple of 90 degrees. sd val Find the SD of a Datum. This does different things depending on what kind of Datum we are dealing with. For a scalar, it just returns 0. For a vector or single-channel image, it returns a scalar. For an image, it returns a vector of the SDs of each channel. Because each individual value in the input set can have its own uncertainty, the uncertainty is pooled - the pooled variance is the mean of the variances plus the variance of the means (Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). For pooling, we make the assumption that the number of items in each input subset (e.g. each pixel) is the same. Pixels with \"bad\" DQ bits will be ignored. setcwl img,cwl Given a 1-band image, create a 'fake' filter with a given centre wavelength and assign it. The transmission of the filter is 1.0, and the fwhm is 30. The image itself is unchanged. This is used in testing only. sin a Calculate sine of an angle in radians sqrt a Calculate square root striproi img stripannots (0) Strip all regions of interest from an image stripsources d Strip all sources from a datum, leaving the data intact. This is sometimes necessary in certain processing pipelines e.g. to avoid bands with multiple sources in spectrum nodes. sum val Find the sum of a Datum. For a multiband image, returns a vector of the sums of each band. For a single band image, a scalar, or a vector, returns a scalar. The uncertainty is pooled differently as this is a sum. The variance will be the variance of the means plus the sum of the variances (still following Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). Pixels with \"bad\" DQ bits will be ignored. tan a Calculate tangent of an angle in radians testf arg1,arg2 This \"docstring\" is mandatory in datumfuncs, because it contains the description and argument types. The function calculates a+2*b, correctly combining sources and propagating uncertainty. testf2 a b (2) Calculates a+b and a-b, creating a custom object to store that data. testimg index usetestfilters (0) Load a test image. uncertainty d If input is an image, create an image made up of uncertainty data for all bands; if input is numeric, output the uncertainty. Ignores ROIs. v n,u dqbits (0) create a new value with uncertainty by combining two values. These can be either numbers or images. \" Ignores and discards ROIs. valuesbyfilter img Rather like the spectrum node, this gathers together all the pixel values for an ROI. However, it just outputs them in a table by filter, not by frequency. Useful for collating certain kinds of calibration data. vec s1... Create a 1D numeric vector from several scalars or vectors.","title":"Expr functions"},{"location":"autodocs/#expr-properties","text":"Below are properties which can be used in the expression evaluation node, expr . Properties are names which can be used as identifiers on the right hand side of a \".\" operator, such as a.w to get the width of an image a . name type of x desc x.w img give the width of an image in pixels (if there are ROIs, give the width of the BB of the ROI union) x.w roi give the width of an ROI in pixels x.h img give the height of an image in pixels (if there are ROIs, give the width of the BB of the ROI union) x.h roi give the width of an ROI in pixels x.n img give the area of an image in pixels (if there are ROIs, give the number of pixels in the ROI union) x.n roi give the number of pixels in an ROI x.n number give the number of items in a vector x.u img return an new image containing the uncertainties of the image's pixels x.u number return the uncertainty of the scalar or vector (if vector, individual uncertainties will be pooled using Rudmin 2010). DQ is ignored. x.bands img return a vector of the centre wavelengths of each band in the image","title":"Expr properties"},{"location":"autodocs/#input-and-output-parameters-for-the-parameter-file-batch-runner","text":"Below are links to pages describing parameters for setting the inputs and outputs in batch files. See batch mode for more details. Input parameters Output parameters Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Input and output parameters for the parameter file (batch) runner"},{"location":"autodocs/PCT_Patch_Detection/","text":"PCT Patch Detection Description A Node that takes in an image holding the ExoMars Rover PCT and outputs the centre coordinates in the image of each of the PCT patches. Connections Inputs Index Name Type Desc 0 img img (none) Outputs Index Name Type Desc 0 img+rois img (none) Parameters PCT Patch Detection dp: float (default 1.0) Inverse ratio of the accumulator resolution to the image resolution for Hough detections minDist: float (default 27.0) Minimum distance between the centers of the detected circles cannyHighParam: float (default 55.0) Higher of the two Canny edge detection parameters cannyLowParam: float (default 24.0) Lower of the two CAnny edge detection parameters minRadius: integer (default 8) Minimum circle radius for patch detection (pixels) maxRadius: integer (default 24) Maximum circle radius for patch detection (pixels) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"PCT Patch Detection"},{"location":"autodocs/PCT_Patch_Detection/#pct-patch-detection","text":"","title":"PCT Patch Detection"},{"location":"autodocs/PCT_Patch_Detection/#description","text":"A Node that takes in an image holding the ExoMars Rover PCT and outputs the centre coordinates in the image of each of the PCT patches.","title":"Description"},{"location":"autodocs/PCT_Patch_Detection/#connections","text":"","title":"Connections"},{"location":"autodocs/PCT_Patch_Detection/#inputs","text":"Index Name Type Desc 0 img img (none)","title":"Inputs"},{"location":"autodocs/PCT_Patch_Detection/#outputs","text":"Index Name Type Desc 0 img+rois img (none)","title":"Outputs"},{"location":"autodocs/PCT_Patch_Detection/#parameters","text":"PCT Patch Detection dp: float (default 1.0) Inverse ratio of the accumulator resolution to the image resolution for Hough detections minDist: float (default 27.0) Minimum distance between the centers of the detected circles cannyHighParam: float (default 55.0) Higher of the two Canny edge detection parameters cannyLowParam: float (default 24.0) Lower of the two CAnny edge detection parameters minRadius: integer (default 8) Minimum circle radius for patch detection (pixels) maxRadius: integer (default 24) Maximum circle radius for patch detection (pixels) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/assignfilters/","text":"assignfilters Description This node can be used to manually assign a camera and filter data to bands in an image if this could not be done automatically. For example, the data could have been loaded from an RGB image in PNG format, or the regular expression tools in the multifile input may not have been sufficient. It can also be useful in testing. Be careful when using this - it can mess up correct information quite easily. Connections Inputs Index Name Type Desc 0 (none) img input image Outputs Index Name Type Desc 0 (none) img output image Parameters assignfilters filters: list of string (default '[]') Names of filters camera: string (default 'PANCAM') Name of camera Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"assignfilters"},{"location":"autodocs/assignfilters/#assignfilters","text":"","title":"assignfilters"},{"location":"autodocs/assignfilters/#description","text":"This node can be used to manually assign a camera and filter data to bands in an image if this could not be done automatically. For example, the data could have been loaded from an RGB image in PNG format, or the regular expression tools in the multifile input may not have been sufficient. It can also be useful in testing. Be careful when using this - it can mess up correct information quite easily.","title":"Description"},{"location":"autodocs/assignfilters/#connections","text":"","title":"Connections"},{"location":"autodocs/assignfilters/#inputs","text":"Index Name Type Desc 0 (none) img input image","title":"Inputs"},{"location":"autodocs/assignfilters/#outputs","text":"Index Name Type Desc 0 (none) img output image","title":"Outputs"},{"location":"autodocs/assignfilters/#parameters","text":"assignfilters filters: list of string (default '[]') Names of filters camera: string (default 'PANCAM') Name of camera Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/banddepth/","text":"banddepth Description Calculate band depth using a linear weighted mean of the two bands either side. Band depth is NOT the distance between the predicted reflectance of the centre band the actual centre band measurement, but the one minus the ratio of those measurements. Reference: \"Revised CRISM spectral parameters... \" Viviano, Seelos et al. 2015. Issues: Ignores ROIs - calculation occurs over the entire image; no ROI support. Ignores FWHM (bandwidth) of all bands. can't do weird stuff like Figs. 7c and 7d in the Viviano et al. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters banddepth bandidx: integer (default -1) Index of the band to calculate band depth for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"banddepth"},{"location":"autodocs/banddepth/#banddepth","text":"","title":"banddepth"},{"location":"autodocs/banddepth/#description","text":"Calculate band depth using a linear weighted mean of the two bands either side. Band depth is NOT the distance between the predicted reflectance of the centre band the actual centre band measurement, but the one minus the ratio of those measurements. Reference: \"Revised CRISM spectral parameters... \" Viviano, Seelos et al. 2015. Issues: Ignores ROIs - calculation occurs over the entire image; no ROI support. Ignores FWHM (bandwidth) of all bands. can't do weird stuff like Figs. 7c and 7d in the Viviano et al.","title":"Description"},{"location":"autodocs/banddepth/#connections","text":"","title":"Connections"},{"location":"autodocs/banddepth/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/banddepth/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/banddepth/#parameters","text":"banddepth bandidx: integer (default -1) Index of the band to calculate band depth for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/batchinputs/","text":"Input parameters Below is the full description for all the parameters controlling inputs that can be modified from a batch file. There are four duplicate entries, one for each input, so it's rather long. Each input has an entry for each input method, and which input method is active is controlled by which parameters are set: If you want to use RGB, make sure to set at least .rgb.filename If you want to use ENVI, make sure to set at least .envi.filename If you want to use PARC, make sure to set at least .parc.filename If you want to use multifile, make sure to set at least at least .multifile.directory and add at least one file by adding to .multifile.files If you want to use PDS4, make sure to set at least at least .pds4.directory and add at least one file by adding to .pds4.files Example # input 0 is an RGB file called foo.png inputs.0.rgb.filename = foo.png # input 1 is a multifile - the images are in ../test_data/images # and they inputs.1.multifile.directory = ../test_data/images # all .bin files called foo0..foo9 are to be added .filenames.+ = foo[0-9].bin # and all .bins called bar0..bar9 .+ = bar[0-9].bin Parameter table inputs 0 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 1 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 2 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 3 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Input parameters"},{"location":"autodocs/batchinputs/#input-parameters","text":"Below is the full description for all the parameters controlling inputs that can be modified from a batch file. There are four duplicate entries, one for each input, so it's rather long. Each input has an entry for each input method, and which input method is active is controlled by which parameters are set: If you want to use RGB, make sure to set at least .rgb.filename If you want to use ENVI, make sure to set at least .envi.filename If you want to use PARC, make sure to set at least .parc.filename If you want to use multifile, make sure to set at least at least .multifile.directory and add at least one file by adding to .multifile.files If you want to use PDS4, make sure to set at least at least .pds4.directory and add at least one file by adding to .pds4.files","title":"Input parameters"},{"location":"autodocs/batchinputs/#example","text":"# input 0 is an RGB file called foo.png inputs.0.rgb.filename = foo.png # input 1 is a multifile - the images are in ../test_data/images # and they inputs.1.multifile.directory = ../test_data/images # all .bin files called foo0..foo9 are to be added .filenames.+ = foo[0-9].bin # and all .bins called bar0..bar9 .+ = bar[0-9].bin","title":"Example"},{"location":"autodocs/batchinputs/#parameter-table","text":"inputs 0 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 1 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 2 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename 3 inputs by index rgb RGB input method filename: string (default None) filename envi ENVI input method filename: string (default None) filename (without extension) multifile Multifile input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. filter_pattern: string (default None) pattern for filter identification camera: string (default 'PANCAM') name of camera to use bit_depth: integer (default None) number of bits used in the image (default is all bits) preset: string (default None) preset name for some params (can be overridden by other params) raw parameters for loading raw data format: string (default 'u16') integer format (u16 u8 or f32) width: integer (default 1024) image width height: integer (default 1024) image height bigendian: boolean (default False) whether the data is big-endian offset: integer (default 0) size of the header in bytes (this is skipped) rot: integer (default 0) counter-clockwise rotation of the image in degrees (must be multiple of 90) horzflip: boolean (default False) whether the image is flipped horizontally (after rotation) vertflip: boolean (default False) whether the image is flipped vertically (after rotation) pds4 PDS4 input method directory: string (default None) directory to load from filenames: list of string (default '[]') list of filenames or filename patterns to match. All files matching each pattern will be added, sorted by name within each pattern. parc PCOT datum archive input method filename: string (default None) filename itemname: string (default 'main') name of the item in the archive direct Direct input method filename: string (default None) filename Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameter table"},{"location":"autodocs/batchoutputs/","text":"Output parameters Below is the full description for all the parameters controlling an output in batch processing. You need to add at least one output and set its parameters. Example outputs.+.node = sink # create a new output, have it use the node called 'sink' .file = foo.pdf # it will write to foo.pdf .clobber = y # existing foo.pdf will be overwritten .annotations = y # annotations will be output .outputs.+.node = expr # new output, using the 'expr' node .file = foo.txt # output to foo.txt .prefix = Result of expr : # some text to prefix with .clobber = y # existing file overwritten Parameter table outputs node: string (default None) node name output: integer (default None) node output connection (or None for the default) file: string (default None) output filename - if not provided, use previous output's value (see 'append') clobber: boolean (default False) overwrite the file if it exists (else raise an exception) format: string (txt, csv, pdf, svg, png, jpg, jpeg, bmp, tiff, parc) (default None) output format for image in lowercase (will determine from extension if not given) annotations: boolean (default None) draw ROIs/annotations on the image (must be false for PARC). If not provided, use previous output's value (or false if first output) name: string (default None) name of the datum (if a PARC is being written) - 'main' if not given description: string (default None) description of the data (if a PARC is being written) width: integer (default 1000) width of output image when exporting to raster formats (in pixels) if annotations is true. If annotations is false or width is negative, no resizing is done. append: boolean (default None) append to a PARC or text file if it exists. If not provided, use previous output's value (or false if first output) prefix: string (default None) prefix for the output (text outputs only) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Output parameters"},{"location":"autodocs/batchoutputs/#output-parameters","text":"Below is the full description for all the parameters controlling an output in batch processing. You need to add at least one output and set its parameters.","title":"Output parameters"},{"location":"autodocs/batchoutputs/#example","text":"outputs.+.node = sink # create a new output, have it use the node called 'sink' .file = foo.pdf # it will write to foo.pdf .clobber = y # existing foo.pdf will be overwritten .annotations = y # annotations will be output .outputs.+.node = expr # new output, using the 'expr' node .file = foo.txt # output to foo.txt .prefix = Result of expr : # some text to prefix with .clobber = y # existing file overwritten","title":"Example"},{"location":"autodocs/batchoutputs/#parameter-table","text":"outputs node: string (default None) node name output: integer (default None) node output connection (or None for the default) file: string (default None) output filename - if not provided, use previous output's value (see 'append') clobber: boolean (default False) overwrite the file if it exists (else raise an exception) format: string (txt, csv, pdf, svg, png, jpg, jpeg, bmp, tiff, parc) (default None) output format for image in lowercase (will determine from extension if not given) annotations: boolean (default None) draw ROIs/annotations on the image (must be false for PARC). If not provided, use previous output's value (or false if first output) name: string (default None) name of the datum (if a PARC is being written) - 'main' if not given description: string (default None) description of the data (if a PARC is being written) width: integer (default 1000) width of output image when exporting to raster formats (in pixels) if annotations is true. If annotations is false or width is negative, no resizing is done. append: boolean (default None) append to a PARC or text file if it exists. If not provided, use previous output's value (or false if first output) prefix: string (default None) prefix for the output (text outputs only) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameter table"},{"location":"autodocs/circle/","text":"circle Description Add a circular ROI to an image (see multidot for multiple circles). Can edit numerically. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. Connections Inputs Index Name Type Desc 0 input img (none) Outputs Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest Parameters circle label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"circle"},{"location":"autodocs/circle/#circle","text":"","title":"circle"},{"location":"autodocs/circle/#description","text":"Add a circular ROI to an image (see multidot for multiple circles). Can edit numerically. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected.","title":"Description"},{"location":"autodocs/circle/#connections","text":"","title":"Connections"},{"location":"autodocs/circle/#inputs","text":"Index Name Type Desc 0 input img (none)","title":"Inputs"},{"location":"autodocs/circle/#outputs","text":"Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest","title":"Outputs"},{"location":"autodocs/circle/#parameters","text":"circle label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/colorchecker/","text":"colorchecker Description Allows the user to locate a GretagMacbeth ColorChecker Classic in an image by specifying control points, move those control points, and generate ROIs for each patch by floodfill. Connections Inputs Index Name Type Desc 0 img img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters colorchecker Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"colorchecker"},{"location":"autodocs/colorchecker/#colorchecker","text":"","title":"colorchecker"},{"location":"autodocs/colorchecker/#description","text":"Allows the user to locate a GretagMacbeth ColorChecker Classic in an image by specifying control points, move those control points, and generate ROIs for each patch by floodfill.","title":"Description"},{"location":"autodocs/colorchecker/#connections","text":"","title":"Connections"},{"location":"autodocs/colorchecker/#inputs","text":"Index Name Type Desc 0 img img (none)","title":"Inputs"},{"location":"autodocs/colorchecker/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/colorchecker/#parameters","text":"colorchecker Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/comment/","text":"comment Description Comment box Connections Parameters comment string: string (default 'comment') comment text boxColour box colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 1.0) The blue component 0-1 textColour text colour (ordered) r: int or float (default 0.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 fontSize: integer (default 12) font size Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"comment"},{"location":"autodocs/comment/#comment","text":"","title":"comment"},{"location":"autodocs/comment/#description","text":"Comment box","title":"Description"},{"location":"autodocs/comment/#connections","text":"","title":"Connections"},{"location":"autodocs/comment/#parameters","text":"comment string: string (default 'comment') comment text boxColour box colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 1.0) The blue component 0-1 textColour text colour (ordered) r: int or float (default 0.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 fontSize: integer (default 12) font size Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/constant/","text":"constant Description Generates a numeric value which can be typed directly into the node's box in the graph Connections Outputs Index Name Type Desc 0 (none) number (none) Parameters constant val: float (default 0.0) The value of the constant Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"constant"},{"location":"autodocs/constant/#constant","text":"","title":"constant"},{"location":"autodocs/constant/#description","text":"Generates a numeric value which can be typed directly into the node's box in the graph","title":"Description"},{"location":"autodocs/constant/#connections","text":"","title":"Connections"},{"location":"autodocs/constant/#outputs","text":"Index Name Type Desc 0 (none) number (none)","title":"Outputs"},{"location":"autodocs/constant/#parameters","text":"constant val: float (default 0.0) The value of the constant Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/contrast_stretch/","text":"contrast stretch Description Perform a simple contrast stretch separately on each channel. The stretch is linear around the midpoint and excessive values are clamped. The knob controls the amount of stretch applied. Uncertainty is discarded. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters contrast stretch tol: float (default 0.2) Contrast stretch tolerance (between 0 and 0.5 exclusive) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"contrast stretch"},{"location":"autodocs/contrast_stretch/#contrast-stretch","text":"","title":"contrast stretch"},{"location":"autodocs/contrast_stretch/#description","text":"Perform a simple contrast stretch separately on each channel. The stretch is linear around the midpoint and excessive values are clamped. The knob controls the amount of stretch applied. Uncertainty is discarded.","title":"Description"},{"location":"autodocs/contrast_stretch/#connections","text":"","title":"Connections"},{"location":"autodocs/contrast_stretch/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/contrast_stretch/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/contrast_stretch/#parameters","text":"contrast stretch tol: float (default 0.2) Contrast stretch tolerance (between 0 and 0.5 exclusive) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/croproi/","text":"croproi Description Crops an image to a rectangle which is the union of its regions of interest. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters croproi Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"croproi"},{"location":"autodocs/croproi/#croproi","text":"","title":"croproi"},{"location":"autodocs/croproi/#description","text":"Crops an image to a rectangle which is the union of its regions of interest.","title":"Description"},{"location":"autodocs/croproi/#connections","text":"","title":"Connections"},{"location":"autodocs/croproi/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/croproi/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/croproi/#parameters","text":"croproi Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/crosscalib/","text":"crosscalib Description \"Cross-calibrate\" two images: given points S in a source image and corresponding points D in a destination image, find a vector of factors v for the bands such that S=vD, and transform S accordingly. Essentially, and crudely speaking, make the colours in S match those in D by sampling the same points in each. Bad pixels in the parent image will be ignored for getting the colours, and the new image will have the DQ bits from S. Uncertainty is not propagated through this node. DQ is propagated from the source image. Connections Inputs Index Name Type Desc 0 source img (none) 1 dest img (none) Outputs Index Name Type Desc 0 out img (none) Parameters crosscalib showSrc: boolean (default True) Show source points showDest: boolean (default True) Show destination points r: integer (default 10) Point radius src: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate dest: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"crosscalib"},{"location":"autodocs/crosscalib/#crosscalib","text":"","title":"crosscalib"},{"location":"autodocs/crosscalib/#description","text":"\"Cross-calibrate\" two images: given points S in a source image and corresponding points D in a destination image, find a vector of factors v for the bands such that S=vD, and transform S accordingly. Essentially, and crudely speaking, make the colours in S match those in D by sampling the same points in each. Bad pixels in the parent image will be ignored for getting the colours, and the new image will have the DQ bits from S. Uncertainty is not propagated through this node. DQ is propagated from the source image.","title":"Description"},{"location":"autodocs/crosscalib/#connections","text":"","title":"Connections"},{"location":"autodocs/crosscalib/#inputs","text":"Index Name Type Desc 0 source img (none) 1 dest img (none)","title":"Inputs"},{"location":"autodocs/crosscalib/#outputs","text":"Index Name Type Desc 0 out img (none)","title":"Outputs"},{"location":"autodocs/crosscalib/#parameters","text":"crosscalib showSrc: boolean (default True) Show source points showDest: boolean (default True) Show destination points r: integer (default 10) Point radius src: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate dest: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/curve/","text":"curve Description Maps the image channel intensities to a logistic sigmoid curve, y=1/(1+e^-(ax+b)), where a is \"mul\" and b is \"add\". Honours regions of interest. Ignores DQ and uncertainty Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters curve mul: float (default 1.0) multiplicative factor (done first) add: float (default 0.0) additive constant (done last) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"curve"},{"location":"autodocs/curve/#curve","text":"","title":"curve"},{"location":"autodocs/curve/#description","text":"Maps the image channel intensities to a logistic sigmoid curve, y=1/(1+e^-(ax+b)), where a is \"mul\" and b is \"add\". Honours regions of interest. Ignores DQ and uncertainty","title":"Description"},{"location":"autodocs/curve/#connections","text":"","title":"Connections"},{"location":"autodocs/curve/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/curve/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/curve/#parameters","text":"curve mul: float (default 1.0) multiplicative factor (done first) add: float (default 0.0) additive constant (done last) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/decorr_stretch/","text":"decorr stretch Description Perform a decorrelation stretch on an RGB image Ignores DQ and uncertainty Connections Inputs Index Name Type Desc 0 rgb img (none) Outputs Index Name Type Desc 0 rgb img (none) Parameters decorr stretch Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"decorr stretch"},{"location":"autodocs/decorr_stretch/#decorr-stretch","text":"","title":"decorr stretch"},{"location":"autodocs/decorr_stretch/#description","text":"Perform a decorrelation stretch on an RGB image Ignores DQ and uncertainty","title":"Description"},{"location":"autodocs/decorr_stretch/#connections","text":"","title":"Connections"},{"location":"autodocs/decorr_stretch/#inputs","text":"Index Name Type Desc 0 rgb img (none)","title":"Inputs"},{"location":"autodocs/decorr_stretch/#outputs","text":"Index Name Type Desc 0 rgb img (none)","title":"Outputs"},{"location":"autodocs/decorr_stretch/#parameters","text":"decorr stretch Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/dqmod/","text":"dqmod Description Modify DQ bits based on conditions in the existing nominal or uncertainty for all bands or just a single band. **WARNING**: This may set \"bad\" bits which will be masked in any calculation. For some settings, these bad bits can mask bands other than those from which they are derived. Calculations involving pixels with these bits will be partially derived from the mask, but this information will not be tracked by the source mechanism. This means that some source tracking information can be lost. (Issue #69) Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters dqmod val: float (default 0.0) Value to test against dq: integer (default 0) DQ bits to set mod: string (set, clear) (default 'set') Set or clear bits test: string (le, ge, gt, lt, always) (default 'le') Test to perform data: string (nominal, uncertainty) (default 'nominal') Data to test band: integer (default None) Band to test (or None for all) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"dqmod"},{"location":"autodocs/dqmod/#dqmod","text":"","title":"dqmod"},{"location":"autodocs/dqmod/#description","text":"Modify DQ bits based on conditions in the existing nominal or uncertainty for all bands or just a single band. **WARNING**: This may set \"bad\" bits which will be masked in any calculation. For some settings, these bad bits can mask bands other than those from which they are derived. Calculations involving pixels with these bits will be partially derived from the mask, but this information will not be tracked by the source mechanism. This means that some source tracking information can be lost. (Issue #69)","title":"Description"},{"location":"autodocs/dqmod/#connections","text":"","title":"Connections"},{"location":"autodocs/dqmod/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/dqmod/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/dqmod/#parameters","text":"dqmod val: float (default 0.0) Value to test against dq: integer (default 0) DQ bits to set mod: string (set, clear) (default 'set') Set or clear bits test: string (le, ge, gt, lt, always) (default 'le') Test to perform data: string (nominal, uncertainty) (default 'nominal') Data to test band: integer (default None) Band to test (or None for all) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/dummy/","text":"dummy Description A dummy node type used when the node type specified in a loaded file cannot be found - perhaps it is from an older PCOT version and is now deprecated, or it's part of a plugin? Connections Parameters dummy Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"dummy"},{"location":"autodocs/dummy/#dummy","text":"","title":"dummy"},{"location":"autodocs/dummy/#description","text":"A dummy node type used when the node type specified in a loaded file cannot be found - perhaps it is from an older PCOT version and is now deprecated, or it's part of a plugin?","title":"Description"},{"location":"autodocs/dummy/#connections","text":"","title":"Connections"},{"location":"autodocs/dummy/#parameters","text":"dummy Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/edge/","text":"edge Description This is an edge detector node. It takes an image and performs Canny edge detection, currently with fixed thresholds. It does not take account of ROIs, since this would be pointless when we're converting from a potentially multispectral image to greyscale (well, boolean). Exercise for the reader - add variable thresholds, either as numeric inputs or as numeric parameters settable from the node tab. DQ bits of the bands in the source are combined together for the single band of the result. Uncertainty is discarded. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters No automatic parameter documentation available for edge Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"edge"},{"location":"autodocs/edge/#edge","text":"","title":"edge"},{"location":"autodocs/edge/#description","text":"This is an edge detector node. It takes an image and performs Canny edge detection, currently with fixed thresholds. It does not take account of ROIs, since this would be pointless when we're converting from a potentially multispectral image to greyscale (well, boolean). Exercise for the reader - add variable thresholds, either as numeric inputs or as numeric parameters settable from the node tab. DQ bits of the bands in the source are combined together for the single band of the result. Uncertainty is discarded.","title":"Description"},{"location":"autodocs/edge/#connections","text":"","title":"Connections"},{"location":"autodocs/edge/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/edge/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/edge/#parameters","text":"No automatic parameter documentation available for edge Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/errortest/","text":"errortest Description Check that a node produces an error. This node will run after all other nodes, but before its children. It checks that the string is the error code (e.g. 'DATA') Connections Inputs Index Name Type Desc 0 (none) any (none) Outputs Index Name Type Desc 0 (none) testresult (none) Parameters errortest string: string (default '') Error code to check for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"errortest"},{"location":"autodocs/errortest/#errortest","text":"","title":"errortest"},{"location":"autodocs/errortest/#description","text":"Check that a node produces an error. This node will run after all other nodes, but before its children. It checks that the string is the error code (e.g. 'DATA')","title":"Description"},{"location":"autodocs/errortest/#connections","text":"","title":"Connections"},{"location":"autodocs/errortest/#inputs","text":"Index Name Type Desc 0 (none) any (none)","title":"Inputs"},{"location":"autodocs/errortest/#outputs","text":"Index Name Type Desc 0 (none) testresult (none)","title":"Outputs"},{"location":"autodocs/errortest/#parameters","text":"errortest string: string (default '') Error code to check for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/example/","text":"example Description This object is not a node, but the singleton to which nodes of this type point to determine their behaviour. This docstring will form the help text for the node in the UI. Markdown is permitted and processed into HTML. Look at (say) XFormGradient for an example of how to write this. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters example parameter: float (default 0.0) Parameter 1 parameter2: string (default 'default') Parameter 2 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"example"},{"location":"autodocs/example/#example","text":"","title":"example"},{"location":"autodocs/example/#description","text":"This object is not a node, but the singleton to which nodes of this type point to determine their behaviour. This docstring will form the help text for the node in the UI. Markdown is permitted and processed into HTML. Look at (say) XFormGradient for an example of how to write this.","title":"Description"},{"location":"autodocs/example/#connections","text":"","title":"Connections"},{"location":"autodocs/example/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/example/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/example/#parameters","text":"example parameter: float (default 0.0) Parameter 1 parameter2: string (default 'default') Parameter 2 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/expr/","text":"expr Description Expression evaluator. The node box will show the text of the expression. The \"run\" button must be clicked to set the node to the new expression and perform it. The input can accept any type of data and the output type is determined when the node is run. The four inputs are assigned to the variables a, b, c, and d. They are typically (but not necessarily) images or numeric values. The standard operators +,/,*,- and ^ all have their usual meanings. When applied to images they work in a pixel-wise fashion, so if a is an image, 2*a will double the brightness. If b is also an image, a+b will add the two images, pixel by pixel. There are two non-standard operators: . for properties and $ for band extraction. These are described below. Image/numeric operators: operator description precedence (higher binds tighter) A + B add A to B (can act on ROIs) 10 A - B subtract A from B (can act on ROIs) 10 A / B divide A by B (can act on ROIs) 20 A * B multiply A by B (can act on ROIs) 20 A ^ B exponentiate A to the power B (can act on ROIs) 30 -A element-wise negation of A (can act on ROIs) 50 A.B property B of entity A (e.g. a.h is height of image a) 80 A$546 extract single band image of wavelength 546 100 A$_2 extract single band image from band 2 explicitly 100 A&B element-wise minimum of A and B (Zadeh's AND operator) 20 A|B element-wise maximum of A and B (Zadeh's OR operator) 20 !A element-wise 1-A (Zadeh's NOT operator) 50 All operators can act on images, 1D vectors and scalars with the exception of . and $ which have images on the left-hand side and identifiers or integers on the right-hand side. Those operators marked with (can act on ROIs) can also act on pairs of ROIs (regions of interest, see below). Binary operations on image pairs These act by performing the binary operation on the two underlying Numpy arrays. This means you may need to be careful about the ordering of the bands in the two images, because they will simply be operated on in the order they appear. For example, consider adding two images $a$ and $b$ with the same bands in a slightly different order: image a image b result of addition 480nm 480nm sum of 480nm bands 500nm 500nm sum of 500nm bands 610nm 670nm a 's 610nm band plus b 's 670nm band 670nm 610nm copy of previous band (addition being commutative) This probably isn't what you wanted. Note that this is obviously not an issue when an operation is being performed on bands in a single image. Binary operators on images with regions of interest If one of the two images has an ROI, the operation is only performed on that ROI; the remaining area of output is taken from the image without an ROI. If both images have an ROI an error will result - it is likely that this is a mistake on the user's part, and doing something more \"intelligent\" might conceal this. The desired result can be achieved using expr nodes on ROIs and an importroi node. Operations with vectors Some functions can generate vectors, such as mean for getting the means of the bands, and vec for generating vectors by hand. If an image is used in a binary operation with a vector on the other side, the vector must have the same number of elements as there are bands in the image. The operation will be performed on each band. Consider a 3-band image and the vector [2,3,4] . If we multiply them, the result will an image with the first band multiplied by 2, the second band multiplied by 3, and the third band multiplied by 4. Operators on ROIs themselves (as opposed to images with ROIs) operator description a+b union a*b intersection a-b difference You can source ROIs from the \"roi\" output of ROI nodes, and impose resulting ROIs on images with \"importroi\" node. Band extraction The notation $name or $wavelength takes an image on the left-hand side and extracts a single band, generating a new monochrome image. The right-hand side is either a filter name, a filter position, a wavelength or a band index preceded by \"_\". Depending on the camera, all these could be valid: expression meaning a$780 the 780nm band in image a a$_2 band 2 in the image a (a+b)$G0 the band named G0 in the image formed by adding images a and b ((a+b)/2)$780 the average of the 780nm bands of images a and b Be aware of caveats in the \"binary operations on image pairs\" section above: it may be better to extract the band before performing the operation, thus: old expression better expression (a+b)$G0 a$G0 + b$G0 ((a+b)/2)$780 (a$780+b$780)/2 Brackets Round brackets are used to group expressions as usual, but square brackets are used for indexing into a vector. For example, a[3] will extract the fourth element of the vector a . However, square brackets cannot (yet) create a vector. To do this, use the vec function - so vec(1,2,3)[1] will return 2. Band extraction can also be performed with vectors provided the vector elements are numeric (i.e. wavelengths): a $ vec(640,550,440) is valid. Properties Properties are indicated by the . operator, e.g. a.w to find an image's width. Help on functions and properties A list of functions can be obtained by right-clicking on either the log pane or function entry pane and selecting \"List all functions.\" Help on an individual function can be found by hovering over the name of a function, right-clicking and selecting \"Get help on 'somefunction'\". Similar actions are supported for properties. Uncertainties are assumed to be independent in all binary operations While uncertainty is propagated through operations (as population standard deviation) all quantities are assumed to be independent (calculating covariances is beyond the scope of this system). Be very careful here. For example, the uncertainty for the expression tan(a) will be calculated correctly, but if you try to use sin(a)/cos(a) the uncertainty will be incorrect because the nominator and denominator are not independent. Connections Inputs Index Name Type Desc 0 a any (none) 1 b any (none) 2 c any (none) 3 d any (none) Outputs Index Name Type Desc 0 (none) none (none) Parameters expr expr: string (default '') Expression to evaluate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"expr"},{"location":"autodocs/expr/#expr","text":"","title":"expr"},{"location":"autodocs/expr/#description","text":"Expression evaluator. The node box will show the text of the expression. The \"run\" button must be clicked to set the node to the new expression and perform it. The input can accept any type of data and the output type is determined when the node is run. The four inputs are assigned to the variables a, b, c, and d. They are typically (but not necessarily) images or numeric values. The standard operators +,/,*,- and ^ all have their usual meanings. When applied to images they work in a pixel-wise fashion, so if a is an image, 2*a will double the brightness. If b is also an image, a+b will add the two images, pixel by pixel. There are two non-standard operators: . for properties and $ for band extraction. These are described below.","title":"Description"},{"location":"autodocs/expr/#connections","text":"","title":"Connections"},{"location":"autodocs/expr/#inputs","text":"Index Name Type Desc 0 a any (none) 1 b any (none) 2 c any (none) 3 d any (none)","title":"Inputs"},{"location":"autodocs/expr/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"autodocs/expr/#parameters","text":"expr expr: string (default '') Expression to evaluate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/gen/","text":"gen Description Generate an image with given channel values. Can also generate patterns. Each band is given a nominal value and uncertainty, along with a centre frequency and a mode (for patterns). Modes are: flat : N and U are used to fill the entire band ripple-n: the N value is not a value, but a multiplier applied to distance from centre - the sine of this gives the value. The U value is generated as in 'flat' ripple-u: as ripple-n, but this time U is used as a multiplier to generate the ripple pattern in uncertainty, while N is generated as in 'flat' ripple-un: both values are ripple multipliers. half: nominal is N on the left, U on the right. Uncertainty is 0.1. (Test value) checkx: nominal is a checquered pattern with each square of size N, offset by U in the x-axis. uncertainty=nominal. checky: nominal is a checquered pattern with each square of size N, offset by U in the y-axis. uncertainty=nominal. rand: both nom. and unc. are filled with non-negative pseudorandom uniform noise multiplied by N and U respectively gaussian: nom. is filled with gaussian noise centered around N with a std. dev. of U. U is zero. The RNG is seeded from the CWL. gradient-x: nom. is filled with a gradient from 0-1, U is zero. Number of steps is N (zero means smooth). gradient-y: nom. is filled with a gradient from 0-1, U is zero. Number of steps is N (zero means smooth). A useful pattern might be something like this: Chan 0: checkx, N=8, U=0 Chan 1: checkx, N=8, U=4 Chan 2: checky, N=8, U=4 To get variation in uncertainty, create a similar pattern but with a longer period using another gen node: Chan 0: checkx, N=16, U=0 Chan 1: checkx, N=16, U=8 Chan 2: checky, N=16, U=8 and merge the two together, using the first gen to create nominal values and the second to create uncertainty values, with an expr node using the expression v(a,b) . Connections Outputs Index Name Type Desc 0 (none) img (none) Parameters gen imgwidth: integer (default 256) Width of the image imgheight: integer (default 256) Height of the image chans: list (ordered) n: float (default 0.0) Nominal control value - 'mode' determines how it is used u: float (default 0.0) Uncertainty control value - 'mode' determines how it is used cwl: integer (default 100) Centre wavelength mode: string (flat, ripple-n, ripple-u, ripple-un, half, checkx, checky, rand, gaussian, gradient-x, gradient-y) (default 'flat') Mode of operation text: string (default '') Text to write on the image textx: integer (default 0) X position of text texty: integer (default 0) Y position of text textsize: float (default 1.0) Size of text (units arbitrary but relative to image size) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"gen"},{"location":"autodocs/gen/#gen","text":"","title":"gen"},{"location":"autodocs/gen/#description","text":"Generate an image with given channel values. Can also generate patterns. Each band is given a nominal value and uncertainty, along with a centre frequency and a mode (for patterns). Modes are: flat : N and U are used to fill the entire band ripple-n: the N value is not a value, but a multiplier applied to distance from centre - the sine of this gives the value. The U value is generated as in 'flat' ripple-u: as ripple-n, but this time U is used as a multiplier to generate the ripple pattern in uncertainty, while N is generated as in 'flat' ripple-un: both values are ripple multipliers. half: nominal is N on the left, U on the right. Uncertainty is 0.1. (Test value) checkx: nominal is a checquered pattern with each square of size N, offset by U in the x-axis. uncertainty=nominal. checky: nominal is a checquered pattern with each square of size N, offset by U in the y-axis. uncertainty=nominal. rand: both nom. and unc. are filled with non-negative pseudorandom uniform noise multiplied by N and U respectively gaussian: nom. is filled with gaussian noise centered around N with a std. dev. of U. U is zero. The RNG is seeded from the CWL. gradient-x: nom. is filled with a gradient from 0-1, U is zero. Number of steps is N (zero means smooth). gradient-y: nom. is filled with a gradient from 0-1, U is zero. Number of steps is N (zero means smooth). A useful pattern might be something like this: Chan 0: checkx, N=8, U=0 Chan 1: checkx, N=8, U=4 Chan 2: checky, N=8, U=4 To get variation in uncertainty, create a similar pattern but with a longer period using another gen node: Chan 0: checkx, N=16, U=0 Chan 1: checkx, N=16, U=8 Chan 2: checky, N=16, U=8 and merge the two together, using the first gen to create nominal values and the second to create uncertainty values, with an expr node using the expression v(a,b) .","title":"Description"},{"location":"autodocs/gen/#connections","text":"","title":"Connections"},{"location":"autodocs/gen/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/gen/#parameters","text":"gen imgwidth: integer (default 256) Width of the image imgheight: integer (default 256) Height of the image chans: list (ordered) n: float (default 0.0) Nominal control value - 'mode' determines how it is used u: float (default 0.0) Uncertainty control value - 'mode' determines how it is used cwl: integer (default 100) Centre wavelength mode: string (flat, ripple-n, ripple-u, ripple-un, half, checkx, checky, rand, gaussian, gradient-x, gradient-y) (default 'flat') Mode of operation text: string (default '') Text to write on the image textx: integer (default 0) X position of text texty: integer (default 0) Y position of text textsize: float (default 1.0) Size of text (units arbitrary but relative to image size) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/gradient/","text":"gradient Description Convert a mono image to an RGB gradient image for better visibility. If the \"insetinto\" input has an image AND there is a valid ROI in the mono image, the image will be inset into the RGB of the insetinto image. NOTE: if you change the \"insetinto\" image's RGB mapping you may need to \"run all\" to see the the change reflected. Ignores DQ and uncertainty The gradient widget has the following behaviour: click and drag to move a colour point doubleclick to delete an existing colour point doubleclick to add a new colour point right click to edit an existing colour point Node parameters: gradient: utils.Gradient object containing gradient info colour: (r,g,b) [0:1] colour of text and border for in-image legend legendrect: (x,y,w,h) rectangle for in-image legend vertical: true if vertical legend fontscale: size of font thickness: border thickness legendPos: string describing position: 'In image', 'Top margin', 'Bottom margin', 'Left margin', 'Right margin', 'None' These are also defined as constants LEFT_MARGIN... IN_IMAGE (and None) Connections Inputs Index Name Type Desc 0 mono img (none) 1 background img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters gradient gradient: list (ordered) x: float (default 0.0) x colour colour (ordered) r: int or float (default 0.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 preset: string (topo, grey, viridis, magma, plasma, inferno, cividis, mako, turbo, rocket) (default None) preset colour legend colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 legendrect legend rectangle if in image (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 100) The width of the rectangle h: integer (default 20) The height of the rectangle vertical: boolean (default False) is legend vertical? fontscale: float (default 10.0) size of legend font thickness: float (default 1.0) thickness of legend border legendPos: string (Left margin, Right margin, Top margin, Bottom margin, In image, None) (default 'In image') legend location normbackground: boolean (default False) normalise background sigfigs: integer (default 6) significant figures Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"gradient"},{"location":"autodocs/gradient/#gradient","text":"","title":"gradient"},{"location":"autodocs/gradient/#description","text":"Convert a mono image to an RGB gradient image for better visibility. If the \"insetinto\" input has an image AND there is a valid ROI in the mono image, the image will be inset into the RGB of the insetinto image. NOTE: if you change the \"insetinto\" image's RGB mapping you may need to \"run all\" to see the the change reflected. Ignores DQ and uncertainty The gradient widget has the following behaviour: click and drag to move a colour point doubleclick to delete an existing colour point doubleclick to add a new colour point right click to edit an existing colour point Node parameters: gradient: utils.Gradient object containing gradient info colour: (r,g,b) [0:1] colour of text and border for in-image legend legendrect: (x,y,w,h) rectangle for in-image legend vertical: true if vertical legend fontscale: size of font thickness: border thickness legendPos: string describing position: 'In image', 'Top margin', 'Bottom margin', 'Left margin', 'Right margin', 'None' These are also defined as constants LEFT_MARGIN... IN_IMAGE (and None)","title":"Description"},{"location":"autodocs/gradient/#connections","text":"","title":"Connections"},{"location":"autodocs/gradient/#inputs","text":"Index Name Type Desc 0 mono img (none) 1 background img (none)","title":"Inputs"},{"location":"autodocs/gradient/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/gradient/#parameters","text":"gradient gradient: list (ordered) x: float (default 0.0) x colour colour (ordered) r: int or float (default 0.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 preset: string (topo, grey, viridis, magma, plasma, inferno, cividis, mako, turbo, rocket) (default None) preset colour legend colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 legendrect legend rectangle if in image (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 100) The width of the rectangle h: integer (default 20) The height of the rectangle vertical: boolean (default False) is legend vertical? fontscale: float (default 10.0) size of legend font thickness: float (default 1.0) thickness of legend border legendPos: string (Left margin, Right margin, Top margin, Bottom margin, In image, None) (default 'In image') legend location normbackground: boolean (default False) normalise background sigfigs: integer (default 6) significant figures Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/histequal/","text":"histequal Description Perform histogram equalisation on all channels of the image separately. Honours ROIs. Currently set to 2000 bins, but I may add a control for that. Ignores DQ bits and uncertainty Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters histequal Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"histequal"},{"location":"autodocs/histequal/#histequal","text":"","title":"histequal"},{"location":"autodocs/histequal/#description","text":"Perform histogram equalisation on all channels of the image separately. Honours ROIs. Currently set to 2000 bins, but I may add a control for that. Ignores DQ bits and uncertainty","title":"Description"},{"location":"autodocs/histequal/#connections","text":"","title":"Connections"},{"location":"autodocs/histequal/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/histequal/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/histequal/#parameters","text":"histequal Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/histogram/","text":"histogram Description Produce a histogram of intensities for each channel in the data - will be very messy if used on a multispectral image. Will only be performed on ROIs if there are active ROIs. BAD pixels in bands will be discounted. The output carries a table - columns are frequencies, rows are bands. Uncertainty is ignored. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 data data a CSV output (use 'dump' or 'sink' to read it) Parameters histogram bincount: integer (default 16) Number of bins in the histogram Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"histogram"},{"location":"autodocs/histogram/#histogram","text":"","title":"histogram"},{"location":"autodocs/histogram/#description","text":"Produce a histogram of intensities for each channel in the data - will be very messy if used on a multispectral image. Will only be performed on ROIs if there are active ROIs. BAD pixels in bands will be discounted. The output carries a table - columns are frequencies, rows are bands. Uncertainty is ignored.","title":"Description"},{"location":"autodocs/histogram/#connections","text":"","title":"Connections"},{"location":"autodocs/histogram/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/histogram/#outputs","text":"Index Name Type Desc 0 data data a CSV output (use 'dump' or 'sink' to read it)","title":"Outputs"},{"location":"autodocs/histogram/#parameters","text":"histogram bincount: integer (default 16) Number of bins in the histogram Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/importroi/","text":"importroi Description Import a ROI into an image which was originally set on another image. The 'roi' input takes either an ROI or an image. If the former, that ROI is imposed on the image passed into the main input. If the latter, all the ROIs from the 'roi' input image are imposed on the image input image. Connections Inputs Index Name Type Desc 0 (none) img (none) 1 roi any (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters importroi new_name: string (default '') new name for the ROI if non-empty new_colour the new colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 recolour: boolean (default False) change colour to new colour Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"importroi"},{"location":"autodocs/importroi/#importroi","text":"","title":"importroi"},{"location":"autodocs/importroi/#description","text":"Import a ROI into an image which was originally set on another image. The 'roi' input takes either an ROI or an image. If the former, that ROI is imposed on the image passed into the main input. If the latter, all the ROIs from the 'roi' input image are imposed on the image input image.","title":"Description"},{"location":"autodocs/importroi/#connections","text":"","title":"Connections"},{"location":"autodocs/importroi/#inputs","text":"Index Name Type Desc 0 (none) img (none) 1 roi any (none)","title":"Inputs"},{"location":"autodocs/importroi/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/importroi/#parameters","text":"importroi new_name: string (default '') new name for the ROI if non-empty new_colour the new colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 recolour: boolean (default False) change colour to new colour Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/in/","text":"in Description The macro input connector (used inside macro prototypes) Connections Outputs Index Name Type Desc 0 (none) variant (none) Parameters in Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"in"},{"location":"autodocs/in/#in","text":"","title":"in"},{"location":"autodocs/in/#description","text":"The macro input connector (used inside macro prototypes)","title":"Description"},{"location":"autodocs/in/#connections","text":"","title":"Connections"},{"location":"autodocs/in/#outputs","text":"Index Name Type Desc 0 (none) variant (none)","title":"Outputs"},{"location":"autodocs/in/#parameters","text":"in Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/input_0/","text":"input 0 Description Imports Input 0's data into the graph Connections Outputs Index Name Type Desc 0 (none) none (none) Parameters input 0 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"input 0"},{"location":"autodocs/input_0/#input-0","text":"","title":"input 0"},{"location":"autodocs/input_0/#description","text":"Imports Input 0's data into the graph","title":"Description"},{"location":"autodocs/input_0/#connections","text":"","title":"Connections"},{"location":"autodocs/input_0/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"autodocs/input_0/#parameters","text":"input 0 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/input_1/","text":"input 1 Description Imports Input 1's data into the graph Connections Outputs Index Name Type Desc 0 (none) none (none) Parameters input 1 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"input 1"},{"location":"autodocs/input_1/#input-1","text":"","title":"input 1"},{"location":"autodocs/input_1/#description","text":"Imports Input 1's data into the graph","title":"Description"},{"location":"autodocs/input_1/#connections","text":"","title":"Connections"},{"location":"autodocs/input_1/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"autodocs/input_1/#parameters","text":"input 1 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/input_2/","text":"input 2 Description Imports Input 2's data into the graph Connections Outputs Index Name Type Desc 0 (none) none (none) Parameters input 2 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"input 2"},{"location":"autodocs/input_2/#input-2","text":"","title":"input 2"},{"location":"autodocs/input_2/#description","text":"Imports Input 2's data into the graph","title":"Description"},{"location":"autodocs/input_2/#connections","text":"","title":"Connections"},{"location":"autodocs/input_2/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"autodocs/input_2/#parameters","text":"input 2 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/input_3/","text":"input 3 Description Imports Input 3's data into the graph Connections Outputs Index Name Type Desc 0 (none) none (none) Parameters input 3 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"input 3"},{"location":"autodocs/input_3/#input-3","text":"","title":"input 3"},{"location":"autodocs/input_3/#description","text":"Imports Input 3's data into the graph","title":"Description"},{"location":"autodocs/input_3/#connections","text":"","title":"Connections"},{"location":"autodocs/input_3/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"autodocs/input_3/#parameters","text":"input 3 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/inset/","text":"inset Description Inset an image inside another. Uses RGB versions of both images, as defined by the RGB mapping set in the previous nodes. Does not honour regions of interest. Note that there is no RGB mapping in the canvas for the tab - RGB mappings should be set in the input nodes. The rectangle can be set either from an ROI or from a rectangle which can be drawn on the canvas. If neither is set, no insetting will be done and only the background will be shown. The rectangle can be cleared by clicking \"Clear rect\" in the tab, but any ROI takes priority. Ignores DQ and uncertainty Connections Inputs Index Name Type Desc 0 img img (none) 1 inset img (none) 2 roi roi (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters inset insetrect The rectangle in which to inset the image (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default -1) The width of the rectangle h: integer (default -1) The height of the rectangle caption: string (default '') Caption to put on the inset captiontop: boolean (default False) Put the caption at the top of the inset fontsize: integer (default 10) Font size for the caption thickness: integer (default 2) Thickness of the border colour Colour of the border and caption (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"inset"},{"location":"autodocs/inset/#inset","text":"","title":"inset"},{"location":"autodocs/inset/#description","text":"Inset an image inside another. Uses RGB versions of both images, as defined by the RGB mapping set in the previous nodes. Does not honour regions of interest. Note that there is no RGB mapping in the canvas for the tab - RGB mappings should be set in the input nodes. The rectangle can be set either from an ROI or from a rectangle which can be drawn on the canvas. If neither is set, no insetting will be done and only the background will be shown. The rectangle can be cleared by clicking \"Clear rect\" in the tab, but any ROI takes priority. Ignores DQ and uncertainty","title":"Description"},{"location":"autodocs/inset/#connections","text":"","title":"Connections"},{"location":"autodocs/inset/#inputs","text":"Index Name Type Desc 0 img img (none) 1 inset img (none) 2 roi roi (none)","title":"Inputs"},{"location":"autodocs/inset/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/inset/#parameters","text":"inset insetrect The rectangle in which to inset the image (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default -1) The width of the rectangle h: integer (default -1) The height of the rectangle caption: string (default '') Caption to put on the inset captiontop: boolean (default False) Put the caption at the top of the inset fontsize: integer (default 10) Font size for the caption thickness: integer (default 2) Thickness of the border colour Colour of the border and caption (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/manual_register/","text":"manual register Description Perform manual registration of two images. One input image is designated as \"fixed\", the other as \"moving\". The node will find a transform that maps the moving image onto the fixed image, and a translation that will ensure that both images remain uncropped. The fixed image will be translated only, the moving image will be transformed and then translated. Both transformed images are output and can then be merged or overlaid in a separate node. The transform is found by designating points in each image which correspond to each other. The number of points must be the same in each image, and at least three (unless \"translate only\" is selected). The canvas view can show the moving input, the fixed image, the transformed moving image, or the transformed fixed image. All images are shown as greyscale (since the input images will likely have different frequency bands). The transform used is one of the following homographies: Euclidean - translation and rotation only Similarity - translation, rotation, and scaling; angles are preserved Affine - translation, rotation, scaling, and shearing; parallel lines are preserved Projective - translation, rotation, scaling, shearing, and perspective; straight lines are preserved Points are added to the moving image by clicking with ctrl. Points are adding to the fixed image by clicking with shift. If only the moving or fixed points are shown, either shift- or ctrl-clicking will add to the appropriate point set. The selected point can be deleted with the Delete key (but this will modify the numbering!) Note that this node does not currently display DQ or uncertainty data in its canvas A point can be selected and dragged by clicking on it. This may be slow because the warping operation will take place every update; disabling 'auto-run on change' is a good idea! Uncertainty is warped along with the original image, as is DQ using nearest-neighbour ( which may not be sufficient ). Connections Inputs Index Name Type Desc 0 moving img (none) 1 fixed img (none) Outputs Index Name Type Desc 0 moving img (none) 1 fixed img (none) Parameters manual register showMoving: boolean (default True) Show moving points showFixed: boolean (default True) Show fixed points translate: boolean (default False) Translate only - no other transform. Uses a single point. moving: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate fixed: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate transform: string (euclidean, similarity, affine, projective) (default 'euclidean') Transform type Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"manual register"},{"location":"autodocs/manual_register/#manual-register","text":"","title":"manual register"},{"location":"autodocs/manual_register/#description","text":"Perform manual registration of two images. One input image is designated as \"fixed\", the other as \"moving\". The node will find a transform that maps the moving image onto the fixed image, and a translation that will ensure that both images remain uncropped. The fixed image will be translated only, the moving image will be transformed and then translated. Both transformed images are output and can then be merged or overlaid in a separate node. The transform is found by designating points in each image which correspond to each other. The number of points must be the same in each image, and at least three (unless \"translate only\" is selected). The canvas view can show the moving input, the fixed image, the transformed moving image, or the transformed fixed image. All images are shown as greyscale (since the input images will likely have different frequency bands). The transform used is one of the following homographies: Euclidean - translation and rotation only Similarity - translation, rotation, and scaling; angles are preserved Affine - translation, rotation, scaling, and shearing; parallel lines are preserved Projective - translation, rotation, scaling, shearing, and perspective; straight lines are preserved Points are added to the moving image by clicking with ctrl. Points are adding to the fixed image by clicking with shift. If only the moving or fixed points are shown, either shift- or ctrl-clicking will add to the appropriate point set. The selected point can be deleted with the Delete key (but this will modify the numbering!) Note that this node does not currently display DQ or uncertainty data in its canvas A point can be selected and dragged by clicking on it. This may be slow because the warping operation will take place every update; disabling 'auto-run on change' is a good idea! Uncertainty is warped along with the original image, as is DQ using nearest-neighbour ( which may not be sufficient ).","title":"Description"},{"location":"autodocs/manual_register/#connections","text":"","title":"Connections"},{"location":"autodocs/manual_register/#inputs","text":"Index Name Type Desc 0 moving img (none) 1 fixed img (none)","title":"Inputs"},{"location":"autodocs/manual_register/#outputs","text":"Index Name Type Desc 0 moving img (none) 1 fixed img (none)","title":"Outputs"},{"location":"autodocs/manual_register/#parameters","text":"manual register showMoving: boolean (default True) Show moving points showFixed: boolean (default True) Show fixed points translate: boolean (default False) Translate only - no other transform. Uses a single point. moving: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate fixed: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate transform: string (euclidean, similarity, affine, projective) (default 'euclidean') Transform type Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/mergetests/","text":"mergetests Description Merge the results of many tests into a single list of failures. Test results are always lists of test failures, this simply concatenates those lists. Connections Inputs Index Name Type Desc 0 (none) testresult (none) 1 (none) testresult (none) 2 (none) testresult (none) 3 (none) testresult (none) 4 (none) testresult (none) 5 (none) testresult (none) 6 (none) testresult (none) 7 (none) testresult (none) Outputs Index Name Type Desc 0 results testresult (none) Parameters mergetests Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"mergetests"},{"location":"autodocs/mergetests/#mergetests","text":"","title":"mergetests"},{"location":"autodocs/mergetests/#description","text":"Merge the results of many tests into a single list of failures. Test results are always lists of test failures, this simply concatenates those lists.","title":"Description"},{"location":"autodocs/mergetests/#connections","text":"","title":"Connections"},{"location":"autodocs/mergetests/#inputs","text":"Index Name Type Desc 0 (none) testresult (none) 1 (none) testresult (none) 2 (none) testresult (none) 3 (none) testresult (none) 4 (none) testresult (none) 5 (none) testresult (none) 6 (none) testresult (none) 7 (none) testresult (none)","title":"Inputs"},{"location":"autodocs/mergetests/#outputs","text":"Index Name Type Desc 0 results testresult (none)","title":"Outputs"},{"location":"autodocs/mergetests/#parameters","text":"mergetests Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/multidot/","text":"multidot Description Add multiple small ROIs which are either circular or painted. Painted modes can be created and edited with a circular brush or a flood fill. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROIs on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. This can also \"capture\" ROIs from the incoming image, so that they can be edited. This copies the ROIs from the image into the node, and suppresses the image's original ROIs. In addition to this, the \"convert circles\" button will convert all circular ROIs in the node into painted ROIs. Quick guide: To add and edit circular ROIs: select \"Circles\" on the left-hand side set the dot size to the desired radius shift-click to add and select a new ROI click to select an existing ROI (or deselect) drag to move the centre of the circle edit parameters like dot size, name, colour, etc. to change the current ROI or next created ROI To add and edit painted ROIs: select \"Painted\" on the left-hand side set the dot size to the desired radius set add/create mode to Brush shift-click to add and select a new painted ROI click inside an ROI to select it ctrl-click to add a circle to a selected painted ROI alt-click to \"unpaint\" a circle from a selected painted ROI To add and edit filled ROIs: select \"Painted\" on the left-hand side set add/create mode to Fill set tolerance to a low number (e.g. 0.1) shift-click to add and select a new filled ROI possibly undo (ctrl-Z) to remove the last fill, then change the tolerance! click inside an ROI to select it ctrl-click to add more flood fill to a selected ROI set add/create mode to \"Brush\" to paint circular brushstrokes on a ROI alt-click to \"unpaint\" a circle from a selected ROI General controls: Circles or Painted selects the type of new ROIs click inside an ROI (or very near a circle) to show and edit its properties Dot size is the size of the circle used for both creating circle ROIs and for circular painting in Painted mode. Scale is the font size for all annotations created by this node Thickness is the border size for (currently) all circles only Colour is the colour of the current ROI's annotation Recolour all will select random colours for all ROIs Name is the name of the current ROI Background is whether a background rectangle is used to make the name clearer for all ROIs Capture captures the ROIs from the incoming image, and suppresses the image's original ROIs Convert circles will convert all circular ROIs in the node into painted ROIs. tolerance is the colour difference between the current pixel and surrounding pixels required to stop flood filling. PICK CAREFULLY - it may need to be very small. add/create mode is whether we are new ROIs are created with a circular brush or flood fill in Painted mode Circle mode: shift-click to add a new ROI drag to move centre of circle Painted mode: shift-click to add a new painted ROI. Will use a circle if \"Paint Mode\" is circle, or a flood fill with the given tolerance if the mode is \"Fill\". ctrl-click to add a circle or flood fill to a selected painted ROI, provided we are in the same mode as the selected ROI. Circle or fill depends on Paint Mode. alt-click to \"unpaint\" a circle from a selected painted ROI (Internal: Note that this type doesn't inherit from XFormROI.) Connections Inputs Index Name Type Desc 0 input img (none) Outputs Index Name Type Desc 0 img img image with ROIs Parameters multidot rois: list type = painted type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius type = circle type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"multidot"},{"location":"autodocs/multidot/#multidot","text":"","title":"multidot"},{"location":"autodocs/multidot/#description","text":"Add multiple small ROIs which are either circular or painted. Painted modes can be created and edited with a circular brush or a flood fill. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROIs on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. This can also \"capture\" ROIs from the incoming image, so that they can be edited. This copies the ROIs from the image into the node, and suppresses the image's original ROIs. In addition to this, the \"convert circles\" button will convert all circular ROIs in the node into painted ROIs.","title":"Description"},{"location":"autodocs/multidot/#connections","text":"","title":"Connections"},{"location":"autodocs/multidot/#inputs","text":"Index Name Type Desc 0 input img (none)","title":"Inputs"},{"location":"autodocs/multidot/#outputs","text":"Index Name Type Desc 0 img img image with ROIs","title":"Outputs"},{"location":"autodocs/multidot/#parameters","text":"multidot rois: list type = painted type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius type = circle type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/normimage/","text":"normimage Description Normalise the image to a single range taken from all channels. Honours ROIs. If you need to normalise each channel separately, use the norm() function in the \"expr\" node which has an optional argument for this. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters normimage mode: integer (default 0) Mode - nonzero means clamp, zero means normalise Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"normimage"},{"location":"autodocs/normimage/#normimage","text":"","title":"normimage"},{"location":"autodocs/normimage/#description","text":"Normalise the image to a single range taken from all channels. Honours ROIs. If you need to normalise each channel separately, use the norm() function in the \"expr\" node which has an optional argument for this.","title":"Description"},{"location":"autodocs/normimage/#connections","text":"","title":"Connections"},{"location":"autodocs/normimage/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/normimage/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/normimage/#parameters","text":"normimage mode: integer (default 0) Mode - nonzero means clamp, zero means normalise Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/offset/","text":"offset Description offset an image. Will create a zero band on one edge and clip on the opposite. ROIs are not honoured, but are passed through. Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters offset x: integer (default 0) X offset y: integer (default 0) Y offset Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"offset"},{"location":"autodocs/offset/#offset","text":"","title":"offset"},{"location":"autodocs/offset/#description","text":"offset an image. Will create a zero band on one edge and clip on the opposite. ROIs are not honoured, but are passed through.","title":"Description"},{"location":"autodocs/offset/#connections","text":"","title":"Connections"},{"location":"autodocs/offset/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/offset/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/offset/#parameters","text":"offset x: integer (default 0) X offset y: integer (default 0) Y offset Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/out/","text":"out Description The macro output connector (used inside macro prototypes) Connections Inputs Index Name Type Desc 0 (none) variant (none) Parameters out Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"out"},{"location":"autodocs/out/#out","text":"","title":"out"},{"location":"autodocs/out/#description","text":"The macro output connector (used inside macro prototypes)","title":"Description"},{"location":"autodocs/out/#connections","text":"","title":"Connections"},{"location":"autodocs/out/#inputs","text":"Index Name Type Desc 0 (none) variant (none)","title":"Inputs"},{"location":"autodocs/out/#parameters","text":"out Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/painted/","text":"painted Description Add a painted ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. Connections Inputs Index Name Type Desc 0 input img (none) Outputs Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest Parameters painted label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"painted"},{"location":"autodocs/painted/#painted","text":"","title":"painted"},{"location":"autodocs/painted/#description","text":"Add a painted ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected.","title":"Description"},{"location":"autodocs/painted/#connections","text":"","title":"Connections"},{"location":"autodocs/painted/#inputs","text":"Index Name Type Desc 0 input img (none)","title":"Inputs"},{"location":"autodocs/painted/#outputs","text":"Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest","title":"Outputs"},{"location":"autodocs/painted/#parameters","text":"painted label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/pct/","text":"pct Description Allows the user to locate the PANCAM Calibration Target in an image by specifying control points, move those control points, and generate ROIs for each patch by floodfill. Connections Inputs Index Name Type Desc 0 img img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters pct Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"pct"},{"location":"autodocs/pct/#pct","text":"","title":"pct"},{"location":"autodocs/pct/#description","text":"Allows the user to locate the PANCAM Calibration Target in an image by specifying control points, move those control points, and generate ROIs for each patch by floodfill.","title":"Description"},{"location":"autodocs/pct/#connections","text":"","title":"Connections"},{"location":"autodocs/pct/#inputs","text":"Index Name Type Desc 0 img img (none)","title":"Inputs"},{"location":"autodocs/pct/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/pct/#parameters","text":"pct Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/pixtest/","text":"pixtest Description Used in testing, but may be useful for running automated tests for users. Contains a table of pixel positions and values and checks them in the input image, flagging any errors. The output is numeric, and is the number of failing tests. Typical setup: * add a set of points for band zero at important places (band zero is the default), using the spectrum view in the canvas if necessary * use the \"duplicate all tests across all bands\" to make all the tests * use the \"set from pixels\" button to set the N, U and DQ values to the values in the image Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 results testresult (none) Parameters pixtest Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"pixtest"},{"location":"autodocs/pixtest/#pixtest","text":"","title":"pixtest"},{"location":"autodocs/pixtest/#description","text":"Used in testing, but may be useful for running automated tests for users. Contains a table of pixel positions and values and checks them in the input image, flagging any errors. The output is numeric, and is the number of failing tests. Typical setup: * add a set of points for band zero at important places (band zero is the default), using the spectrum view in the canvas if necessary * use the \"duplicate all tests across all bands\" to make all the tests * use the \"set from pixels\" button to set the N, U and DQ values to the values in the image","title":"Description"},{"location":"autodocs/pixtest/#connections","text":"","title":"Connections"},{"location":"autodocs/pixtest/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/pixtest/#outputs","text":"Index Name Type Desc 0 results testresult (none)","title":"Outputs"},{"location":"autodocs/pixtest/#parameters","text":"pixtest Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/poly/","text":"poly Description Add a polygonal ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. Connections Inputs Index Name Type Desc 0 input img (none) Outputs Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest Parameters poly label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) points: list (ordered) x: int or float (default 0.0) x y: int or float (default 0.0) y drawPoints: boolean (default True) should we draw points Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"poly"},{"location":"autodocs/poly/#poly","text":"","title":"poly"},{"location":"autodocs/poly/#description","text":"Add a polygonal ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected.","title":"Description"},{"location":"autodocs/poly/#connections","text":"","title":"Connections"},{"location":"autodocs/poly/#inputs","text":"Index Name Type Desc 0 input img (none)","title":"Inputs"},{"location":"autodocs/poly/#outputs","text":"Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest","title":"Outputs"},{"location":"autodocs/poly/#parameters","text":"poly label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) points: list (ordered) x: int or float (default 0.0) x y: int or float (default 0.0) y drawPoints: boolean (default True) should we draw points Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/rect/","text":"rect Description Add a rectangular ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected. Connections Inputs Index Name Type Desc 0 input img (none) Outputs Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest Parameters rect label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bb rectangle (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"rect"},{"location":"autodocs/rect/#rect","text":"","title":"rect"},{"location":"autodocs/rect/#description","text":"Add a rectangular ROI to an image. Most subsequent operations will only be performed on the union of all regions of interest. Also outputs an RGB image annotated with the ROI on the 'ann' RGB input, or the input image converted to RGB if that input is not connected.","title":"Description"},{"location":"autodocs/rect/#connections","text":"","title":"Connections"},{"location":"autodocs/rect/#inputs","text":"Index Name Type Desc 0 input img (none)","title":"Inputs"},{"location":"autodocs/rect/#outputs","text":"Index Name Type Desc 0 img img image with ROI 1 roi roi the region of interest","title":"Outputs"},{"location":"autodocs/rect/#parameters","text":"rect label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bb rectangle (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/reflectance/","text":"reflectance Description Given an image which has source data and a set of labelled ROIs (regions of interest), generate gradient and intercept values to correct the image to reflectance values. The ROIs must correspond to calibration target patches in the image. The calibration target can be selected by the user. The image must know what camera and filters it came from, and the camera must have filter information including nominal reflectances for each patch on that target. The outputs are the gradient and intercept of the fit for each filter. All source data is removed, so it does not obscure image data in subsequent operations. The next operation should be an expr node performing out=(in-c)/m. Connections Inputs Index Name Type Desc 0 img img (none) Outputs Index Name Type Desc 0 m number (none) 1 c number (none) Parameters reflectance target: string (default None) The calibration target to use show_patches: boolean (default True) Show the patch names on the plot zero_fudge: boolean (default False) Add an extra zero point simpler_data_fudge: boolean (default False) Force all points to have same number of pixels and same SD Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"reflectance"},{"location":"autodocs/reflectance/#reflectance","text":"","title":"reflectance"},{"location":"autodocs/reflectance/#description","text":"Given an image which has source data and a set of labelled ROIs (regions of interest), generate gradient and intercept values to correct the image to reflectance values. The ROIs must correspond to calibration target patches in the image. The calibration target can be selected by the user. The image must know what camera and filters it came from, and the camera must have filter information including nominal reflectances for each patch on that target. The outputs are the gradient and intercept of the fit for each filter. All source data is removed, so it does not obscure image data in subsequent operations. The next operation should be an expr node performing out=(in-c)/m.","title":"Description"},{"location":"autodocs/reflectance/#connections","text":"","title":"Connections"},{"location":"autodocs/reflectance/#inputs","text":"Index Name Type Desc 0 img img (none)","title":"Inputs"},{"location":"autodocs/reflectance/#outputs","text":"Index Name Type Desc 0 m number (none) 1 c number (none)","title":"Outputs"},{"location":"autodocs/reflectance/#parameters","text":"reflectance target: string (default None) The calibration target to use show_patches: boolean (default True) Show the patch names on the plot zero_fudge: boolean (default False) Add an extra zero point simpler_data_fudge: boolean (default False) Force all points to have same number of pixels and same SD Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/roicull/","text":"roicull Description This node allows certain ROIs to be removed from the input image by name. Connections Inputs Index Name Type Desc 0 img img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters roicull rois: list of string (default '[]') ROIs to Cull cullbad: boolean (default False) Cull bad ROIs (ROIs with all pixels BAD in any band) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"roicull"},{"location":"autodocs/roicull/#roicull","text":"","title":"roicull"},{"location":"autodocs/roicull/#description","text":"This node allows certain ROIs to be removed from the input image by name.","title":"Description"},{"location":"autodocs/roicull/#connections","text":"","title":"Connections"},{"location":"autodocs/roicull/#inputs","text":"Index Name Type Desc 0 img img (none)","title":"Inputs"},{"location":"autodocs/roicull/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/roicull/#parameters","text":"roicull rois: list of string (default '[]') ROIs to Cull cullbad: boolean (default False) Cull bad ROIs (ROIs with all pixels BAD in any band) Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/roidq/","text":"roidq Description Automatically generate an ROI from DQ bits in a band or in all bands. **WARNING**: the ROI will be generated from DQ data from any bands in this image. It can then be applied to any other image or band - but this information is not tracked by the source mechanism. This means that some source tracking information can be lost. (Issue #68) Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 img img (none) 1 roi roi (none) Parameters roidq caption: string (default 'unknown') Caption captiontop: boolean (default False) Caption goes on top? fontsize: integer (default 10) Font size thickness: integer (default 2) Line thickness colour Colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 drawbg: boolean (default True) Draw background? band: integer (default -2) Band (-2 for all, -1 for any) dq: string (default 'DsZ?CE') DQ bits (as characters, e.g. su for SAT+NODATA) condition: string (allpresent, somepresent, allabsent, someabsent) (default 'somepresent') Condition Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"roidq"},{"location":"autodocs/roidq/#roidq","text":"","title":"roidq"},{"location":"autodocs/roidq/#description","text":"Automatically generate an ROI from DQ bits in a band or in all bands. **WARNING**: the ROI will be generated from DQ data from any bands in this image. It can then be applied to any other image or band - but this information is not tracked by the source mechanism. This means that some source tracking information can be lost. (Issue #68)","title":"Description"},{"location":"autodocs/roidq/#connections","text":"","title":"Connections"},{"location":"autodocs/roidq/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/roidq/#outputs","text":"Index Name Type Desc 0 img img (none) 1 roi roi (none)","title":"Outputs"},{"location":"autodocs/roidq/#parameters","text":"roidq caption: string (default 'unknown') Caption captiontop: boolean (default False) Caption goes on top? fontsize: integer (default 10) Font size thickness: integer (default 2) Line thickness colour Colour (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 1.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 drawbg: boolean (default True) Draw background? band: integer (default -2) Band (-2 for all, -1 for any) dq: string (default 'DsZ?CE') DQ bits (as characters, e.g. su for SAT+NODATA) condition: string (allpresent, somepresent, allabsent, someabsent) (default 'somepresent') Condition Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/roiexpr/","text":"roiexpr Description This node allows a region of interest to be composed from several regions of interest using an expression and imposed on an image. It is not a node for creating several ROIs at once - the output is always a single ROI . ROIs can be created for use within the expression by using the \"Add ROI\" button. These will be assigned to the variables a,b,c.. within the expression, and can be edited by: clicking on their label in the left-most column of the table (to select the entire row) and then clicking and dragging on the canvas, double clicking on the description text in the table to open a numerical editor (not for poly or painted). Additional ROIs can be connected to the p, q, r inputs; these will be assigned to those variables within the expression. The input image's ROIs are combined into a single ROI and assigned to the variable 'i'. The input image itself is available as the variable 'img'. Other properties of the image are available and other calculations may be made, but the result of the expression must be an ROI. Examples: a+b : the union of ROIs 'a' and 'b' from the node's ROI list a*b : the intersection of ROIs 'a' and 'b' a-b : ROI 'a' with ROI 'b' removed -a : the negative of ROI 'a' (i.e. the entire image area as an ROI but with a hole in it) roi(img) - p : any ROIs on the image already, but with the ROI on input 'p' cut out Connections Inputs Index Name Type Desc 0 (none) img Image input 1 p roi ROI which appears as 'p' in expression 2 q roi ROI which appears as 'q' in expression 3 r roi ROI which appears as 'r' in expression Outputs Index Name Type Desc 0 (none) img Output image with ROI from expression result imposed 1 (none) roi The ROI generated from the expression Parameters roiexpr rois: list type = painted type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius type = circle type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius type = poly type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) points: list (ordered) x: int or float (default 0.0) x y: int or float (default 0.0) y drawPoints: boolean (default True) should we draw points type = rect type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bb rectangle (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle expr: string (default '') Expression Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"roiexpr"},{"location":"autodocs/roiexpr/#roiexpr","text":"","title":"roiexpr"},{"location":"autodocs/roiexpr/#description","text":"This node allows a region of interest to be composed from several regions of interest using an expression and imposed on an image. It is not a node for creating several ROIs at once - the output is always a single ROI . ROIs can be created for use within the expression by using the \"Add ROI\" button. These will be assigned to the variables a,b,c.. within the expression, and can be edited by: clicking on their label in the left-most column of the table (to select the entire row) and then clicking and dragging on the canvas, double clicking on the description text in the table to open a numerical editor (not for poly or painted). Additional ROIs can be connected to the p, q, r inputs; these will be assigned to those variables within the expression. The input image's ROIs are combined into a single ROI and assigned to the variable 'i'. The input image itself is available as the variable 'img'. Other properties of the image are available and other calculations may be made, but the result of the expression must be an ROI. Examples: a+b : the union of ROIs 'a' and 'b' from the node's ROI list a*b : the intersection of ROIs 'a' and 'b' a-b : ROI 'a' with ROI 'b' removed -a : the negative of ROI 'a' (i.e. the entire image area as an ROI but with a hole in it) roi(img) - p : any ROIs on the image already, but with the ROI on input 'p' cut out","title":"Description"},{"location":"autodocs/roiexpr/#connections","text":"","title":"Connections"},{"location":"autodocs/roiexpr/#inputs","text":"Index Name Type Desc 0 (none) img Image input 1 p roi ROI which appears as 'p' in expression 2 q roi ROI which appears as 'q' in expression 3 r roi ROI which appears as 'r' in expression","title":"Inputs"},{"location":"autodocs/roiexpr/#outputs","text":"Index Name Type Desc 0 (none) img Output image with ROI from expression result imposed 1 (none) roi The ROI generated from the expression","title":"Outputs"},{"location":"autodocs/roiexpr/#parameters","text":"roiexpr rois: list type = painted type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bbrect bounding box (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle map: ndarray (default None) mask r: int or float (default 10) brush radius type = circle type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius type = poly type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) points: list (ordered) x: int or float (default 0.0) x y: int or float (default 0.0) y drawPoints: boolean (default True) should we draw points type = rect type: string (default '') type of the ROI label: string (default '') the ROI's label labeltop: boolean (default False) should the label be above or below the ROI when rendered? colour the colour of the ROI (ordered) r: int or float (default 1.0) The red component 0-1 g: int or float (default 0.0) The green component 0-1 b: int or float (default 0.0) The blue component 0-1 thickness: int or float (default 0) the border thickness for rendering fontsize: int or float (default 10) the fontsize for rendering drawbg: boolean (default True) should the label's background container be filled with black/white (depending on colour)? drawBox: boolean (default True) draw the ROI bounding box? (painted only) drawEdge: boolean (default True) draw the ROI edge, or fill it? (painted only) bb rectangle (ordered) x: integer (default 0) The x coordinate of the top left corner y: integer (default 0) The y coordinate of the top left corner w: integer (default 0) The width of the rectangle h: integer (default 0) The height of the rectangle expr: string (default '') Expression Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/scalartest/","text":"scalartest Description Test a scalar against a value Connections Inputs Index Name Type Desc 0 (none) number (none) Outputs Index Name Type Desc 0 results testresult (none) Parameters scalartest n: float (default 0.0) Nominal value to test u: float (default 0.0) Uncertainty to test dq: integer (default 0) DQ to test nTest: string (equal, notequal, lessthan, greaterthan, Equals, Not equals, Less than, Greater than) (default 'equal') Test to apply to nominal uTest: string (equal, notequal, lessthan, greaterthan, Equals, Not equals, Less than, Greater than) (default 'equal') Test to apply to uncertainty dqTest: string (equal, notequal, contains, notcontains, Equals, Does not equal, Contains, Does not contain) (default 'equal') Test to apply to DQ Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"scalartest"},{"location":"autodocs/scalartest/#scalartest","text":"","title":"scalartest"},{"location":"autodocs/scalartest/#description","text":"Test a scalar against a value","title":"Description"},{"location":"autodocs/scalartest/#connections","text":"","title":"Connections"},{"location":"autodocs/scalartest/#inputs","text":"Index Name Type Desc 0 (none) number (none)","title":"Inputs"},{"location":"autodocs/scalartest/#outputs","text":"Index Name Type Desc 0 results testresult (none)","title":"Outputs"},{"location":"autodocs/scalartest/#parameters","text":"scalartest n: float (default 0.0) Nominal value to test u: float (default 0.0) Uncertainty to test dq: integer (default 0) DQ to test nTest: string (equal, notequal, lessthan, greaterthan, Equals, Not equals, Less than, Greater than) (default 'equal') Test to apply to nominal uTest: string (equal, notequal, lessthan, greaterthan, Equals, Not equals, Less than, Greater than) (default 'equal') Test to apply to uncertainty dqTest: string (equal, notequal, contains, notcontains, Equals, Does not equal, Contains, Does not contain) (default 'equal') Test to apply to DQ Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/sink/","text":"sink Description This provides a simple way to view any kind of data - images will be shown on a canvas, other data will be converted to text. Connections Inputs Index Name Type Desc 0 (none) any (none) Parameters sink Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"sink"},{"location":"autodocs/sink/#sink","text":"","title":"sink"},{"location":"autodocs/sink/#description","text":"This provides a simple way to view any kind of data - images will be shown on a canvas, other data will be converted to text.","title":"Description"},{"location":"autodocs/sink/#connections","text":"","title":"Connections"},{"location":"autodocs/sink/#inputs","text":"Index Name Type Desc 0 (none) any (none)","title":"Inputs"},{"location":"autodocs/sink/#parameters","text":"sink Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/spectrum/","text":"spectrum Description Show the mean intensities for each frequency band in each region of interest (ROI) in each input. If an input has no ROI, the intensities of all the pixels in the input are used. It's quite possible for the different inputs to be different images, to permit comparison. Each region (or input) has a separate line in the resulting plot, labelled with the annotation for the ROI (or \"inputN\" for an input with no ROI). If ROIs in different inputs have the same annotation, they are labelled as \"inN:annotation\" where N is the input number. Each pixel has its own variance, so the shown variance is the pooled variance of all the pixels in the region. This is calculated as the variance of the means, plus the mean of the variances (Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). We assume the number of samples that went into each pixel is the same. For those who might want to work with a library, SpectrumSet handles this part of the operation. If a point has data with BAD DQ bits in a band, those pixels are ignored in that band. If there are no good points, the point is not plotted for that band. A table of the values is also produced, and this output as a table datum. The table has one row per ROI or input, and the columns name - the name of the ROI or input m wavelength - the mean intensity for the given wavelength band s wavelength - the population standard deviation of the mean intensity for the given wavelength band p wavelength - the number of pixels in the given wavelength band (usually the same as the number of pixels in the ROI, but may be fewer if the ROI has \"bad\" pixels in that band) The last two columns are repeated for each wavelength band. Connections Inputs Index Name Type Desc 0 0 img a single line in the plot 1 1 img a single line in the plot 2 2 img a single line in the plot 3 3 img a single line in the plot 4 4 img a single line in the plot 5 5 img a single line in the plot 6 6 img a single line in the plot 7 7 img a single line in the plot Outputs Index Name Type Desc 0 data data a CSV output (use 'dump' or 'sink' to read it) Parameters spectrum sortlist: list of string (default '[]') List of inputs to sort by legendFontSize: integer (default 8) Legend font size axisFontSize: integer (default 8) Axis font size labelFontSize: integer (default 12) Label font size bottomSpace: integer (default 0) Bottom space rightSpace: integer (default 0) Right space stackSep: integer (default 0) Stack separation errorbarmode: string (none, stderror, stddev) (default 'stddev') Error bar mode colourmode: string (fromROIs, scheme1, scheme2) (default 'fromROIs') Colour mode bandwidthmode: string (none, errorbar, vertbar) (default 'none') Bandwidth mode ignorePixSD: boolean (default False) Ignore pixel standard deviation Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"spectrum"},{"location":"autodocs/spectrum/#spectrum","text":"","title":"spectrum"},{"location":"autodocs/spectrum/#description","text":"Show the mean intensities for each frequency band in each region of interest (ROI) in each input. If an input has no ROI, the intensities of all the pixels in the input are used. It's quite possible for the different inputs to be different images, to permit comparison. Each region (or input) has a separate line in the resulting plot, labelled with the annotation for the ROI (or \"inputN\" for an input with no ROI). If ROIs in different inputs have the same annotation, they are labelled as \"inN:annotation\" where N is the input number. Each pixel has its own variance, so the shown variance is the pooled variance of all the pixels in the region. This is calculated as the variance of the means, plus the mean of the variances (Rudmin, J. W. (2010). Calculating the exact pooled variance. arXiv preprint arXiv:1007.1012). We assume the number of samples that went into each pixel is the same. For those who might want to work with a library, SpectrumSet handles this part of the operation. If a point has data with BAD DQ bits in a band, those pixels are ignored in that band. If there are no good points, the point is not plotted for that band. A table of the values is also produced, and this output as a table datum. The table has one row per ROI or input, and the columns name - the name of the ROI or input m wavelength - the mean intensity for the given wavelength band s wavelength - the population standard deviation of the mean intensity for the given wavelength band p wavelength - the number of pixels in the given wavelength band (usually the same as the number of pixels in the ROI, but may be fewer if the ROI has \"bad\" pixels in that band) The last two columns are repeated for each wavelength band.","title":"Description"},{"location":"autodocs/spectrum/#connections","text":"","title":"Connections"},{"location":"autodocs/spectrum/#inputs","text":"Index Name Type Desc 0 0 img a single line in the plot 1 1 img a single line in the plot 2 2 img a single line in the plot 3 3 img a single line in the plot 4 4 img a single line in the plot 5 5 img a single line in the plot 6 6 img a single line in the plot 7 7 img a single line in the plot","title":"Inputs"},{"location":"autodocs/spectrum/#outputs","text":"Index Name Type Desc 0 data data a CSV output (use 'dump' or 'sink' to read it)","title":"Outputs"},{"location":"autodocs/spectrum/#parameters","text":"spectrum sortlist: list of string (default '[]') List of inputs to sort by legendFontSize: integer (default 8) Legend font size axisFontSize: integer (default 8) Axis font size labelFontSize: integer (default 12) Label font size bottomSpace: integer (default 0) Bottom space rightSpace: integer (default 0) Right space stackSep: integer (default 0) Stack separation errorbarmode: string (none, stderror, stddev) (default 'stddev') Error bar mode colourmode: string (fromROIs, scheme1, scheme2) (default 'fromROIs') Colour mode bandwidthmode: string (none, errorbar, vertbar) (default 'none') Bandwidth mode ignorePixSD: boolean (default False) Ignore pixel standard deviation Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/stitch/","text":"stitch Description This node performs manual stitching of multiple images into a single image. Connections Inputs Index Name Type Desc 0 0 img Input image 0 1 1 img Input image 1 2 2 img Input image 2 3 3 img Input image 3 4 4 img Input image 4 5 5 img Input image 5 6 6 img Input image 6 7 7 img Input image 7 Outputs Index Name Type Desc 0 (none) img Output image Parameters stitch order: list of integer (default [0, 1, 2, 3, 4, 5, 6, 7]) Order of images showImage: boolean (default True) Show image offsets: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"stitch"},{"location":"autodocs/stitch/#stitch","text":"","title":"stitch"},{"location":"autodocs/stitch/#description","text":"This node performs manual stitching of multiple images into a single image.","title":"Description"},{"location":"autodocs/stitch/#connections","text":"","title":"Connections"},{"location":"autodocs/stitch/#inputs","text":"Index Name Type Desc 0 0 img Input image 0 1 1 img Input image 1 2 2 img Input image 2 3 3 img Input image 3 4 4 img Input image 4 5 5 img Input image 5 6 6 img Input image 6 7 7 img Input image 7","title":"Inputs"},{"location":"autodocs/stitch/#outputs","text":"Index Name Type Desc 0 (none) img Output image","title":"Outputs"},{"location":"autodocs/stitch/#parameters","text":"stitch order: list of integer (default [0, 1, 2, 3, 4, 5, 6, 7]) Order of images showImage: boolean (default True) Show image offsets: list (ordered) x: integer (default 0) X coordinate y: integer (default 0) Y coordinate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/stringtest/","text":"stringtest Description Convert the output of a node into string. Assert that this matches a given string. Both strings are stripped of whitespace and CRLF is converted to LF. Connections Inputs Index Name Type Desc 0 (none) any (none) Outputs Index Name Type Desc 0 (none) testresult (none) Parameters stringtest string: string (default '') String to check for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"stringtest"},{"location":"autodocs/stringtest/#stringtest","text":"","title":"stringtest"},{"location":"autodocs/stringtest/#description","text":"Convert the output of a node into string. Assert that this matches a given string. Both strings are stripped of whitespace and CRLF is converted to LF.","title":"Description"},{"location":"autodocs/stringtest/#connections","text":"","title":"Connections"},{"location":"autodocs/stringtest/#inputs","text":"Index Name Type Desc 0 (none) any (none)","title":"Inputs"},{"location":"autodocs/stringtest/#outputs","text":"Index Name Type Desc 0 (none) testresult (none)","title":"Outputs"},{"location":"autodocs/stringtest/#parameters","text":"stringtest string: string (default '') String to check for Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/striproi/","text":"striproi Description Strip ROIs from an image Connections Inputs Index Name Type Desc 0 (none) img (none) Outputs Index Name Type Desc 0 (none) img (none) Parameters striproi Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"striproi"},{"location":"autodocs/striproi/#striproi","text":"","title":"striproi"},{"location":"autodocs/striproi/#description","text":"Strip ROIs from an image","title":"Description"},{"location":"autodocs/striproi/#connections","text":"","title":"Connections"},{"location":"autodocs/striproi/#inputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Inputs"},{"location":"autodocs/striproi/#outputs","text":"Index Name Type Desc 0 (none) img (none)","title":"Outputs"},{"location":"autodocs/striproi/#parameters","text":"striproi Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"autodocs/tvl1_autoreg/","text":"tvl1 autoreg Description Use the TV-L1 solver to find an optical flow field for transforming one image into another. Not generally advised, and very slow. The node will output a version of the 'moving' image, distorted to map onto the 'fixed' image. Propagates uncertainty of the moving image by distorting that of the source image, and propagates DQs using nearest neighbour. Connections Inputs Index Name Type Desc 0 moving img (none) 1 fixed img (none) Outputs Index Name Type Desc 0 moved img (none) Parameters tvl1 autoreg Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"tvl1 autoreg"},{"location":"autodocs/tvl1_autoreg/#tvl1-autoreg","text":"","title":"tvl1 autoreg"},{"location":"autodocs/tvl1_autoreg/#description","text":"Use the TV-L1 solver to find an optical flow field for transforming one image into another. Not generally advised, and very slow. The node will output a version of the 'moving' image, distorted to map onto the 'fixed' image. Propagates uncertainty of the moving image by distorting that of the source image, and propagates DQs using nearest neighbour.","title":"Description"},{"location":"autodocs/tvl1_autoreg/#connections","text":"","title":"Connections"},{"location":"autodocs/tvl1_autoreg/#inputs","text":"Index Name Type Desc 0 moving img (none) 1 fixed img (none)","title":"Inputs"},{"location":"autodocs/tvl1_autoreg/#outputs","text":"Index Name Type Desc 0 moved img (none)","title":"Outputs"},{"location":"autodocs/tvl1_autoreg/#parameters","text":"tvl1 autoreg Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"devguide/","text":"Overview These pages describe how to develop plugins for PCOT and use PCOT as a library, and may also contain various internal developer notes. Much of this documentation is quite brief - for developing plugins and libraries it's a good idea to supplement your knowledge by looking at examples and reading the source code! Important classes - probably read this first. Using PCOT as a library Writing PCOT plugins Node serialisation Miscellaneous notes on type internals and adding new types . notes on how operations/functions on Datum and Value work test/scratchpad of LaTeX support . How to make a new release","title":"Overview"},{"location":"devguide/#overview","text":"These pages describe how to develop plugins for PCOT and use PCOT as a library, and may also contain various internal developer notes. Much of this documentation is quite brief - for developing plugins and libraries it's a good idea to supplement your knowledge by looking at examples and reading the source code! Important classes - probably read this first. Using PCOT as a library Writing PCOT plugins Node serialisation","title":"Overview"},{"location":"devguide/#miscellaneous","text":"notes on type internals and adding new types . notes on how operations/functions on Datum and Value work test/scratchpad of LaTeX support . How to make a new release","title":"Miscellaneous"},{"location":"devguide/classes/","text":"Important classes pcot.document.Document, and an overview PCOT keeps all its user data in a pcot.document.Document : This contains: a pcot.documentsettings.DocumentSettings object a pcot.inputs.inp.InputManager object handling the inputs a pcot.document.UndoRedoStore which uses the serialisation/deserialisation system to handle an undo stack A dictionary of pcot.macros.XFormMacro objects - the user macros most importantly a pcot.xform.XFormGraph - a set of nodes of type pcot.xform.XForm connected together to do things. Each XForm points to a pcot.xform.XFormType singleton which controls its behaviour. Nodes (XForms) communicate by passing pcot.datum.Datum objects. Each Datum points to a pcot.datumtype.Type subclass singleton providing serialisation, copy, and display facilities. They also have values - two common value classes are pcot.imagecube.ImageCube for images, and pcot.value.Value objects for other arrays and scalars. ImageCubes can generate SubImageCube objects which are subsets of the image covered by ROIs and with bad pixels masked out (\"bad\" according to the data quality bits). pcot.xform.XFormGraph This represents the graph of nodes which take data from inputs and perform operations on them. There is one inside the document, and instances of macros also contain their own graphs (and the macro itself contains a \"template\" graph from which these are created). A graph contains: A set of pcot.xform.XForm objects (usually called nodes ) connected together by their inputs fields, which are tuples of (source node, index of output). A graph runs by finding those nodes which have no inputs and performing them; the entire graph will be recursively traversed. Nodes are only run if their inputs have data (if their parent nodes have run). pcot.xform.XForm All nodes are of the same type. Polymorphism - different nodes behaving differently - is accomplished through each node having a reference to a pcot.xform.XFormType object in its type member that controls its behaviour. Nodes communicate by passing pcot.datum.Datum objects. When a node runs its perform method It reads the inputs by deferencing the inputs fields and getting the input node and index of the output of that node, and reading the Datum stored in that output. It processes the data and stores the results in its outputs field as Datum objects. It then performs its \"child\" nodes. pcot.datum.Datum This is the fundamental data type, used both for communicating between nodes and handling values in the expr node. A Datum has a type , which is a pcot.datumtypes.Type object a value , whose type depends on the type field a source indicating where the data came from If the value is None , the Datum is null. There is a constant Datum.null for null data. pcot.datumtypes.Type The DatumType object provides methods for serialisation, copying, and display. Each is a singleton. It's easy to create custom DatumTypes. The most commonly used builtins are: Datum.IMG : contains an ImageCube Datum.NUMBER : contains a Value (these names are for the singleton objects, not their types - for example, Datum.IMG has the type pcot.datumtypes.ImgType .) pcot.imagecube.ImageCube This is the fundamental image type, consisting of image data - 2D (H x W) if there is only one band (channel), 3D otherwise (H x W x D). Type is float32. uncertainty data , same shape and type as image data. This is the standard deviation of each pixel. DQ data . This a 16-bit bitfield for each pixel. Some of these are considered errors - these are called \"bad\" DQ bits (e.g. no uncertainty, saturated, results from a division by zero). regions of interest annotations that have been added to the image mapping used to render the image as RGB sources for each band pcot.imagecube.SubImageCube These objects can be generated by calling the subimg() method on an ImageCube. They are the subset of an image covered by the regions of interest it has, along with a mask for those regions. Additionally, \"bad\" parts of the image (which can be different in different bands) can be masked out. Operations on images are typically done on these subimages, and then modifyWithSub is called on the imagecube to return a copy of that imagecube with the modified subimage spliced in. Useful subimage methods include: masked() : return the masked nominal image data maskedUncertainty() : return the masked uncertainty data maskedDQ() : return the masked DQ bits All these return numpy masked arrays. pcot.value.Value This is the fundamental numeric type, consisting of nominal value uncertainty value (standard deviation) DQ bits It's usually used for scalars but can also hold array data - internally ImageCubes (or parts of them) are converted into array Values for maths. If it does hold array data, the three elements must be the same shape. It's possible to create 1D vectors in an expr node using square brackets, and some functions and operations return such vectors. This type supports mathematical operations which propagate uncertainty and DQ. More on how Values work here Note : You may wonder why ImageCube and SubImageCube don't use Value internally. The answer is simply historical reasons: they were created a very long time before Value, and refactoring now could cause huge problems.","title":"Important classes"},{"location":"devguide/classes/#important-classes","text":"","title":"Important classes"},{"location":"devguide/classes/#pcotdocumentdocument-and-an-overview","text":"PCOT keeps all its user data in a pcot.document.Document : This contains: a pcot.documentsettings.DocumentSettings object a pcot.inputs.inp.InputManager object handling the inputs a pcot.document.UndoRedoStore which uses the serialisation/deserialisation system to handle an undo stack A dictionary of pcot.macros.XFormMacro objects - the user macros most importantly a pcot.xform.XFormGraph - a set of nodes of type pcot.xform.XForm connected together to do things. Each XForm points to a pcot.xform.XFormType singleton which controls its behaviour. Nodes (XForms) communicate by passing pcot.datum.Datum objects. Each Datum points to a pcot.datumtype.Type subclass singleton providing serialisation, copy, and display facilities. They also have values - two common value classes are pcot.imagecube.ImageCube for images, and pcot.value.Value objects for other arrays and scalars. ImageCubes can generate SubImageCube objects which are subsets of the image covered by ROIs and with bad pixels masked out (\"bad\" according to the data quality bits).","title":"pcot.document.Document, and an overview"},{"location":"devguide/classes/#pcotxformxformgraph","text":"This represents the graph of nodes which take data from inputs and perform operations on them. There is one inside the document, and instances of macros also contain their own graphs (and the macro itself contains a \"template\" graph from which these are created). A graph contains: A set of pcot.xform.XForm objects (usually called nodes ) connected together by their inputs fields, which are tuples of (source node, index of output). A graph runs by finding those nodes which have no inputs and performing them; the entire graph will be recursively traversed. Nodes are only run if their inputs have data (if their parent nodes have run).","title":"pcot.xform.XFormGraph"},{"location":"devguide/classes/#pcotxformxform","text":"All nodes are of the same type. Polymorphism - different nodes behaving differently - is accomplished through each node having a reference to a pcot.xform.XFormType object in its type member that controls its behaviour. Nodes communicate by passing pcot.datum.Datum objects. When a node runs its perform method It reads the inputs by deferencing the inputs fields and getting the input node and index of the output of that node, and reading the Datum stored in that output. It processes the data and stores the results in its outputs field as Datum objects. It then performs its \"child\" nodes.","title":"pcot.xform.XForm"},{"location":"devguide/classes/#pcotdatumdatum","text":"This is the fundamental data type, used both for communicating between nodes and handling values in the expr node. A Datum has a type , which is a pcot.datumtypes.Type object a value , whose type depends on the type field a source indicating where the data came from If the value is None , the Datum is null. There is a constant Datum.null for null data.","title":"pcot.datum.Datum"},{"location":"devguide/classes/#pcotdatumtypestype","text":"The DatumType object provides methods for serialisation, copying, and display. Each is a singleton. It's easy to create custom DatumTypes. The most commonly used builtins are: Datum.IMG : contains an ImageCube Datum.NUMBER : contains a Value (these names are for the singleton objects, not their types - for example, Datum.IMG has the type pcot.datumtypes.ImgType .)","title":"pcot.datumtypes.Type"},{"location":"devguide/classes/#pcotimagecubeimagecube","text":"This is the fundamental image type, consisting of image data - 2D (H x W) if there is only one band (channel), 3D otherwise (H x W x D). Type is float32. uncertainty data , same shape and type as image data. This is the standard deviation of each pixel. DQ data . This a 16-bit bitfield for each pixel. Some of these are considered errors - these are called \"bad\" DQ bits (e.g. no uncertainty, saturated, results from a division by zero). regions of interest annotations that have been added to the image mapping used to render the image as RGB sources for each band","title":"pcot.imagecube.ImageCube"},{"location":"devguide/classes/#pcotimagecubesubimagecube","text":"These objects can be generated by calling the subimg() method on an ImageCube. They are the subset of an image covered by the regions of interest it has, along with a mask for those regions. Additionally, \"bad\" parts of the image (which can be different in different bands) can be masked out. Operations on images are typically done on these subimages, and then modifyWithSub is called on the imagecube to return a copy of that imagecube with the modified subimage spliced in. Useful subimage methods include: masked() : return the masked nominal image data maskedUncertainty() : return the masked uncertainty data maskedDQ() : return the masked DQ bits All these return numpy masked arrays.","title":"pcot.imagecube.SubImageCube"},{"location":"devguide/classes/#pcotvaluevalue","text":"This is the fundamental numeric type, consisting of nominal value uncertainty value (standard deviation) DQ bits It's usually used for scalars but can also hold array data - internally ImageCubes (or parts of them) are converted into array Values for maths. If it does hold array data, the three elements must be the same shape. It's possible to create 1D vectors in an expr node using square brackets, and some functions and operations return such vectors. This type supports mathematical operations which propagate uncertainty and DQ. More on how Values work here Note : You may wonder why ImageCube and SubImageCube don't use Value internally. The answer is simply historical reasons: they were created a very long time before Value, and refactoring now could cause huge problems.","title":"pcot.value.Value"},{"location":"devguide/latex/","text":"A quick test of LaTeX support Please ignore this page - I use it as a scratchpad for various LaTeX tests. It might seem a bit unpleasant to have this as part of the live documentation, but that absolutely guarantees that it works everywhere! LaTeX in these documents is handled with the pymdownx.arithmatex plugin, which really just hands processing off to MathJax. MathJax then renders the LaTeX using JavaScript. Lots and lots of very clever JavaScript. Here are some tests: Inline equation: y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) should work. Inline equation: $y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right)$ should work. Block equations. This has to use double-backslash: \\[ y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) \\] \\\\[ y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) \\\\] but this one doesn't: \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} Align. Note that the reference doesn't work! \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} Matrix. Note I've had to wrap in an equation. \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation} \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation} \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation}","title":"A quick test of LaTeX support"},{"location":"devguide/latex/#a-quick-test-of-latex-support","text":"Please ignore this page - I use it as a scratchpad for various LaTeX tests. It might seem a bit unpleasant to have this as part of the live documentation, but that absolutely guarantees that it works everywhere! LaTeX in these documents is handled with the pymdownx.arithmatex plugin, which really just hands processing off to MathJax. MathJax then renders the LaTeX using JavaScript. Lots and lots of very clever JavaScript. Here are some tests: Inline equation: y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) should work. Inline equation: $y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right)$ should work. Block equations. This has to use double-backslash: \\[ y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) \\] \\\\[ y = \\sigma\\left(b+\\sum_i w_i x_i (1+h) s_i\\right) \\\\] but this one doesn't: \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} \\begin{equation} f(1,0)=1 \\iff w_1 \\ge -b \\label{eq:f101} \\end{equation} Align. Note that the reference doesn't work! \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} \\begin{align} f(x,y) &= H (b+w_1 x + w_2 y)&\\text{(Eq.~\\ref{eq:f101})}\\\\ 0 &= H(b+w_1+w_2)&\\text{(subst.)}\\\\ H(b+w_1+w_2) &= 0\\label{eq:f110in}\\\\ b+w_1+w_2 & < 0 & \\text{(Heaviside step)}\\\\ w_1+w_2 & < -b. \\end{align} Matrix. Note I've had to wrap in an equation. \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation} \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation} \\begin{equation} X= \\left(\\begin{matrix} 1 & 2 & 1\\\\ 2 & 4 & 2\\\\ 1 & 2 & 1 \\end{matrix}\\right) \\end{equation}","title":"A quick test of LaTeX support"},{"location":"devguide/library/","text":"Using PCOT as a library As well as being a stand-alone application, PCOT can be used as a library by other Python programs. This page discusses three ways to do this, although the various elements can be easily blended in a single program: Loading a PCOT document, reading some data and passing it through the document's graph; Building a PCOT document programmatically and passing data through it; Using PCOT functions and data types without a graph. While the latter two techniques can be useful for quick ad-hoc work, on the whole we feel it is better to exchange PCOT documents for traceability and clarity. Loading and running PCOT documents A typical example might be a script to read a PCOT document and run some data through that document's graph. You could do that like this: # This example opens a graph, process some ENVI files through that graph, # and saves them back to an ENVI. It assumes the graph has an \"input 0\" node # which receives an image and a \"sink\" node which receives the processed # image. import pcot from pcot.document import Document from pcot.datum import Datum from pcot.dataformats.envi import write # initialise PCOT pcot.setup() # load the document doc = Document(\"1.pcot\") # run the graph for some ENVI files. We'll just do one here, the ENVI # document contained in the files 1.hdr and 1.dat (an ENVI document # consists of two files: header and data). for file in (\"1\",): # load the given ENVI file into input 0 rv = doc.setInputENVI(0, file+\".hdr\") if rv is not None: raise Exception(f\"{rv}\") # run the document's graph doc.run() # get the \"sink\" node outNode = doc.getNodeByName(\"sink\") # get its output img = outNode.out.get(Datum.IMG) # write to new ENVI, e.g. 1b.hdr write(file+\"b\",img) Building a PCOT document It's also possible to build a PCOT document, creating nodes within its graph. Consider the graph Figure: A simple graph. Click on image to expand. This could be built and run for a particular file with the following code: #!/usr/bin/env python import pcot import pcot.document from pcot.datum import Datum pcot.setup() doc = pcot.document.Document() result = doc.setInputENVI(0, \"/home/white/PCOT/fff.hdr\") is not None assert result is not None # create a document with just an input node in it, to bring that input into the document's graph innode = doc.graph.create(\"input 0\") # add a region of interest (ROI) node to the image roinode = doc.graph.create(\"circle\") # set the circle to be centred at (32,32) with a radius of 3 pixels roinode.roi.set(32,32,3) # connect its first input to the input node's first output # args: # input on this node to connect # node to get connection from # index on that node to connect to. roinode.connect(0,innode,0) # connect a node to the ROI node which takes the resulting image-with-ROI # and plots the spectrum of that ROI specnode = doc.graph.create(\"spectrum\") specnode.connect(0,roinode,0) # run the document doc.run() # get the output of the spectrum node, which will be a Datum, # and dereference the Datum, ensuring that the data is of the right type. # The result will be a Table object. output = specnode.getOutput(0, Datum.DATA) # print the table as CSV. print(output) Using PCOT functions and data types without a graph Often it is much simpler to just use the underlying PCOT data types without a graph. The operation described in the previous section is an example of this. We could use the dataformats.load package to load the data directly and manipulate it: #!/usr/bin/env python import pcot from pcot.datum import Datum from pcot.dataformats import load from pcot.rois import ROICircle from pcot.utils.spectrum import SpectrumSet pcot.setup() # load the ENVI file as a Datum object. Will raise an exception # if there is a problem datum = load.envi(\"/home/white/PCOT/fff.hdr\") # retrieve the image, ensuring it's an IMG datum img = datum.get(Datum.IMG) # add a region of interest (ROI) node to the image: # a circle to be centred at (32,32) with a radius of 3 pixels img.rois.append(ROICircle(32,32,3)) # construct a spectrum set from this image - this can create spectra # for multiple sources and combine them. Here we are just using a single # source - the image we are working with - and we're calling it \"in.\" ss = SpectrumSet({\"in\": img}) # Generate a table from the results and print it (as a CSV table). print(ss.table())","title":"Using PCOT as a library"},{"location":"devguide/library/#using-pcot-as-a-library","text":"As well as being a stand-alone application, PCOT can be used as a library by other Python programs. This page discusses three ways to do this, although the various elements can be easily blended in a single program: Loading a PCOT document, reading some data and passing it through the document's graph; Building a PCOT document programmatically and passing data through it; Using PCOT functions and data types without a graph. While the latter two techniques can be useful for quick ad-hoc work, on the whole we feel it is better to exchange PCOT documents for traceability and clarity.","title":"Using PCOT as a library"},{"location":"devguide/library/#loading-and-running-pcot-documents","text":"A typical example might be a script to read a PCOT document and run some data through that document's graph. You could do that like this: # This example opens a graph, process some ENVI files through that graph, # and saves them back to an ENVI. It assumes the graph has an \"input 0\" node # which receives an image and a \"sink\" node which receives the processed # image. import pcot from pcot.document import Document from pcot.datum import Datum from pcot.dataformats.envi import write # initialise PCOT pcot.setup() # load the document doc = Document(\"1.pcot\") # run the graph for some ENVI files. We'll just do one here, the ENVI # document contained in the files 1.hdr and 1.dat (an ENVI document # consists of two files: header and data). for file in (\"1\",): # load the given ENVI file into input 0 rv = doc.setInputENVI(0, file+\".hdr\") if rv is not None: raise Exception(f\"{rv}\") # run the document's graph doc.run() # get the \"sink\" node outNode = doc.getNodeByName(\"sink\") # get its output img = outNode.out.get(Datum.IMG) # write to new ENVI, e.g. 1b.hdr write(file+\"b\",img)","title":"Loading and running PCOT documents"},{"location":"devguide/library/#building-a-pcot-document","text":"It's also possible to build a PCOT document, creating nodes within its graph. Consider the graph Figure: A simple graph. Click on image to expand. This could be built and run for a particular file with the following code: #!/usr/bin/env python import pcot import pcot.document from pcot.datum import Datum pcot.setup() doc = pcot.document.Document() result = doc.setInputENVI(0, \"/home/white/PCOT/fff.hdr\") is not None assert result is not None # create a document with just an input node in it, to bring that input into the document's graph innode = doc.graph.create(\"input 0\") # add a region of interest (ROI) node to the image roinode = doc.graph.create(\"circle\") # set the circle to be centred at (32,32) with a radius of 3 pixels roinode.roi.set(32,32,3) # connect its first input to the input node's first output # args: # input on this node to connect # node to get connection from # index on that node to connect to. roinode.connect(0,innode,0) # connect a node to the ROI node which takes the resulting image-with-ROI # and plots the spectrum of that ROI specnode = doc.graph.create(\"spectrum\") specnode.connect(0,roinode,0) # run the document doc.run() # get the output of the spectrum node, which will be a Datum, # and dereference the Datum, ensuring that the data is of the right type. # The result will be a Table object. output = specnode.getOutput(0, Datum.DATA) # print the table as CSV. print(output)","title":"Building a PCOT document"},{"location":"devguide/library/#using-pcot-functions-and-data-types-without-a-graph","text":"Often it is much simpler to just use the underlying PCOT data types without a graph. The operation described in the previous section is an example of this. We could use the dataformats.load package to load the data directly and manipulate it: #!/usr/bin/env python import pcot from pcot.datum import Datum from pcot.dataformats import load from pcot.rois import ROICircle from pcot.utils.spectrum import SpectrumSet pcot.setup() # load the ENVI file as a Datum object. Will raise an exception # if there is a problem datum = load.envi(\"/home/white/PCOT/fff.hdr\") # retrieve the image, ensuring it's an IMG datum img = datum.get(Datum.IMG) # add a region of interest (ROI) node to the image: # a circle to be centred at (32,32) with a radius of 3 pixels img.rois.append(ROICircle(32,32,3)) # construct a spectrum set from this image - this can create spectra # for multiple sources and combine them. Here we are just using a single # source - the image we are working with - and we're calling it \"in.\" ss = SpectrumSet({\"in\": img}) # Generate a table from the results and print it (as a CSV table). print(ss.table())","title":"Using PCOT functions and data types without a graph"},{"location":"devguide/nodeser/","text":"Node serialisation This page discusses how to serialise the parameters for node types. If you don't know what that means, you're either in the wrong place or haven't read Writing PCOT plugins . Most nodes need to store some data. Sometimes this data can just be stored directly, as attributes of the XForm object (i.e. the node). It's not really \"polite\" programming, but this is the kind of thing you can do with Python. However, often \"parameter\" data controlling how the node operates needs to be saved inside the PCOT document file, and loaded when we reopen the file. For example, the expr node needs to store a string: the expression to be run. Parameters for some nodes can be complicated: multidot needs to be able to store a list of circular regions of interest, for example. We also need to do this to handle undo operations - every time a change is made, the entire document is \"saved\" into an archive in memory so it can be undone. This process - converting node data into data which can be saved to archives - is called serialisation , and there no less than four different mechanisms for doing it. This is largely for historical reasons, but also because the different mechanisms serve different needs: In order of preference, with the best at the top: TaggedAggregate serialisation - the data is JSON-serialisable but we want to make it possible to edit it from a batch/parameter file (see batch mode ). Probably the best choice if you can. complex serialisation via TaggedAggregate - the data is not serialisable, but we want to edit it from a parameter file. Probably the second-best and suitable where simple TA-serialisation can't handle the more complex data involved. autoserialisation - for when your data is already JSON-serialisable and you don't need to edit it from a parameter file. It is very simple to implement, but doesn't allow editing from a batch file and doesn't document itself automatically. Used only in legacy nodes. complex serialisation - for when your data is not directly JSON-serialisable (for example, regions of interest) and you don't need to edit it from a parameter file because it makes no sense (such as painted ROIs) TaggedAggregate serialisation This is the method we use when we want to be able to edit the parameters of nodes in batch mode, using parameter files (see batch mode ). It is probably the best method to use because of this, but it is rather more complicated. We make use of tagged aggregate structures , which can be found in pcot.utils.taggedaggregate . These are dictionaries and lists, but each has a formal, typed structure with \"tags\" giving the names of the members, their types, and default values. Each is described by a type singleton object which provides descriptions of its elements (these are the \"tags\" in the name). Calling create() on the singleton builds an instance of the structure with all the values filled in with defaults. TaggedDictType The main type used is TaggedDictType , which describes the format of a set of key/value pairs. Calling create() on one of these objects builds a TaggedDict object containing the default values. The TaggedDictType constructor takes a set of keyword arguments. Each key is the same of an element in the dict, and each value describes that element as a tuple of: a description used in the documentation a type: either a primitive type such as int or str, or another TaggedAggregateType subclass for nested structures a default value (must be None for aggregates, which provide their own defaults) for string values, an optional list of acceptable strings If you call setOrdered on the constructed type object you will get an \"ordered dict\" - this will be serialised as a tuple with the contained data having an implicit ordering. You might wonder why we don't make all tagged dicts ordered, so they are all serialised as tuples. The answer is that doing that would make it harder to implement backcompatibility - if we serialise as a tuple, adding and removing fields in the future becomes difficult. Only use ordered dicts for things where we are very unlikely to change the structure. One advantage is that it is possible to set all values in an ordered dict in one line inside a batch file. For example, here is a TaggedDictType definition for a rectangle: taggedRectType = TaggedDictType( x=(\"The x coordinate of the top left corner\", Number, 0), y=(\"The y coordinate of the top left corner\", Number, 0), w=(\"The width of the rectangle\", Number, 10), h=(\"The height of the rectangle\", Number, 10)).setOrdered() We are using Number here to indicate that either ints or floats are acceptable. We can then create a rectangle TaggedDict and access its values: r = taggedRectType.create() # create TaggedDict from type print(f\"Rectangle at {r.x}, {r.y}\") # show values r.w = 20 # set values r.h = 30 Bear in mind that there are functions for generating rectangle and colour type object in pcot.utils.taggedaggregates: taggedColourType and taggedRectType. You probably shouldn't create a rectangle type yourself. We can also specify that a parameter is another TaggedDict , allowing us to build complex nested structures. Here we nest the taggedRectType we defined above in another dict type: taggedThingType = TaggedDictType( rect=(\"The rectangle\", taggedRectType), somenumber=(\"Some numerical value\",Number,0)) TaggedListType These objects describe lists, and calling create() on them generates a TaggedList item. This is rarely done directly - it's more usual for a TaggedDictType to specify that one of its values is a list, in which case the list object is created when the containing TaggedDict is created. Here's an example where the dict contains both our rectangle type and a list of rectangles: taggedThingType = TaggedDictType( main=(\"The main rectangle\", taggedRectType), others=(\"Some other rectangles\", TaggedListType(taggedRectType,0)), somenumber=(\"Some numerical value\",Number,0)) We can then create our parameters and add a new default rect to the list: thing = taggedThingType.create() listOfThings.others.append_default() We can then access these items: print(taggedThingType.others.[0].x) The TaggedListType constructor takes the following arguments: Type of item (must be a TaggedAggregateType subclass or a primitive type - int, str, etc.) Default length (if a list of aggregates) or default list (if a list of primitives) Optional default value to append when a new item is created, ignored for lists of aggregates which will create their own default item For more details on how to use these structures, read the tests in tests/test_taggedaggs.py . You'll note that all the elements of a TaggedAggregate structure are JSON-serialisable 1 , although some can be numpy arrays. However, the nature of the structure allows defaults - and documentation - to be generated automatically. Using TaggedAggregates to serialise nodes To use a TaggedAggregate to serialise node data, create a TaggedDictType and assign it to the params member of the XFormType in the constructor. For example: self.params = TaggedDictType( mul=(\"multiplicative factor (done first)\", float, 1.0), add=(\"additive constant (done last)\", float, 0.0)) When a new node is created, a default structure will be created from this type and stored in the node's params field where it can be accessed from the perform method: output = node.params.add + node.params.mul * node.getInput(0, Datum.IMG) When the node is serialised, the structure will be serialised. complex TaggedAggregate serialisation (CTAS) The previous method dealt with data which can be JSON-serialised directly. If we need to modify non-JSON-serialisable data with parameter files, we need to do something similar to the complex serialisation method described above but going through a TaggedAggregate: we set the TaggedAggregate from our complex data, and then PCOT will serialise that. To do this, we write code as before to store the data in a TaggedDict in node.params , and we store any simple data we have in that structure. Then we write a serialise method containing code which converts our more complex data into a simpler form and stores it in node.params so it can be serialised. Here's an example: def serialise(self, node): # fill in the node.params with data node.params.foo = some_data_or_other node.params.bar = some_data_or_other # we don't return anything, because node.params will have been set to # represent our data; we don't need to add anything directly to the # JSON-serialisable dict. return None This method is also used by the legacy serialisation mechanism, where it would return a JSON dict directly. Here we need to return None instead. We must also write a nodeDataFromParams method. This takes a node, and uses its params field (which will be a TaggedDict, of course) to set the node's internal data: def nodeDataFromParams(self, node): # convert some data in node.params into our own private data our_data = some_function_of(node.params.foo, node.params.bar) Maybe Types and TaggedAggregate type objects can be \"wrapped\" in Maybe objects if they might be null: tdt = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", Maybe(str), \"foo\"), # string or null c=(\"c\", float, 3.14) ) td = tdt.create() # create new dict td.b = \"hello\" # this is fine td.b = None # and so is this Note: I would have used Optional from the typing package, but that can only take actual types - not the type objects we use here. Avoid using Maybe for tagged aggregates etc., because you can't create a new one using a parameter file, just modify values in an existing one. TaggedVariantDicts Sometimes it is necessary to store different kinds of object in a list. We can do this with TaggedAggregates, provided the objects are all TaggedDicts and there is a field in all the dicts which tells us which type it is - a \"discriminator\". You can find more details on TVDs here Where to find examples Probably the most complex but straightforward example of CTAS is the gradient node. This converts a monochrome image to a false colour RGB gradient based on a set of gradient colours. This exists as a pcot.utils.gradient.Gradient object which is a wrapper around a list of (x,(r,g,b)) tuples defining the colour r,g,b at value x . The CTAS methods are responsible for converting tagged aggregate data and this Gradient object. They also handle a preset string which can override the data when set from a parameter file . Regions of interest are a rather more complex example... Autoserialisation This is the serialisation method used for a few legacy nodes and nodes which don't require batch editing, like comment . In the simplest case, the data stored in the XForm object for a particular node is already JSON-serialisable: that is, it is either a Python primitive type (number, string, tuple, list or dict) or a Numpy array (PCOT handles serialising these automatically). In this case you can simply list the names of the attributes in a tuple called autoserialise in the XFormType , along with some defaults which are used in case the items are not found in the saved data. For example, the constructor for XFormSpectrum could look like this (not any more, because it now uses TA-serialisation): super().__init__(\"spectrum\", \"data\", \"0.0.0\") self.autoserialise = ('sortlist', 'errorbarmode', 'legendFontSize', 'axisFontSize', 'stackSep', 'labelFontSize', 'bottomSpace', 'colourmode', 'rightSpace', # these have defaults because they were developed later. ('ignorePixSD', False), ('bandwidthmode', BANDWIDTHMODE_NONE), ) for i in range(NUMINPUTS): self.addInputConnector(str(i), Datum.IMG, \"a single line in the plot\") self.addOutputConnector(\"data\", Datum.DATA, \"a CSV output (use 'dump' or 'sink' to read it)\") Note that the default values are optional - if you don't specify a default you can just use the attribute name rather than a (name, default) tuple, but you will get an error if you try to load from data which doesn't have that attribute stored. When the system serialises the node it will read the named fields from the node, and it will do the reverse when it deserialises a node (i.e. load it from an archive). Complex serialisation In this case, we have data which isn't JSON-serialisable, and where it doesn't make any sense to have the parameters editable outside the PCOT user interface. Here, we add serialise and deserialise methods to the XFormType to convert our data to and from data which is JSON-serialisable. The serialise method will take the node and return a dict (which is JSON-serialisable), while the deserialise method will take the node and the dict and set the appropriate values a newly created node. The dict returned by serialise will be merged with the dict generated by autoserialisation and other, general node data (e.g. canvas data). Make sure you use unique names and not any of: canvas, displayName, inputTypes, ins, outs, mapping, md5, type, ver, w, xy, outputTypes An example: imagine our node stores a list of objects of some class Foo . We could write our methods thus: def serialise(self, node): return {'foolist': [(x.a, x.b) for x in node.foo_list]} def deserialise(self, node, d): node.foo_list = [Foo(a, b) for a, b in d['foolist']] In this case we are converting the Foo objects into tuples to serialise them, and constructing them from the tuples when we deserialise. It would probably be better to write Foo so that it can serialise itself and has a deserialise constructor: class Foo: def __init__(self...): ... def serialise(self)->dict: return ... @staticmethod def deserialise(d: dict): return Foo(...) and def serialise(self, node): return {'foolist': [x.serialise() for x in node.foo_list]} def deserialise(self, node, d): node.foo_list = [Foo.deserialise(x) for x in d['foolist']] i.e. they can be turned directly into JSON - they are primitive types (int, float, str etc.), dicts, lists, or tuples. \u21a9","title":"Node serialisation"},{"location":"devguide/nodeser/#node-serialisation","text":"This page discusses how to serialise the parameters for node types. If you don't know what that means, you're either in the wrong place or haven't read Writing PCOT plugins . Most nodes need to store some data. Sometimes this data can just be stored directly, as attributes of the XForm object (i.e. the node). It's not really \"polite\" programming, but this is the kind of thing you can do with Python. However, often \"parameter\" data controlling how the node operates needs to be saved inside the PCOT document file, and loaded when we reopen the file. For example, the expr node needs to store a string: the expression to be run. Parameters for some nodes can be complicated: multidot needs to be able to store a list of circular regions of interest, for example. We also need to do this to handle undo operations - every time a change is made, the entire document is \"saved\" into an archive in memory so it can be undone. This process - converting node data into data which can be saved to archives - is called serialisation , and there no less than four different mechanisms for doing it. This is largely for historical reasons, but also because the different mechanisms serve different needs: In order of preference, with the best at the top: TaggedAggregate serialisation - the data is JSON-serialisable but we want to make it possible to edit it from a batch/parameter file (see batch mode ). Probably the best choice if you can. complex serialisation via TaggedAggregate - the data is not serialisable, but we want to edit it from a parameter file. Probably the second-best and suitable where simple TA-serialisation can't handle the more complex data involved. autoserialisation - for when your data is already JSON-serialisable and you don't need to edit it from a parameter file. It is very simple to implement, but doesn't allow editing from a batch file and doesn't document itself automatically. Used only in legacy nodes. complex serialisation - for when your data is not directly JSON-serialisable (for example, regions of interest) and you don't need to edit it from a parameter file because it makes no sense (such as painted ROIs)","title":"Node serialisation"},{"location":"devguide/nodeser/#taggedaggregate-serialisation","text":"This is the method we use when we want to be able to edit the parameters of nodes in batch mode, using parameter files (see batch mode ). It is probably the best method to use because of this, but it is rather more complicated. We make use of tagged aggregate structures , which can be found in pcot.utils.taggedaggregate . These are dictionaries and lists, but each has a formal, typed structure with \"tags\" giving the names of the members, their types, and default values. Each is described by a type singleton object which provides descriptions of its elements (these are the \"tags\" in the name). Calling create() on the singleton builds an instance of the structure with all the values filled in with defaults.","title":"TaggedAggregate serialisation"},{"location":"devguide/nodeser/#taggeddicttype","text":"The main type used is TaggedDictType , which describes the format of a set of key/value pairs. Calling create() on one of these objects builds a TaggedDict object containing the default values. The TaggedDictType constructor takes a set of keyword arguments. Each key is the same of an element in the dict, and each value describes that element as a tuple of: a description used in the documentation a type: either a primitive type such as int or str, or another TaggedAggregateType subclass for nested structures a default value (must be None for aggregates, which provide their own defaults) for string values, an optional list of acceptable strings If you call setOrdered on the constructed type object you will get an \"ordered dict\" - this will be serialised as a tuple with the contained data having an implicit ordering. You might wonder why we don't make all tagged dicts ordered, so they are all serialised as tuples. The answer is that doing that would make it harder to implement backcompatibility - if we serialise as a tuple, adding and removing fields in the future becomes difficult. Only use ordered dicts for things where we are very unlikely to change the structure. One advantage is that it is possible to set all values in an ordered dict in one line inside a batch file. For example, here is a TaggedDictType definition for a rectangle: taggedRectType = TaggedDictType( x=(\"The x coordinate of the top left corner\", Number, 0), y=(\"The y coordinate of the top left corner\", Number, 0), w=(\"The width of the rectangle\", Number, 10), h=(\"The height of the rectangle\", Number, 10)).setOrdered() We are using Number here to indicate that either ints or floats are acceptable. We can then create a rectangle TaggedDict and access its values: r = taggedRectType.create() # create TaggedDict from type print(f\"Rectangle at {r.x}, {r.y}\") # show values r.w = 20 # set values r.h = 30 Bear in mind that there are functions for generating rectangle and colour type object in pcot.utils.taggedaggregates: taggedColourType and taggedRectType. You probably shouldn't create a rectangle type yourself. We can also specify that a parameter is another TaggedDict , allowing us to build complex nested structures. Here we nest the taggedRectType we defined above in another dict type: taggedThingType = TaggedDictType( rect=(\"The rectangle\", taggedRectType), somenumber=(\"Some numerical value\",Number,0))","title":"TaggedDictType"},{"location":"devguide/nodeser/#taggedlisttype","text":"These objects describe lists, and calling create() on them generates a TaggedList item. This is rarely done directly - it's more usual for a TaggedDictType to specify that one of its values is a list, in which case the list object is created when the containing TaggedDict is created. Here's an example where the dict contains both our rectangle type and a list of rectangles: taggedThingType = TaggedDictType( main=(\"The main rectangle\", taggedRectType), others=(\"Some other rectangles\", TaggedListType(taggedRectType,0)), somenumber=(\"Some numerical value\",Number,0)) We can then create our parameters and add a new default rect to the list: thing = taggedThingType.create() listOfThings.others.append_default() We can then access these items: print(taggedThingType.others.[0].x) The TaggedListType constructor takes the following arguments: Type of item (must be a TaggedAggregateType subclass or a primitive type - int, str, etc.) Default length (if a list of aggregates) or default list (if a list of primitives) Optional default value to append when a new item is created, ignored for lists of aggregates which will create their own default item For more details on how to use these structures, read the tests in tests/test_taggedaggs.py . You'll note that all the elements of a TaggedAggregate structure are JSON-serialisable 1 , although some can be numpy arrays. However, the nature of the structure allows defaults - and documentation - to be generated automatically.","title":"TaggedListType"},{"location":"devguide/nodeser/#using-taggedaggregates-to-serialise-nodes","text":"To use a TaggedAggregate to serialise node data, create a TaggedDictType and assign it to the params member of the XFormType in the constructor. For example: self.params = TaggedDictType( mul=(\"multiplicative factor (done first)\", float, 1.0), add=(\"additive constant (done last)\", float, 0.0)) When a new node is created, a default structure will be created from this type and stored in the node's params field where it can be accessed from the perform method: output = node.params.add + node.params.mul * node.getInput(0, Datum.IMG) When the node is serialised, the structure will be serialised.","title":"Using TaggedAggregates to serialise nodes"},{"location":"devguide/nodeser/#complex-taggedaggregate-serialisation-ctas","text":"The previous method dealt with data which can be JSON-serialised directly. If we need to modify non-JSON-serialisable data with parameter files, we need to do something similar to the complex serialisation method described above but going through a TaggedAggregate: we set the TaggedAggregate from our complex data, and then PCOT will serialise that. To do this, we write code as before to store the data in a TaggedDict in node.params , and we store any simple data we have in that structure. Then we write a serialise method containing code which converts our more complex data into a simpler form and stores it in node.params so it can be serialised. Here's an example: def serialise(self, node): # fill in the node.params with data node.params.foo = some_data_or_other node.params.bar = some_data_or_other # we don't return anything, because node.params will have been set to # represent our data; we don't need to add anything directly to the # JSON-serialisable dict. return None This method is also used by the legacy serialisation mechanism, where it would return a JSON dict directly. Here we need to return None instead. We must also write a nodeDataFromParams method. This takes a node, and uses its params field (which will be a TaggedDict, of course) to set the node's internal data: def nodeDataFromParams(self, node): # convert some data in node.params into our own private data our_data = some_function_of(node.params.foo, node.params.bar) Maybe Types and TaggedAggregate type objects can be \"wrapped\" in Maybe objects if they might be null: tdt = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", Maybe(str), \"foo\"), # string or null c=(\"c\", float, 3.14) ) td = tdt.create() # create new dict td.b = \"hello\" # this is fine td.b = None # and so is this Note: I would have used Optional from the typing package, but that can only take actual types - not the type objects we use here. Avoid using Maybe for tagged aggregates etc., because you can't create a new one using a parameter file, just modify values in an existing one. TaggedVariantDicts Sometimes it is necessary to store different kinds of object in a list. We can do this with TaggedAggregates, provided the objects are all TaggedDicts and there is a field in all the dicts which tells us which type it is - a \"discriminator\". You can find more details on TVDs here Where to find examples Probably the most complex but straightforward example of CTAS is the gradient node. This converts a monochrome image to a false colour RGB gradient based on a set of gradient colours. This exists as a pcot.utils.gradient.Gradient object which is a wrapper around a list of (x,(r,g,b)) tuples defining the colour r,g,b at value x . The CTAS methods are responsible for converting tagged aggregate data and this Gradient object. They also handle a preset string which can override the data when set from a parameter file . Regions of interest are a rather more complex example...","title":"complex TaggedAggregate serialisation (CTAS)"},{"location":"devguide/nodeser/#autoserialisation","text":"This is the serialisation method used for a few legacy nodes and nodes which don't require batch editing, like comment . In the simplest case, the data stored in the XForm object for a particular node is already JSON-serialisable: that is, it is either a Python primitive type (number, string, tuple, list or dict) or a Numpy array (PCOT handles serialising these automatically). In this case you can simply list the names of the attributes in a tuple called autoserialise in the XFormType , along with some defaults which are used in case the items are not found in the saved data. For example, the constructor for XFormSpectrum could look like this (not any more, because it now uses TA-serialisation): super().__init__(\"spectrum\", \"data\", \"0.0.0\") self.autoserialise = ('sortlist', 'errorbarmode', 'legendFontSize', 'axisFontSize', 'stackSep', 'labelFontSize', 'bottomSpace', 'colourmode', 'rightSpace', # these have defaults because they were developed later. ('ignorePixSD', False), ('bandwidthmode', BANDWIDTHMODE_NONE), ) for i in range(NUMINPUTS): self.addInputConnector(str(i), Datum.IMG, \"a single line in the plot\") self.addOutputConnector(\"data\", Datum.DATA, \"a CSV output (use 'dump' or 'sink' to read it)\") Note that the default values are optional - if you don't specify a default you can just use the attribute name rather than a (name, default) tuple, but you will get an error if you try to load from data which doesn't have that attribute stored. When the system serialises the node it will read the named fields from the node, and it will do the reverse when it deserialises a node (i.e. load it from an archive).","title":"Autoserialisation"},{"location":"devguide/nodeser/#complex-serialisation","text":"In this case, we have data which isn't JSON-serialisable, and where it doesn't make any sense to have the parameters editable outside the PCOT user interface. Here, we add serialise and deserialise methods to the XFormType to convert our data to and from data which is JSON-serialisable. The serialise method will take the node and return a dict (which is JSON-serialisable), while the deserialise method will take the node and the dict and set the appropriate values a newly created node. The dict returned by serialise will be merged with the dict generated by autoserialisation and other, general node data (e.g. canvas data). Make sure you use unique names and not any of: canvas, displayName, inputTypes, ins, outs, mapping, md5, type, ver, w, xy, outputTypes An example: imagine our node stores a list of objects of some class Foo . We could write our methods thus: def serialise(self, node): return {'foolist': [(x.a, x.b) for x in node.foo_list]} def deserialise(self, node, d): node.foo_list = [Foo(a, b) for a, b in d['foolist']] In this case we are converting the Foo objects into tuples to serialise them, and constructing them from the tuples when we deserialise. It would probably be better to write Foo so that it can serialise itself and has a deserialise constructor: class Foo: def __init__(self...): ... def serialise(self)->dict: return ... @staticmethod def deserialise(d: dict): return Foo(...) and def serialise(self, node): return {'foolist': [x.serialise() for x in node.foo_list]} def deserialise(self, node, d): node.foo_list = [Foo.deserialise(x) for x in d['foolist']] i.e. they can be turned directly into JSON - they are primitive types (int, float, str etc.), dicts, lists, or tuples. \u21a9","title":"Complex serialisation"},{"location":"devguide/plugins/","text":"Writing PCOT plugins The plugin path PCOT can be extended by adding Python scripts to a directory in the plugin path. By default this is just pcotplugins in the user's home directory, but it can be changed by editing the .pcot.ini file in that directory and modifying the pluginpath value in the Locations section. This should be a semicolon-separated list of directories. You may be tempted to use quotes in the names of you plugin directories: don't. It should work fine, even if your directories have spaces in them. For example, use ~/blah/RIM Dewarping and not ~/blah/\"RIM Dewarping\" Adding new types This is covered in a separate document , as it's not often done and is a little involved. Adding new Datum functions (for use in expr and Python code) The functions used in the expr expression node can also be used in Python programs which use PCOT as a library (this is covered here ). These are called Datum functions because they both take and return Datum objects. To create a Datum function: use the @datumfunc decorator. This will register the function and wrap it in two separate wrappers: one for use in expr , the other for use in Python. write a docstring in the correct format, as illustrated below. Here's an example which declares a function to take two numbers a,b and calculate a+b*2. Note that as in all PCOT code we have to make sure the sources are handled correctly. @datumfunc def example_func(a, b): \"\"\" Example function that takes two numbers a,b and returns a+b*2 @param a: number: first number @param b: number: second number \"\"\" return a + b * Datum.k(2) This is a trivial example that relies on Datum objects having operator overloads, but note that we need to multiply b by a Datum, not a number. To do this we use Datum.k to create a scalar constant. The docstring This should consist of a number of lines describing the function followed by a number of @param lines, one for each parameter. These contain the following, separated by colons: The string `@param' A Datum type name - these can be found in the constructors of datum type objects in datumtypes.py , but the most common are number , img , roi , string . A description of the parameter Optional numeric/string arguments Optional arguments with defaults can be provided, but only if they are numeric or strings (because these are the only types which make sense for the default values). In this case the defaults will be converted to Datum objects if they are used. Here is an example of a function which multiplies an image by a constant, with the default being 2: @datumfunc def example_func(img, k=2): \"\"\" Example function that takes two numbers a,b and returns a+b*2 @param img:img:the image @param k:number:the multiplier \"\"\" # no need to construct a Datum with Datum.k(), because k is already # a Datum. return img * k Here's another example which adds two numbers or multiplies them, depending on a string - and the default is to add: @datumfunc def stringexample(a, b, op='add'): \"\"\" String argument example @param a: number: first number @param b: number: second number @param op: string: operation to perform \"\"\" if op.get(Datum.STRING) == 'add': return a + b elif op.get(Datum.STRING) == 'sub': return a - b else: raise ValueError(\"Unknown operation\") Note that you usually have to extract the actual value from the Datum objects, as we do with the op argument above. In previous examples, we take advantage of Datum's extensive operator overloading. Variadic arguments For a variable number of arguments, use the *args keyword. Here, you have to check the types by hand. For example, this function will sum numbers: @datumfunc def sumall(*args): \"\"\" Sum all arguments \"\"\" s = sum([x.get(Datum.NUMBER).n for x in args]) return Datum(Datum.NUMBER, Value(s, 0, NOUNCERTAINTY), nullSourceSet) Note the use of Value here to construct a scalar value with standard deviation (zero here) and DQ bits (indicating no uncertainty data). Also note the mandatory use of a source set - just the nullSourceSet here to indicate there is no source; this is just a test function. In a real function we would combine the input sources. For more examples of functions, look at the ExpressionEvaluator constructor in the pcot.expressions.eval module. Adding new menu items This is done by adding a function to a list of functions called when a a main window is opened. Writing code here will require some knowledge of Qt. Here is a menu option added to the File menu which will look for selected node in the document's graph, fetch its first output, and save it as an ENVI file (assuming it is an image - error checking is left as an exercise for the reader). import pcot import os from PySide2 import QtWidgets from PySide2.QtWidgets import QAction, QMessageBox from pcot.dataformats import envi def saveEnvi(w): \"\"\"Function takes a PCOT main window. It finds the first selected node, gets its output 0, and then saves an ENVI from that image.\"\"\" sel = w.doc.getSelection() if len(sel) == 0: ui.log(\"no selected node\") return node = sel[0] directory = os.path.expanduser(pcot.config.getDefaultDir('pcotfiles')) res = QtWidgets.QFileDialog.getSaveFileName(w, \"ENVI file \", os.path.expanduser(pcot.config.getDefaultDir('pcotfiles')), \"ENVI files (*.hdr)\") if res[0] != '': # get the output of that node (root, ext) = os.path.splitext(res[0]) img = node.getOutput(0,pcot.datum.Datum.IMG) envi.write(root,img) # the function to add the new menu item. This will take a single parameter: # the window to which the menu should be added. def addMenus(w): \"\"\"Add an item to the Edit menu\"\"\" # create the menu action (i.e. the item) act = QAction(\"save to ENVI\",parent=w) # find (or create) the Edit menu and add the action to it w.findOrAddMenu(\"Edit\").addAction(act) # link the action to the saveEnvi function, using a closure to # make sure the window argument is passed into that function. act.triggered.connect(lambda: saveEnvi(w)) # Add the addMenus functions to the list of functions called as the # main window opens. pcot.config.addMainWindowHook(addMenus) Adding new node types Node types are represented by singletons of subclasses of XFormType (Nodes themselves are of type XForm , which stands for transform for historical reasons). Developing new XFormType subclasses is largely beyond the scope of this document, but you can learn a lot from looking at the pcot.xforms package and the modules therein. A commented example node can be found in xforms/xformexample.py . To create a new node type, declare a new subclass of XFormType and decorate it with the @xformtype decorator. This will make the type autoregister: the singleton will be constructed and added to the internal type registry. You will need to write the following methods in your subclass: __init__(self) to construct the type object (NOT the individual nodes). This will call the superconstructor to set the type's name, group (for the palette), and version. It will add the input and output connectors. init(self, node) will initialise any private data in the node itself (which is an XForm ). Don't confuse this with __init__ ! createTab(self, node, window) will create a new node area (i.e. tab). Often, this can be a TabData which will look at the node's out attribute, which should be a Datum. perform(self, node) will actually perform the node's action, reading inputs and setting outputs. Remember: there is only one XFormType object for each node type. All nodes are of type XForm , and they link to an XFormType object to tell them how to behave. This might seem a really odd way to do things, but it follows \"favour composition over inheritance\" and saves messiness elsewhere. Here is an example which does edge detection with OpenCV: import cv2 as cv import numpy as np from pcot.sources import SourceSet from pcot.xform import XFormType, xformtype, Datum from pcot.xforms.tabdata import TabData from pcot.imagecube import ImageCube import pcot.config # The first part of the plugin creates a new type of node. # this decorator will cause the node to auto-register. @xformtype class XFormEdgeDetect(XFormType): \"\"\"This is an edge detector node. It takes an image and performs Canny edge detection, currently with fixed thresholds. It does not take account of ROIs, since this would be pointless when we're converting from a potentially multispectral image to greyscale (well, boolean). Exercise for the reader - add variable thresholds, either as numeric inputs or as numeric parameters settable from the node tab.\"\"\" def __init__(self): # this node should appear in the maths group. super().__init__(\"edge\", \"maths\", \"0.0.0\") # set input and output - they are images and are unnamed. self.addInputConnector(\"\", Datum.IMG) self.addOutputConnector(\"\", Datum.IMG) def createTab(self, n, w): # there is no custom tab, we just use a data canvas. This expects \"node.out\" to be set to # either None or a Datum. return TabData(n, w) def init(self, n): # No initialisation required. pass def perform(self, node): # get the input image img = node.getInput(0, Datum.IMG) if img is not None: # find mean of all channels - construct a transform array and then use it. mat = np.array([1 / img.channels] * img.channels).reshape((1, img.channels)) grey = cv.transform(img.img, mat) # convert to 8-bit integer from 32-bit float img8 = (grey * 255).astype('uint8') # Perform edge detection out = cv.Canny(img8, 100, 200) # Convert back to 32-bit float out = (out / 255).astype('float32') # create the imagecube and set node.out for the canvas in the tab img = ImageCube(out, None, img.sources) node.out = Datum(Datum.IMG, img) else: # no image on the input, set node.out to None node.out = Datum.null # output node.out node.setOutput(0, node.out) Writing custom Tabs As noted above, a new XFormType subclass (i.e. a new node type) can often just use TabData , which will display the Datum stored on its first output (output 0). Sometimes, however, a custom tab needs to be written. This can be a complex task, but an example is given in xformexample.py in the xforms package. All the standard XFormTypes are in this package, so you can also look at them. The basic idea is: Create a subclass of pcot.ui.tabs.Tab Write the constructor to call the superclass constructor and create the UI (or load a Designer-created UI by passing an argument to the superclass. constructor), and call self.nodeChanged() at the end to refresh the tab from the node. Override onNodeChanged() to update the tab from the node. Use the Qt signal/slot mechanism to connect the tab's controls to methods in the tab class and write code to update the node from the tab in these methods, calling self.changed() at the end of each method. The tab will have a node field which addresses the node it is viewing (but see below for a \"gotcha\" - the value of this field will change after an undo operation!) Using Canvas in custom tabs Creating a Canvas programmatically is straightforward, and there is an example of this in xformexample.py . If you are creating a tab in Designer, you need to add a canvas as a QWidget which you then \"promote\" to a custom control (the canvas). In the promote dialog, the class should be Canvas and the header file pcot.ui.canvas (i.e. the package name). In your onNodeChanged() method you will need to update the canvas. This involves doing a little setup, then getting the image we want to display - usually the output - and telling the canvas to display it: # do some setup self.canvas.setNode(self.node) # then display the image img = self.node.getOutput(0, Datum.IMG) self.canvas.display(img) The setup synchronises the canvas with the node, telling the canvas about the RGB mappings and that it should store data in the node for serialisation. We have to set that up each time because of how undo works, which is discussed in the next section. Undo and references to data in nodes This is a major \"gotcha.\" Whenever an undo occurs, the old node is discarded and a new node created from a previously archived version in memory. This means that the node field changes. Because of this, your tab must not store references to objects inside the node, because after an undo those references will be stale. Instead, always use self.node... to access data. It is OK to use the tab to store UI-only data which is not persisted (saved to a file or to the undo mechanism). Node parameters: serialisation and deserialisation Your node may have parameters which need to be saved to .pcot files or saved into the undo data. To do this, PCOT needs to convert them into data which is \"JSON-serialisable\" - convertable to text in the JSON format. PCOT refers to the process of conversion into JSON-serialisable data as \"serialisation,\" even though it's really only the first step. It's just that the JSON library takes care of the rest of the process. JSON-serialisable data consists of built-in Python types: strings, numbers, lists, dicts and tuples. PCOT has no less than four mechanisms for converting node data into JSON: read more about them here .","title":"Writing PCOT plugins"},{"location":"devguide/plugins/#writing-pcot-plugins","text":"","title":"Writing PCOT plugins"},{"location":"devguide/plugins/#the-plugin-path","text":"PCOT can be extended by adding Python scripts to a directory in the plugin path. By default this is just pcotplugins in the user's home directory, but it can be changed by editing the .pcot.ini file in that directory and modifying the pluginpath value in the Locations section. This should be a semicolon-separated list of directories. You may be tempted to use quotes in the names of you plugin directories: don't. It should work fine, even if your directories have spaces in them. For example, use ~/blah/RIM Dewarping and not ~/blah/\"RIM Dewarping\"","title":"The plugin path"},{"location":"devguide/plugins/#adding-new-types","text":"This is covered in a separate document , as it's not often done and is a little involved.","title":"Adding new types"},{"location":"devguide/plugins/#adding-new-datum-functions-for-use-in-expr-and-python-code","text":"The functions used in the expr expression node can also be used in Python programs which use PCOT as a library (this is covered here ). These are called Datum functions because they both take and return Datum objects. To create a Datum function: use the @datumfunc decorator. This will register the function and wrap it in two separate wrappers: one for use in expr , the other for use in Python. write a docstring in the correct format, as illustrated below. Here's an example which declares a function to take two numbers a,b and calculate a+b*2. Note that as in all PCOT code we have to make sure the sources are handled correctly. @datumfunc def example_func(a, b): \"\"\" Example function that takes two numbers a,b and returns a+b*2 @param a: number: first number @param b: number: second number \"\"\" return a + b * Datum.k(2) This is a trivial example that relies on Datum objects having operator overloads, but note that we need to multiply b by a Datum, not a number. To do this we use Datum.k to create a scalar constant.","title":"Adding new Datum functions (for use in expr and Python code)"},{"location":"devguide/plugins/#the-docstring","text":"This should consist of a number of lines describing the function followed by a number of @param lines, one for each parameter. These contain the following, separated by colons: The string `@param' A Datum type name - these can be found in the constructors of datum type objects in datumtypes.py , but the most common are number , img , roi , string . A description of the parameter","title":"The docstring"},{"location":"devguide/plugins/#optional-numericstring-arguments","text":"Optional arguments with defaults can be provided, but only if they are numeric or strings (because these are the only types which make sense for the default values). In this case the defaults will be converted to Datum objects if they are used. Here is an example of a function which multiplies an image by a constant, with the default being 2: @datumfunc def example_func(img, k=2): \"\"\" Example function that takes two numbers a,b and returns a+b*2 @param img:img:the image @param k:number:the multiplier \"\"\" # no need to construct a Datum with Datum.k(), because k is already # a Datum. return img * k Here's another example which adds two numbers or multiplies them, depending on a string - and the default is to add: @datumfunc def stringexample(a, b, op='add'): \"\"\" String argument example @param a: number: first number @param b: number: second number @param op: string: operation to perform \"\"\" if op.get(Datum.STRING) == 'add': return a + b elif op.get(Datum.STRING) == 'sub': return a - b else: raise ValueError(\"Unknown operation\") Note that you usually have to extract the actual value from the Datum objects, as we do with the op argument above. In previous examples, we take advantage of Datum's extensive operator overloading.","title":"Optional numeric/string arguments"},{"location":"devguide/plugins/#variadic-arguments","text":"For a variable number of arguments, use the *args keyword. Here, you have to check the types by hand. For example, this function will sum numbers: @datumfunc def sumall(*args): \"\"\" Sum all arguments \"\"\" s = sum([x.get(Datum.NUMBER).n for x in args]) return Datum(Datum.NUMBER, Value(s, 0, NOUNCERTAINTY), nullSourceSet) Note the use of Value here to construct a scalar value with standard deviation (zero here) and DQ bits (indicating no uncertainty data). Also note the mandatory use of a source set - just the nullSourceSet here to indicate there is no source; this is just a test function. In a real function we would combine the input sources. For more examples of functions, look at the ExpressionEvaluator constructor in the pcot.expressions.eval module.","title":"Variadic arguments"},{"location":"devguide/plugins/#adding-new-menu-items","text":"This is done by adding a function to a list of functions called when a a main window is opened. Writing code here will require some knowledge of Qt. Here is a menu option added to the File menu which will look for selected node in the document's graph, fetch its first output, and save it as an ENVI file (assuming it is an image - error checking is left as an exercise for the reader). import pcot import os from PySide2 import QtWidgets from PySide2.QtWidgets import QAction, QMessageBox from pcot.dataformats import envi def saveEnvi(w): \"\"\"Function takes a PCOT main window. It finds the first selected node, gets its output 0, and then saves an ENVI from that image.\"\"\" sel = w.doc.getSelection() if len(sel) == 0: ui.log(\"no selected node\") return node = sel[0] directory = os.path.expanduser(pcot.config.getDefaultDir('pcotfiles')) res = QtWidgets.QFileDialog.getSaveFileName(w, \"ENVI file \", os.path.expanduser(pcot.config.getDefaultDir('pcotfiles')), \"ENVI files (*.hdr)\") if res[0] != '': # get the output of that node (root, ext) = os.path.splitext(res[0]) img = node.getOutput(0,pcot.datum.Datum.IMG) envi.write(root,img) # the function to add the new menu item. This will take a single parameter: # the window to which the menu should be added. def addMenus(w): \"\"\"Add an item to the Edit menu\"\"\" # create the menu action (i.e. the item) act = QAction(\"save to ENVI\",parent=w) # find (or create) the Edit menu and add the action to it w.findOrAddMenu(\"Edit\").addAction(act) # link the action to the saveEnvi function, using a closure to # make sure the window argument is passed into that function. act.triggered.connect(lambda: saveEnvi(w)) # Add the addMenus functions to the list of functions called as the # main window opens. pcot.config.addMainWindowHook(addMenus)","title":"Adding new menu items"},{"location":"devguide/plugins/#adding-new-node-types","text":"Node types are represented by singletons of subclasses of XFormType (Nodes themselves are of type XForm , which stands for transform for historical reasons). Developing new XFormType subclasses is largely beyond the scope of this document, but you can learn a lot from looking at the pcot.xforms package and the modules therein. A commented example node can be found in xforms/xformexample.py . To create a new node type, declare a new subclass of XFormType and decorate it with the @xformtype decorator. This will make the type autoregister: the singleton will be constructed and added to the internal type registry. You will need to write the following methods in your subclass: __init__(self) to construct the type object (NOT the individual nodes). This will call the superconstructor to set the type's name, group (for the palette), and version. It will add the input and output connectors. init(self, node) will initialise any private data in the node itself (which is an XForm ). Don't confuse this with __init__ ! createTab(self, node, window) will create a new node area (i.e. tab). Often, this can be a TabData which will look at the node's out attribute, which should be a Datum. perform(self, node) will actually perform the node's action, reading inputs and setting outputs. Remember: there is only one XFormType object for each node type. All nodes are of type XForm , and they link to an XFormType object to tell them how to behave. This might seem a really odd way to do things, but it follows \"favour composition over inheritance\" and saves messiness elsewhere. Here is an example which does edge detection with OpenCV: import cv2 as cv import numpy as np from pcot.sources import SourceSet from pcot.xform import XFormType, xformtype, Datum from pcot.xforms.tabdata import TabData from pcot.imagecube import ImageCube import pcot.config # The first part of the plugin creates a new type of node. # this decorator will cause the node to auto-register. @xformtype class XFormEdgeDetect(XFormType): \"\"\"This is an edge detector node. It takes an image and performs Canny edge detection, currently with fixed thresholds. It does not take account of ROIs, since this would be pointless when we're converting from a potentially multispectral image to greyscale (well, boolean). Exercise for the reader - add variable thresholds, either as numeric inputs or as numeric parameters settable from the node tab.\"\"\" def __init__(self): # this node should appear in the maths group. super().__init__(\"edge\", \"maths\", \"0.0.0\") # set input and output - they are images and are unnamed. self.addInputConnector(\"\", Datum.IMG) self.addOutputConnector(\"\", Datum.IMG) def createTab(self, n, w): # there is no custom tab, we just use a data canvas. This expects \"node.out\" to be set to # either None or a Datum. return TabData(n, w) def init(self, n): # No initialisation required. pass def perform(self, node): # get the input image img = node.getInput(0, Datum.IMG) if img is not None: # find mean of all channels - construct a transform array and then use it. mat = np.array([1 / img.channels] * img.channels).reshape((1, img.channels)) grey = cv.transform(img.img, mat) # convert to 8-bit integer from 32-bit float img8 = (grey * 255).astype('uint8') # Perform edge detection out = cv.Canny(img8, 100, 200) # Convert back to 32-bit float out = (out / 255).astype('float32') # create the imagecube and set node.out for the canvas in the tab img = ImageCube(out, None, img.sources) node.out = Datum(Datum.IMG, img) else: # no image on the input, set node.out to None node.out = Datum.null # output node.out node.setOutput(0, node.out)","title":"Adding new node types"},{"location":"devguide/plugins/#writing-custom-tabs","text":"As noted above, a new XFormType subclass (i.e. a new node type) can often just use TabData , which will display the Datum stored on its first output (output 0). Sometimes, however, a custom tab needs to be written. This can be a complex task, but an example is given in xformexample.py in the xforms package. All the standard XFormTypes are in this package, so you can also look at them. The basic idea is: Create a subclass of pcot.ui.tabs.Tab Write the constructor to call the superclass constructor and create the UI (or load a Designer-created UI by passing an argument to the superclass. constructor), and call self.nodeChanged() at the end to refresh the tab from the node. Override onNodeChanged() to update the tab from the node. Use the Qt signal/slot mechanism to connect the tab's controls to methods in the tab class and write code to update the node from the tab in these methods, calling self.changed() at the end of each method. The tab will have a node field which addresses the node it is viewing (but see below for a \"gotcha\" - the value of this field will change after an undo operation!)","title":"Writing custom Tabs"},{"location":"devguide/plugins/#using-canvas-in-custom-tabs","text":"Creating a Canvas programmatically is straightforward, and there is an example of this in xformexample.py . If you are creating a tab in Designer, you need to add a canvas as a QWidget which you then \"promote\" to a custom control (the canvas). In the promote dialog, the class should be Canvas and the header file pcot.ui.canvas (i.e. the package name). In your onNodeChanged() method you will need to update the canvas. This involves doing a little setup, then getting the image we want to display - usually the output - and telling the canvas to display it: # do some setup self.canvas.setNode(self.node) # then display the image img = self.node.getOutput(0, Datum.IMG) self.canvas.display(img) The setup synchronises the canvas with the node, telling the canvas about the RGB mappings and that it should store data in the node for serialisation. We have to set that up each time because of how undo works, which is discussed in the next section.","title":"Using Canvas in custom tabs"},{"location":"devguide/plugins/#undo-and-references-to-data-in-nodes","text":"This is a major \"gotcha.\" Whenever an undo occurs, the old node is discarded and a new node created from a previously archived version in memory. This means that the node field changes. Because of this, your tab must not store references to objects inside the node, because after an undo those references will be stale. Instead, always use self.node... to access data. It is OK to use the tab to store UI-only data which is not persisted (saved to a file or to the undo mechanism).","title":"Undo and references to data in nodes"},{"location":"devguide/plugins/#node-parameters-serialisation-and-deserialisation","text":"Your node may have parameters which need to be saved to .pcot files or saved into the undo data. To do this, PCOT needs to convert them into data which is \"JSON-serialisable\" - convertable to text in the JSON format. PCOT refers to the process of conversion into JSON-serialisable data as \"serialisation,\" even though it's really only the first step. It's just that the JSON library takes care of the rest of the process. JSON-serialisable data consists of built-in Python types: strings, numbers, lists, dicts and tuples. PCOT has no less than four mechanisms for converting node data into JSON: read more about them here .","title":"Node parameters: serialisation and deserialisation"},{"location":"devguide/release_procedure/","text":"Release procedure To make a new release, follow this checklist. Make sure the dev branch is merged into the master branch. Create a new version number and codeword. For the version number, use semantic versioning . This means: Version number in MAJOR.MINOR.PATCH format append \"-alpha\" if we are still in alpha increment the MAJOR number if backward incompatible changes are introduced increment the MINOR number if new, backward compatible changes are introduced OR if deprecating functionality. increment the PATCH number if backward compatible bug fixes are introduced The version codeword is a helpful mnemonic. We were using a script to generate Rainbow codes , but we've now moved on to Megalithic sites in the UK running through letters of the alphabet and roughly south to north, trying to stick with more memorable names. Only increment the name letter for a major release - for example, DRIFT STONES had a minor change, so went to DYNAS COVE rather than (say) EATHORNE. Edit PCOT/src/pcot/VERSION.txt to add the new version data, also specifying codename and the date (in ISO 8601, YYYY-MM-DD format). Edit PCOT/pyproject.toml to add the new version data (without the codename) Run poetry install on both Windows and Linux and check PCOT still runs (and that the title bar and About version data is correct) Run generate_autodocs.py in mkdocs to get up-to-date autodocs. Create a list of the changes by looking at the Git log and add this to PCOT/mkdocs/docs/releases.md under a new section for the new release. (pyinstaller only) Make pyInstaller builds for Windows and Linux. Do this by going into the pyInstaller directory on the required platform and running pyinstaller file.spec . Check each build works, fixing if necessary. Note that the splash screen will have been automatically generated by the file.spec file from the VERSION.txt, so if it looks weird, check here. Make a MacOS pyInstaller build. This is more complex, and is shown below. Upload to the release repo (see below) (not done now we're not using PyInstaller) Upload the docs to the documentation site. Commit with \"version bump for release (VERSION NAME)\" in the text Make a release tag in the repository, copying the change list from the documentation.","title":"Release procedure"},{"location":"devguide/release_procedure/#release-procedure","text":"To make a new release, follow this checklist. Make sure the dev branch is merged into the master branch. Create a new version number and codeword. For the version number, use semantic versioning . This means: Version number in MAJOR.MINOR.PATCH format append \"-alpha\" if we are still in alpha increment the MAJOR number if backward incompatible changes are introduced increment the MINOR number if new, backward compatible changes are introduced OR if deprecating functionality. increment the PATCH number if backward compatible bug fixes are introduced The version codeword is a helpful mnemonic. We were using a script to generate Rainbow codes , but we've now moved on to Megalithic sites in the UK running through letters of the alphabet and roughly south to north, trying to stick with more memorable names. Only increment the name letter for a major release - for example, DRIFT STONES had a minor change, so went to DYNAS COVE rather than (say) EATHORNE. Edit PCOT/src/pcot/VERSION.txt to add the new version data, also specifying codename and the date (in ISO 8601, YYYY-MM-DD format). Edit PCOT/pyproject.toml to add the new version data (without the codename) Run poetry install on both Windows and Linux and check PCOT still runs (and that the title bar and About version data is correct) Run generate_autodocs.py in mkdocs to get up-to-date autodocs. Create a list of the changes by looking at the Git log and add this to PCOT/mkdocs/docs/releases.md under a new section for the new release. (pyinstaller only) Make pyInstaller builds for Windows and Linux. Do this by going into the pyInstaller directory on the required platform and running pyinstaller file.spec . Check each build works, fixing if necessary. Note that the splash screen will have been automatically generated by the file.spec file from the VERSION.txt, so if it looks weird, check here. Make a MacOS pyInstaller build. This is more complex, and is shown below. Upload to the release repo (see below) (not done now we're not using PyInstaller) Upload the docs to the documentation site. Commit with \"version bump for release (VERSION NAME)\" in the text Make a release tag in the repository, copying the change list from the documentation.","title":"Release procedure"},{"location":"devguide/roi_ctas/","text":"Regions of interest and tagged aggregates Hopefully you will never need to know about the hackery involved in the complex tagged aggregate serialisation involved in regions of interest, but if you do... ROIs are a bit complicated. All ROIs have a set of attributes in common, and each subtype has some extra. To do that, we have code like this: BASEROIFIELDS = [ (\"type\", (\"type\", str, \"\")), (\"label\", (\"label\", Maybe(str), \"\")), (\"labeltop\", (\"label on top?\", bool, False)), (\"colour\", (\"colour\", ROICOLOURTYPE)), (\"thickness\", (\"thickness\", Number, 0)), (\"fontsize\", (\"fontsize\", Number, 10)), (\"drawbg\", (\"draw background\", bool, True)), (\"drawBox\", (\"draw box\", bool, True)), (\"drawEdge\", (\"draw edge\", bool, True)), ] # ... class ROIRect(ROI): \"\"\"Rectangular ROI\"\"\" tpname = \"rect\" # build the tagged dict structure we use for serialising rects - it's a tagged dict with the fields # of a rect, plus the base ROI fields. TAGGEDDICTDEFINITION = BASEROIFIELDS + [ ('bb', ('rectangle', rectType)), ('isset', ('is rectangle set? (internal)', bool, False))] TAGGEDDICT = TaggedDictType(*TAGGEDDICTDEFINITION) so that ROIRect.TAGGEDDICT is a type definition for all the fields we need. We then have methods in each ROI: to_tagged_dict(self) will convert the ROI to the right kind of tagged dict from_tagged_dict(self,td) will set the current ROI from its tagged dict The CTAS methods for XFormRect (the rectangle node) are then: def serialise(self, node): # this returns None, but stores data into .params ready for serialisation # Again - more hackery to get around ROI having 'type' and this clashing with node's 'type' node.params = TaggedDict(self.params) # build a dict without the 'type' (see init) # Serialise the ROI into a TaggedDict, and copy fields from that into the node.params we just made. rser = node.roi.to_tagged_dict() for k in node.params.keys(): node.params[k] = rser[k] # don't return anything; our return is essentially the # node.params return None def nodeDataFromParams(self, node): node.roi.from_tagged_dict(node.params) Finally ROI.new_from_tagged_dict(td) will create an ROI from a tagged dict; it will inspect the type field to see what type is. How multidot does CTAS of ROIs The multidot node uses this system in quite a complex way, because it needs to be able to store a list of different ROIs. It does this using a list of variant dicts - here are the type definitions: class XFormMultidot(XFormType): # ... TAGGEDVDICT = TaggedVariantDictType(\"type\", { \"painted\": ROIPainted.TAGGEDDICT, \"circle\": ROICircle.TAGGEDDICT }) TAGGEDLIST = TaggedListType(TAGGEDVDICT, 0) Here is the serialise method: def serialise(self, node): # create the list of ROI data lst = self.TAGGEDLIST.create() for r in node.rois: # for each ROI, convert to a TaggedDict d = r.to_tagged_dict() # wrap it in a TaggedVariantDict and store it in the list dv = self.TAGGEDVDICT.create().set(d) lst.append(dv) node.params = TaggedDict(self.params) node.params.rois = lst # and don't return anything, because we've stored the data in node.params. return None and here is the nodeDataFromParams method: def nodeDataFromParams(self, node): \"\"\"CTAS deserialisation\"\"\" lst = node.params.rois rs = [] for x in lst: d = x.get() roi = ROI.new_from_tagged_dict(d) rs.append(roi) # filter out any zero-radius circles node.rois = [r for r in rs if isinstance(r, ROIPainted) or r.r > 0]","title":"Regions of interest and tagged aggregates"},{"location":"devguide/roi_ctas/#regions-of-interest-and-tagged-aggregates","text":"Hopefully you will never need to know about the hackery involved in the complex tagged aggregate serialisation involved in regions of interest, but if you do... ROIs are a bit complicated. All ROIs have a set of attributes in common, and each subtype has some extra. To do that, we have code like this: BASEROIFIELDS = [ (\"type\", (\"type\", str, \"\")), (\"label\", (\"label\", Maybe(str), \"\")), (\"labeltop\", (\"label on top?\", bool, False)), (\"colour\", (\"colour\", ROICOLOURTYPE)), (\"thickness\", (\"thickness\", Number, 0)), (\"fontsize\", (\"fontsize\", Number, 10)), (\"drawbg\", (\"draw background\", bool, True)), (\"drawBox\", (\"draw box\", bool, True)), (\"drawEdge\", (\"draw edge\", bool, True)), ] # ... class ROIRect(ROI): \"\"\"Rectangular ROI\"\"\" tpname = \"rect\" # build the tagged dict structure we use for serialising rects - it's a tagged dict with the fields # of a rect, plus the base ROI fields. TAGGEDDICTDEFINITION = BASEROIFIELDS + [ ('bb', ('rectangle', rectType)), ('isset', ('is rectangle set? (internal)', bool, False))] TAGGEDDICT = TaggedDictType(*TAGGEDDICTDEFINITION) so that ROIRect.TAGGEDDICT is a type definition for all the fields we need. We then have methods in each ROI: to_tagged_dict(self) will convert the ROI to the right kind of tagged dict from_tagged_dict(self,td) will set the current ROI from its tagged dict The CTAS methods for XFormRect (the rectangle node) are then: def serialise(self, node): # this returns None, but stores data into .params ready for serialisation # Again - more hackery to get around ROI having 'type' and this clashing with node's 'type' node.params = TaggedDict(self.params) # build a dict without the 'type' (see init) # Serialise the ROI into a TaggedDict, and copy fields from that into the node.params we just made. rser = node.roi.to_tagged_dict() for k in node.params.keys(): node.params[k] = rser[k] # don't return anything; our return is essentially the # node.params return None def nodeDataFromParams(self, node): node.roi.from_tagged_dict(node.params) Finally ROI.new_from_tagged_dict(td) will create an ROI from a tagged dict; it will inspect the type field to see what type is.","title":"Regions of interest and tagged aggregates"},{"location":"devguide/roi_ctas/#how-multidot-does-ctas-of-rois","text":"The multidot node uses this system in quite a complex way, because it needs to be able to store a list of different ROIs. It does this using a list of variant dicts - here are the type definitions: class XFormMultidot(XFormType): # ... TAGGEDVDICT = TaggedVariantDictType(\"type\", { \"painted\": ROIPainted.TAGGEDDICT, \"circle\": ROICircle.TAGGEDDICT }) TAGGEDLIST = TaggedListType(TAGGEDVDICT, 0) Here is the serialise method: def serialise(self, node): # create the list of ROI data lst = self.TAGGEDLIST.create() for r in node.rois: # for each ROI, convert to a TaggedDict d = r.to_tagged_dict() # wrap it in a TaggedVariantDict and store it in the list dv = self.TAGGEDVDICT.create().set(d) lst.append(dv) node.params = TaggedDict(self.params) node.params.rois = lst # and don't return anything, because we've stored the data in node.params. return None and here is the nodeDataFromParams method: def nodeDataFromParams(self, node): \"\"\"CTAS deserialisation\"\"\" lst = node.params.rois rs = [] for x in lst: d = x.get() roi = ROI.new_from_tagged_dict(d) rs.append(roi) # filter out any zero-radius circles node.rois = [r for r in rs if isinstance(r, ROIPainted) or r.r > 0]","title":"How multidot does CTAS of ROIs"},{"location":"devguide/taggedvardicts/","text":"TaggedVariantDicts Sometimes it is necessary to store different kinds of object in a list. We can do this with TaggedAggregates, provided the objects are all TaggedDicts and there is a field in all the dicts which tells us which type it is - a \"discriminator\". For example, we can create two dict types: tdt1 = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", str, \"foo\"), c=(\"c\", float, 3.14) ) tdt2 = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", float, 3.14), d=(\"d\", str, \"wibble\"), e=(\"e\", bool, False) ) and wrap them both in a TaggedVariantDictType: tvdt = TaggedVariantDictType(\"type\", { \"type1\": tdt1, \"type2\": tdt2 }) Here, the discriminator is a field called \"type\", and value \"type1\" tells us it must be of the first type defined above. We don't necessarily need to define field in the dict types, because the variant wrapper will add it automatically. We can now create a list of these variants: tl = TaggedListType(tvdt, 0) lst = tl.create() create a \"type 1\" dict: d = tdt1.create() create a new variant wrapper and set it to that new dict: t = tvdt.create().set(d) and append it to the list: lst.append(t) We can pass a type name to the create method of the variant type to automatically create an embedded dict of the correct type, so we could write the above lines as: d = tvdt.create(\"type1\") lst.append(d) To examine an item, we can get its discriminator value: assert lst[0].get_type() == 'type1' and get the item itself with the get method in the variant: assert lst[0].get().a == 10","title":"TaggedVariantDicts"},{"location":"devguide/taggedvardicts/#taggedvariantdicts","text":"Sometimes it is necessary to store different kinds of object in a list. We can do this with TaggedAggregates, provided the objects are all TaggedDicts and there is a field in all the dicts which tells us which type it is - a \"discriminator\". For example, we can create two dict types: tdt1 = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", str, \"foo\"), c=(\"c\", float, 3.14) ) tdt2 = TaggedDictType( a=(\"a\", int, 10), b=(\"b\", float, 3.14), d=(\"d\", str, \"wibble\"), e=(\"e\", bool, False) ) and wrap them both in a TaggedVariantDictType: tvdt = TaggedVariantDictType(\"type\", { \"type1\": tdt1, \"type2\": tdt2 }) Here, the discriminator is a field called \"type\", and value \"type1\" tells us it must be of the first type defined above. We don't necessarily need to define field in the dict types, because the variant wrapper will add it automatically. We can now create a list of these variants: tl = TaggedListType(tvdt, 0) lst = tl.create() create a \"type 1\" dict: d = tdt1.create() create a new variant wrapper and set it to that new dict: t = tvdt.create().set(d) and append it to the list: lst.append(t) We can pass a type name to the create method of the variant type to automatically create an embedded dict of the correct type, so we could write the above lines as: d = tvdt.create(\"type1\") lst.append(d) To examine an item, we can get its discriminator value: assert lst[0].get_type() == 'type1' and get the item itself with the get method in the variant: assert lst[0].get().a == 10","title":"TaggedVariantDicts"},{"location":"devguide/types/","text":"Notes on types Built-in types of data (i.e. of Datum objects) are kept in datum.py . A type specifies: its name whether it is an image subtype (it's tricky to write a method for this, since ImgType is defined after Type) whether it is an \"internal type\" used in the expression evaluator and not for connections (e.g. IdentType, FuncType and NoneType) optional serialisation and deserialisation methods taking and returning Datum, which convert to and from JSON-serialisable values - i.e. primitive types, tuples, lists and dicts; no objects. The Datum class has a type list, which contains singletons of all the type objects. To register a type: create a type object append to the Datum types list if required, register a connector brush with connbrushes.register. Deal with binary and unary operators in expr expressions if required (see below). An example This code is taken from the example1.py plugin file. This (in part) declares a function which takes two numbers, and produces an object containing the dividend and remainder of those numbers. These are held in a custom object: # Our data will be an object of class TestObject. class TestObject: def __init__(self, div, rem): self.div = div self.rem = rem def __str__(self): return f\"({self.div}, {self.rem})\" Now we need to provide the type singleton: # now the type singleton, which controls how Datum objects which hold TestObjects # serialise and deserialise themselves (turn themselves into JSON-serialisable # data and back). # I'm naming the class with an underscore - the type object will be without this. class _TestObjectType(Type): def __init__(self): # just call the superconstructor telling it the name of the type # and in this case, that the stringification (the result of __str__ on the # value) is short enough to fit into an expr node's graph box. super().__init__('testtuple', outputStringShort=True) # now we have to write code which converts Datums of this type into # stuff which can be converted to JSON and back again. Converting # into JSON-serialisable is termed \"serialisation\" and reconstructing # the original Datum object and all its data is \"deserialisation\". def serialise(self, d): # how to serialise data of this type: serialise() methods must return # a tuple of typename and contents. # The contents must be JSON-serialisable, and must contain both the # data to be saved and the serialised source information. # First convert TestObject to something we can serialise serialisedObject = d.val.div, d.val.rem # and create the serialised datum of the name and contents return self.name, (serialisedObject, d.getSources().serialise()) def deserialise(self, d, document): # given a serialised tuple generated by serialise(), produce a Datum # of this type. serialisedObject, serialisedSources = d # first generate the contents # deserialise the serialised sources data sources = SourceSet.deserialise(serialisedSources, document) # then pass to the datum constructor along with the type singleton. return Datum(self, serialisedObject, sources) We register the type singleton, but keep a reference to the object so we can use it in our own code when we create Datum objects. We also provide a connector brush, so that connections of this type are rendered differently in the graph: # create the singleton and register it, but keep hold of the variable so # we can use it to create new Datum objects. TestObjectType = _TestObjectType() Datum.registerType(TestObjectType) # add a brush for the connections in the graph pcot.connbrushes.register(TestObjectType, QColor(\"darkMagenta\")) See example1.py for how this new type is used. Operators Work in progress - read the Value documentation to see how this works in detail, particularly with numeric and image data (i.e. data which can be expressed as (mean,sd,DQ) triples). Previously operators were entirely hardwired in utils.ops. This stopped us creating new types. We could define something like binop(self,other) in the type classes, but this wouldn't allow us to add new types as the RHS for operations which have built-in types as the LHS. The Simplest Thing That Can Possibly Work is a dictionary of operation functions keyed (in the case of binops) by a tuple of types. So this is how operators work now, relying on two registration processes. The first is the registration of the operator lexeme (e.g. \"*\" or \"+\") and precedence, and an associated function to call. This happens as part of Parser. The function calls a binop() function in the ops module, passing in the operator ID. The second is the registration of operator ID (e.g. Operator.ADD) and types in the ops module, with an associated function to call. This is often a wrapper function around a lambda: the wrapper knows to unpack (say) image and number data, and the lambda says they should be processed with addition. Adding a new type with operator semantics Create a subclass of datum.Type add serialisation methods if required call Datum.registerType() with the type If required, add a new connector brush with connbrushes.register() To use the type, use the Type object with the Datum constructor and Datum.get() method. Adding operator semantics call ops.registerBinop and ops.registerUnop to register functions to perform the required operations. The function should take Datum objects and return a Datum.","title":"Notes on types"},{"location":"devguide/types/#notes-on-types","text":"Built-in types of data (i.e. of Datum objects) are kept in datum.py . A type specifies: its name whether it is an image subtype (it's tricky to write a method for this, since ImgType is defined after Type) whether it is an \"internal type\" used in the expression evaluator and not for connections (e.g. IdentType, FuncType and NoneType) optional serialisation and deserialisation methods taking and returning Datum, which convert to and from JSON-serialisable values - i.e. primitive types, tuples, lists and dicts; no objects. The Datum class has a type list, which contains singletons of all the type objects. To register a type: create a type object append to the Datum types list if required, register a connector brush with connbrushes.register. Deal with binary and unary operators in expr expressions if required (see below).","title":"Notes on types"},{"location":"devguide/types/#an-example","text":"This code is taken from the example1.py plugin file. This (in part) declares a function which takes two numbers, and produces an object containing the dividend and remainder of those numbers. These are held in a custom object: # Our data will be an object of class TestObject. class TestObject: def __init__(self, div, rem): self.div = div self.rem = rem def __str__(self): return f\"({self.div}, {self.rem})\" Now we need to provide the type singleton: # now the type singleton, which controls how Datum objects which hold TestObjects # serialise and deserialise themselves (turn themselves into JSON-serialisable # data and back). # I'm naming the class with an underscore - the type object will be without this. class _TestObjectType(Type): def __init__(self): # just call the superconstructor telling it the name of the type # and in this case, that the stringification (the result of __str__ on the # value) is short enough to fit into an expr node's graph box. super().__init__('testtuple', outputStringShort=True) # now we have to write code which converts Datums of this type into # stuff which can be converted to JSON and back again. Converting # into JSON-serialisable is termed \"serialisation\" and reconstructing # the original Datum object and all its data is \"deserialisation\". def serialise(self, d): # how to serialise data of this type: serialise() methods must return # a tuple of typename and contents. # The contents must be JSON-serialisable, and must contain both the # data to be saved and the serialised source information. # First convert TestObject to something we can serialise serialisedObject = d.val.div, d.val.rem # and create the serialised datum of the name and contents return self.name, (serialisedObject, d.getSources().serialise()) def deserialise(self, d, document): # given a serialised tuple generated by serialise(), produce a Datum # of this type. serialisedObject, serialisedSources = d # first generate the contents # deserialise the serialised sources data sources = SourceSet.deserialise(serialisedSources, document) # then pass to the datum constructor along with the type singleton. return Datum(self, serialisedObject, sources) We register the type singleton, but keep a reference to the object so we can use it in our own code when we create Datum objects. We also provide a connector brush, so that connections of this type are rendered differently in the graph: # create the singleton and register it, but keep hold of the variable so # we can use it to create new Datum objects. TestObjectType = _TestObjectType() Datum.registerType(TestObjectType) # add a brush for the connections in the graph pcot.connbrushes.register(TestObjectType, QColor(\"darkMagenta\")) See example1.py for how this new type is used.","title":"An example"},{"location":"devguide/types/#operators","text":"Work in progress - read the Value documentation to see how this works in detail, particularly with numeric and image data (i.e. data which can be expressed as (mean,sd,DQ) triples). Previously operators were entirely hardwired in utils.ops. This stopped us creating new types. We could define something like binop(self,other) in the type classes, but this wouldn't allow us to add new types as the RHS for operations which have built-in types as the LHS. The Simplest Thing That Can Possibly Work is a dictionary of operation functions keyed (in the case of binops) by a tuple of types. So this is how operators work now, relying on two registration processes. The first is the registration of the operator lexeme (e.g. \"*\" or \"+\") and precedence, and an associated function to call. This happens as part of Parser. The function calls a binop() function in the ops module, passing in the operator ID. The second is the registration of operator ID (e.g. Operator.ADD) and types in the ops module, with an associated function to call. This is often a wrapper function around a lambda: the wrapper knows to unpack (say) image and number data, and the lambda says they should be processed with addition.","title":"Operators"},{"location":"devguide/types/#adding-a-new-type-with-operator-semantics","text":"Create a subclass of datum.Type add serialisation methods if required call Datum.registerType() with the type If required, add a new connector brush with connbrushes.register() To use the type, use the Type object with the Datum constructor and Datum.get() method. Adding operator semantics call ops.registerBinop and ops.registerUnop to register functions to perform the required operations. The function should take Datum objects and return a Datum.","title":"Adding a new type with operator semantics"},{"location":"devguide/values/","text":"How Values work Value is PCOT's fundamental type for working with uncertain numerical data. Values are triplets of: a 32-bit floating point nominal value (i.e. a mean), a 32-bit floating point uncertainty value (as standard deviation) around that mean, and a set of 16 data quality (DQ) bits. These are usually scalar, but 1D vectors can also be created. As a user of PCOT you may never encounter Values, but they are used internally whenever any operations involving uncertainty are done. Here are the typical situations where that occurs: Binary and unary operations on Value objects This is the \"base case\". Values have dunder methods for operations. These are usually quite simple, although some (such as exponentiation) are nasty. They: perform the operation to get the new nominal value. This is usually done with a lambda function, relying on Numpy's broadcasting to do the \"right thing\" with what it's given. call a function to calculate the new uncertainty value call a function to combine the two DQs pass the three results into Value's constructor to get a new value Examples - the multiplication operator: def __mul__(self, other): return Value(self.n * other.n, mul_unc(self.n, self.u, other.n, other.u), combineDQs(self, other)) The AND operator: def __and__(self, other): \"\"\"The & operator actually finds the minimum (Zadeh fuzzy op)\"\"\" n = np.where(self.n > other.n, other.n, self.n) u = np.where(self.n > other.n, other.u, self.u) d = np.where(self.n > other.n, other.dq, self.dq) return Value(n, u, d) Binary and unary operations on Datum objects This is for binops - unary operations are much the same, but a lot simpler (although imageUnop currently takes the underlying numpy array). Datum dunder function runs - this is usually very simple. For example, for addition it's just return ops.binop(ops.Operator.ADD, self, other) . Calls pcot.expressions.ops.binop with the appopriate ops.Operator code ops.binop runs converting any raw numbers to Datum.NUMBER data with zero uncertainty. Each possible triple (operator, leftdatum, rightdatum) of operator and Datum types will have a \"binop semantics Datum wrapper\" method registered for it by ops.initOps. For example, (multiplication, Datum.IMG, Datum.NUMBER) will call ops.imageNumberBinop with the lambda lambda x,y: x*y where the latter will take and return Value. ops.binop calls the binop semantics method for the two Datum types For pairings of numbers and/or images, The binop semantics Datum wrapper converts any image data into a Value after performing subimage extraction, then calls the provided lambda. This calls the appropriate dunder method on the two Values. In fact, we could replace lambda x,y: x*y with Value.__mul__ etc., but it's much clearer this way. see above for details of this. In imageNumberBinop (or equivalent) the returned array Value will be stitched back into the left-hand image using ImageCube.modifyWithSub , and that new image will be returned wrapped in a Datum. If the semantic wrapper was numberBinop, so that a number is returned, we just wrap that in a Datum. For other types - not Datum.NUMBER or Datum.IMG - other semantics methods will have been written. For example, the ROI types have binary operators which construct ROIBinop objects using dunder methods: def regROIBinopSemantics(op, fn): \"\"\"Used to register binops for ROIs, which support a subset of ops.\"\"\" registerBinopSemantics(op, Datum.ROI, Datum.ROI, lambda dx, dy: ROIBinop(dx, dy, fn)) regROIBinopSemantics(Operator.ADD, lambda x, y: x + y) regROIBinopSemantics(Operator.SUB, lambda x, y: x - y) regROIBinopSemantics(Operator.MUL, lambda x, y: x * y) regROIBinopSemantics(Operator.DIV, lambda x, y: x / y) regROIBinopSemantics(Operator.POW, lambda x, y: x ** y) Binary and unary operations in an expr node pcot.expressions.parse.InstOp runs, popping two Datum objects and calling a callback function stored in binopRegistry. This callback will have been registered by Parser.registerBinop, and is simply a lambda calling pcot.expressions.ops.binop with an Operator code for the operator, e.g. p.registerBinop('*', 20, lambda a, b: binop(Operator.MUL, a, b)) (20 is the precedence) The callback will call ops.binop, which is step 3 in the preceding section. Again, unary operations are very similar but rather simpler. functions of Value objects Value generally has a method for each supported function. For example, the tan method will perform the trigonometric tan with uncertainty, returning a new Value. This is rarely called directly - instead, we generally work with Datum objects. datumfuncs: functions of Datum objects These are registered with the @datumfunc operator (see the plugins documentation ) which can be found in pcot.expressions.register. This registers two separate wrappers: the expression wrapper, used when the function is called from an expr node; and the Python function wrapper, called when the function is called from inside Python. They can take any number of arguments, but they must be either Datum objects or numeric. Datumfuncs called from Python The @datumfunc decorator will have wrapped the function in the Python decorator, so that subsequent calls from Python will go through the wrapper. This wrapper will set default values for missing arguments (which must be string or numeric) convert numeric and string arguments to Datum objects call the original function and return the result (which must be Datum) Datumfuncs called from expr nodes The @datumfunc decorator also wraps the function in another wrapper - the exprwrapper - and registers the wrapped function with the expression parser. The wrapper itself is simpler than the Python wrapper because it can make use of facilities in the parser to convert values and check for errors. When data reaches the exprwrapper, we already know that the arguments are all Datum objects. Datumfuncs of single numeric/scalar arguments Functions of a single numeric or image Datum are sometimes written using an \"inner wrapper\", which will turn imagecubes and numbers into Value objects in a similar way to how the semantic binop wrappers work for operations. It will also wrap the resulting Value in a Datum. An example is pcot.datumfuncs.func_wrapper . Here is the datumfunc sin in full: @datumfunc def sin(a): \"\"\" Calculate sine of an angle in radians @param a:img,number:the angle (or image in which each pixel is a single angle) \"\"\" return func_wrapper(lambda xx: xx.sin(), a) We don't need worry about whether the argument is an image or a scalar, because the func_wrapper will deal with it. However, func_wrapper can only deal with images and scalars. The stats wrapper Another datumfunc inner wrapper which can be used is stats_wrapper. This wraps a function which takes a (nominal,uncertainty,dq) tuple and returns another tuple of the same type. It does the following: If provided with a numeric value, calls the wrapped function and creates a Value from the returned data. If provided with an ImageCube, gets the subimage, splits it into bands, and calls the wrapped function on the non-BAD values in each band. The results for each band - assumed to be scalar - are converted into a vector Value with one value for each band. This lets us write the mean like this: @datumfunc def mean(val): return stats_wrapper(val, lambda n, u, d: (np.mean(n), pooled_sd(n, u), pcot.dq.NONE)) and it will work on scalar Values, vector Values and images - in the latter case, producing a vector Value. The pooled_sd function will take the nominal and uncertainty arrays and pool their variation into a single value. This is done using the method in Rudmin, J. W. (2010) \"Calculating the exact pooled variance\" arXiv preprint arXiv:1007.1012. We assume that each value in the array was obtained from sample sets of the same size.","title":"How Values work"},{"location":"devguide/values/#how-values-work","text":"Value is PCOT's fundamental type for working with uncertain numerical data. Values are triplets of: a 32-bit floating point nominal value (i.e. a mean), a 32-bit floating point uncertainty value (as standard deviation) around that mean, and a set of 16 data quality (DQ) bits. These are usually scalar, but 1D vectors can also be created. As a user of PCOT you may never encounter Values, but they are used internally whenever any operations involving uncertainty are done. Here are the typical situations where that occurs:","title":"How Values work"},{"location":"devguide/values/#binary-and-unary-operations-on-value-objects","text":"This is the \"base case\". Values have dunder methods for operations. These are usually quite simple, although some (such as exponentiation) are nasty. They: perform the operation to get the new nominal value. This is usually done with a lambda function, relying on Numpy's broadcasting to do the \"right thing\" with what it's given. call a function to calculate the new uncertainty value call a function to combine the two DQs pass the three results into Value's constructor to get a new value Examples - the multiplication operator: def __mul__(self, other): return Value(self.n * other.n, mul_unc(self.n, self.u, other.n, other.u), combineDQs(self, other)) The AND operator: def __and__(self, other): \"\"\"The & operator actually finds the minimum (Zadeh fuzzy op)\"\"\" n = np.where(self.n > other.n, other.n, self.n) u = np.where(self.n > other.n, other.u, self.u) d = np.where(self.n > other.n, other.dq, self.dq) return Value(n, u, d)","title":"Binary and unary operations on Value objects"},{"location":"devguide/values/#binary-and-unary-operations-on-datum-objects","text":"This is for binops - unary operations are much the same, but a lot simpler (although imageUnop currently takes the underlying numpy array). Datum dunder function runs - this is usually very simple. For example, for addition it's just return ops.binop(ops.Operator.ADD, self, other) . Calls pcot.expressions.ops.binop with the appopriate ops.Operator code ops.binop runs converting any raw numbers to Datum.NUMBER data with zero uncertainty. Each possible triple (operator, leftdatum, rightdatum) of operator and Datum types will have a \"binop semantics Datum wrapper\" method registered for it by ops.initOps. For example, (multiplication, Datum.IMG, Datum.NUMBER) will call ops.imageNumberBinop with the lambda lambda x,y: x*y where the latter will take and return Value. ops.binop calls the binop semantics method for the two Datum types For pairings of numbers and/or images, The binop semantics Datum wrapper converts any image data into a Value after performing subimage extraction, then calls the provided lambda. This calls the appropriate dunder method on the two Values. In fact, we could replace lambda x,y: x*y with Value.__mul__ etc., but it's much clearer this way. see above for details of this. In imageNumberBinop (or equivalent) the returned array Value will be stitched back into the left-hand image using ImageCube.modifyWithSub , and that new image will be returned wrapped in a Datum. If the semantic wrapper was numberBinop, so that a number is returned, we just wrap that in a Datum. For other types - not Datum.NUMBER or Datum.IMG - other semantics methods will have been written. For example, the ROI types have binary operators which construct ROIBinop objects using dunder methods: def regROIBinopSemantics(op, fn): \"\"\"Used to register binops for ROIs, which support a subset of ops.\"\"\" registerBinopSemantics(op, Datum.ROI, Datum.ROI, lambda dx, dy: ROIBinop(dx, dy, fn)) regROIBinopSemantics(Operator.ADD, lambda x, y: x + y) regROIBinopSemantics(Operator.SUB, lambda x, y: x - y) regROIBinopSemantics(Operator.MUL, lambda x, y: x * y) regROIBinopSemantics(Operator.DIV, lambda x, y: x / y) regROIBinopSemantics(Operator.POW, lambda x, y: x ** y)","title":"Binary and unary operations on Datum objects"},{"location":"devguide/values/#binary-and-unary-operations-in-an-expr-node","text":"pcot.expressions.parse.InstOp runs, popping two Datum objects and calling a callback function stored in binopRegistry. This callback will have been registered by Parser.registerBinop, and is simply a lambda calling pcot.expressions.ops.binop with an Operator code for the operator, e.g. p.registerBinop('*', 20, lambda a, b: binop(Operator.MUL, a, b)) (20 is the precedence) The callback will call ops.binop, which is step 3 in the preceding section. Again, unary operations are very similar but rather simpler.","title":"Binary and unary operations in an expr node"},{"location":"devguide/values/#functions-of-value-objects","text":"Value generally has a method for each supported function. For example, the tan method will perform the trigonometric tan with uncertainty, returning a new Value. This is rarely called directly - instead, we generally work with Datum objects.","title":"functions of Value objects"},{"location":"devguide/values/#datumfuncs-functions-of-datum-objects","text":"These are registered with the @datumfunc operator (see the plugins documentation ) which can be found in pcot.expressions.register. This registers two separate wrappers: the expression wrapper, used when the function is called from an expr node; and the Python function wrapper, called when the function is called from inside Python. They can take any number of arguments, but they must be either Datum objects or numeric.","title":"datumfuncs: functions of Datum objects"},{"location":"devguide/values/#datumfuncs-called-from-python","text":"The @datumfunc decorator will have wrapped the function in the Python decorator, so that subsequent calls from Python will go through the wrapper. This wrapper will set default values for missing arguments (which must be string or numeric) convert numeric and string arguments to Datum objects call the original function and return the result (which must be Datum)","title":"Datumfuncs called from Python"},{"location":"devguide/values/#datumfuncs-called-from-expr-nodes","text":"The @datumfunc decorator also wraps the function in another wrapper - the exprwrapper - and registers the wrapped function with the expression parser. The wrapper itself is simpler than the Python wrapper because it can make use of facilities in the parser to convert values and check for errors. When data reaches the exprwrapper, we already know that the arguments are all Datum objects.","title":"Datumfuncs called from expr nodes"},{"location":"devguide/values/#datumfuncs-of-single-numericscalar-arguments","text":"Functions of a single numeric or image Datum are sometimes written using an \"inner wrapper\", which will turn imagecubes and numbers into Value objects in a similar way to how the semantic binop wrappers work for operations. It will also wrap the resulting Value in a Datum. An example is pcot.datumfuncs.func_wrapper . Here is the datumfunc sin in full: @datumfunc def sin(a): \"\"\" Calculate sine of an angle in radians @param a:img,number:the angle (or image in which each pixel is a single angle) \"\"\" return func_wrapper(lambda xx: xx.sin(), a) We don't need worry about whether the argument is an image or a scalar, because the func_wrapper will deal with it. However, func_wrapper can only deal with images and scalars.","title":"Datumfuncs of single numeric/scalar arguments"},{"location":"devguide/values/#the-stats-wrapper","text":"Another datumfunc inner wrapper which can be used is stats_wrapper. This wraps a function which takes a (nominal,uncertainty,dq) tuple and returns another tuple of the same type. It does the following: If provided with a numeric value, calls the wrapped function and creates a Value from the returned data. If provided with an ImageCube, gets the subimage, splits it into bands, and calls the wrapped function on the non-BAD values in each band. The results for each band - assumed to be scalar - are converted into a vector Value with one value for each band. This lets us write the mean like this: @datumfunc def mean(val): return stats_wrapper(val, lambda n, u, d: (np.mean(n), pooled_sd(n, u), pcot.dq.NONE)) and it will work on scalar Values, vector Values and images - in the latter case, producing a vector Value. The pooled_sd function will take the nominal and uncertainty arrays and pool their variation into a single value. This is done using the method in Rudmin, J. W. (2010) \"Calculating the exact pooled variance\" arXiv preprint arXiv:1007.1012. We assume that each value in the array was obtained from sample sets of the same size.","title":"The stats wrapper"},{"location":"gettingstarted/","text":"Overview This provides a basic introduction to PCOT, telling you how to install it, providing important information on the core concepts behind PCOT, and giving a basic tutorial. Once you have tinkered with PCOT a little the User Guide will provide reference information. How to install and run PCOT Introduction to core PCOT concepts A first tutorial Video introduction There is a brief video introduction to PCOT.","title":"Overview"},{"location":"gettingstarted/#overview","text":"This provides a basic introduction to PCOT, telling you how to install it, providing important information on the core concepts behind PCOT, and giving a basic tutorial. Once you have tinkered with PCOT a little the User Guide will provide reference information. How to install and run PCOT Introduction to core PCOT concepts A first tutorial","title":"Overview"},{"location":"gettingstarted/#video-introduction","text":"There is a brief video introduction to PCOT.","title":"Video introduction"},{"location":"gettingstarted/concepts/","text":"Concepts How PCOT works (and why) PCOT was originally designed to help scientists and engineers analyse PanCam data and produce useful secondary data. It acts downstream from the Rover Operations Control Centre (ROCC) on images which have already been processed to some extent, and is a successor to ExoSpec 1 . As such, its primary purpose is to generate relative reflectance images and spectral parameter maps, although it will also be able to produce spectra from regions of interest. Indeed, it should be flexible enough to perform a wide range of unforeseen calculations. PCOT can also handle many other kinds of data. It is particularly suited to processing multispectral images with uncertainty and error data, and can currently read PDS4 and ENVI formats, alongside more common RGB formats which can be collated into multispectral images. Of paramount importance is the verifiability and reproducibility of data generated from PCOT. To this end, a PCOT document is a data product which can be shared between users, which also fully describes how the data was generated from the initial import to the final output. Users are encouraged to exchange PCOT documents in addition to, or instead of, the generated images or data. The Graph To achieve this, a PCOT document manipulates data in a graph - a network of nodes, each of which takes some data, works on it, perhaps displays some data, and perhaps outputs derived data. Technically speaking, this is a \"directed acyclic graph\": each connection has a direction, going from the output of one node to the input of another, and there can't be any loops. As an example, consider that we might want to overlay some kind of spectral parameter map, converted to a colour gradient, over an RGB image (note: I'm not a geologist, I'm a software engineer, so perhaps this is a very artificial example). One way to do it might be this: Figure: An example graph. Click on image to expand. We can see the graph in the panel on the right, showing each node as a box with connections to other nodes (ignore the green numbers, they just show how many times each node has run - it's a debugging aid!) The colours on the input connectors at the top and the output connectors at the bottom show the types of the values accepted and produced by those connectors, as shown on this page . Here's what each node in the graph is doing: The input 0 node reads input number 0 into the graph. The inputs are set up separately from the graph, and can be multispectral images or other data (e.g. housekeeping) from outside PCOT. The rect node lets the user draw a rectangle on the image to define a region of interest. Images can have many regions of interest and several different kinds are available. The node with 4 inputs a,b,c,d is an expr node, which calculates the result of a mathematical expression performed on each pixel. The node is showing the expression it is running: a$671 / a$438 . This will read the bands whose wavelengths are 671nm and 438nm in the node's a input, and find their ratio for every pixel. The result will be a single-band image. Expr nodes can perform much more complex calculations than this. The gradient node will convert a single-band image into an RGB image with a user-defined gradient and inset it into the RGB representation of another image - here we are insetting into the input image, using the RGB representation used by that node. Here is another example, showing a spectral plot: The input node again brings a multispectral image into the graph. The multidot node adds a number of small, circular regions of interest. Each has a different name and colour, in this case set automatically to just numbers and random colours. Creating the regions is as easy as clicking on the image. The spectrum node plots a spectrogram of the regions present in the image for all the wavelengths in that image. Figure: Spectrogram example. Click on image to expand. Here I have \"undocked\" the spectrum node's view to be a separate window for easy viewing. The spectrum can also be saved as a PDF or converted into CSV data. I'm also showing the entire app, including the menu bar and four input buttons. The Document A PCOT document is a file which can be shared among users. It consists of The inputs - data loaded from sources external to PCOT. These are kept separate from the graph, because you might want to use a different graph on the same inputs, or the load the same inputs into a different graph. There are currently up to four inputs, but this can easily be changed. The graph - a set of nodes and connections between them which define operations to be performed on inputs, as shown above. The settings - these are global to the entire application. Macros - these are sets of nodes which can be used multiple times and appear as single nodes in the graph, although each one has its own \"private\" graph. Currently very experimental (and largely undocumented). Important The data being sent out of the inputs into the graph is saved and loaded with the document, so the original source data does not need to be stored - you can send the document to someone and it will still work even if they don't have the sources. Quantities All numerical quantities in PCOT - whether scalar values or pixels in images - consist of three values: nominal value, uncertainty and data quality (DQ) bits. Uncertainty and DQ bitss can be viewed as overlays in the canvas (PCOT's image viewer component). Uncertainty The uncertainty is expressed as standard deviation, and is propagated through most operations. Be careful here, however - all values are assumed to be independent. This can lead to grossly incorrect results . For example, we could set up a graph consisting of a node with the value 0\\pm1 0\\pm1 , feed it into a mathematical expression ( expr ) node as variable *a), and set the node's expression to a-a a-a : Figure: Incorrect uncertainty calculation. Click on image to expand. The result, 0\\pm\\sqrt{2} 0\\pm\\sqrt{2} , is a consequence of the subtraction assuming that all quantities are independent even when they are clearly the same quantity. Data quality bits Each pixel in an image has a set of bits describing its quality in the source data, which are propagated through all operations. Additionally, other DQ bits may be added as the data moves through the graph - for example, division by zero may occur, or an operation whose result is undefined for those values. While DQ bits are flexible, the following are probably stable: Bit Meaning NODATA no data present (typically for a particular pixel) NOUNCERTAINTY Pixel has no uncertainty (again typically for pixels) SAT Data is saturated DIVZERO A division by zero has occurred UNDEF Result of operation is undefined COMPLEX Data is real part of complex result ERROR Unspecified error There may also be user-defined or \"test\" bits for use in development. Move on to a First Tutorial Allender, Elyse J., Roger B. Stabbins, Matthew D. Gunn, Claire R. Cousins, and Andrew J. Coates. \"The ExoMars spectral tool (ExoSpec): An image analysis tool for ExoMars 2020 PanCam imagery.\" In Image and Signal Processing for Remote Sensing XXIV , vol. 10789, pp. 163-181. SPIE, 2018. link to PDF \u21a9","title":"PCOT concepts"},{"location":"gettingstarted/concepts/#concepts","text":"How PCOT works (and why) PCOT was originally designed to help scientists and engineers analyse PanCam data and produce useful secondary data. It acts downstream from the Rover Operations Control Centre (ROCC) on images which have already been processed to some extent, and is a successor to ExoSpec 1 . As such, its primary purpose is to generate relative reflectance images and spectral parameter maps, although it will also be able to produce spectra from regions of interest. Indeed, it should be flexible enough to perform a wide range of unforeseen calculations. PCOT can also handle many other kinds of data. It is particularly suited to processing multispectral images with uncertainty and error data, and can currently read PDS4 and ENVI formats, alongside more common RGB formats which can be collated into multispectral images. Of paramount importance is the verifiability and reproducibility of data generated from PCOT. To this end, a PCOT document is a data product which can be shared between users, which also fully describes how the data was generated from the initial import to the final output. Users are encouraged to exchange PCOT documents in addition to, or instead of, the generated images or data.","title":"Concepts"},{"location":"gettingstarted/concepts/#the-graph","text":"To achieve this, a PCOT document manipulates data in a graph - a network of nodes, each of which takes some data, works on it, perhaps displays some data, and perhaps outputs derived data. Technically speaking, this is a \"directed acyclic graph\": each connection has a direction, going from the output of one node to the input of another, and there can't be any loops. As an example, consider that we might want to overlay some kind of spectral parameter map, converted to a colour gradient, over an RGB image (note: I'm not a geologist, I'm a software engineer, so perhaps this is a very artificial example). One way to do it might be this: Figure: An example graph. Click on image to expand. We can see the graph in the panel on the right, showing each node as a box with connections to other nodes (ignore the green numbers, they just show how many times each node has run - it's a debugging aid!) The colours on the input connectors at the top and the output connectors at the bottom show the types of the values accepted and produced by those connectors, as shown on this page . Here's what each node in the graph is doing: The input 0 node reads input number 0 into the graph. The inputs are set up separately from the graph, and can be multispectral images or other data (e.g. housekeeping) from outside PCOT. The rect node lets the user draw a rectangle on the image to define a region of interest. Images can have many regions of interest and several different kinds are available. The node with 4 inputs a,b,c,d is an expr node, which calculates the result of a mathematical expression performed on each pixel. The node is showing the expression it is running: a$671 / a$438 . This will read the bands whose wavelengths are 671nm and 438nm in the node's a input, and find their ratio for every pixel. The result will be a single-band image. Expr nodes can perform much more complex calculations than this. The gradient node will convert a single-band image into an RGB image with a user-defined gradient and inset it into the RGB representation of another image - here we are insetting into the input image, using the RGB representation used by that node. Here is another example, showing a spectral plot: The input node again brings a multispectral image into the graph. The multidot node adds a number of small, circular regions of interest. Each has a different name and colour, in this case set automatically to just numbers and random colours. Creating the regions is as easy as clicking on the image. The spectrum node plots a spectrogram of the regions present in the image for all the wavelengths in that image. Figure: Spectrogram example. Click on image to expand. Here I have \"undocked\" the spectrum node's view to be a separate window for easy viewing. The spectrum can also be saved as a PDF or converted into CSV data. I'm also showing the entire app, including the menu bar and four input buttons.","title":"The Graph"},{"location":"gettingstarted/concepts/#the-document","text":"A PCOT document is a file which can be shared among users. It consists of The inputs - data loaded from sources external to PCOT. These are kept separate from the graph, because you might want to use a different graph on the same inputs, or the load the same inputs into a different graph. There are currently up to four inputs, but this can easily be changed. The graph - a set of nodes and connections between them which define operations to be performed on inputs, as shown above. The settings - these are global to the entire application. Macros - these are sets of nodes which can be used multiple times and appear as single nodes in the graph, although each one has its own \"private\" graph. Currently very experimental (and largely undocumented). Important The data being sent out of the inputs into the graph is saved and loaded with the document, so the original source data does not need to be stored - you can send the document to someone and it will still work even if they don't have the sources.","title":"The Document"},{"location":"gettingstarted/concepts/#quantities","text":"All numerical quantities in PCOT - whether scalar values or pixels in images - consist of three values: nominal value, uncertainty and data quality (DQ) bits. Uncertainty and DQ bitss can be viewed as overlays in the canvas (PCOT's image viewer component).","title":"Quantities"},{"location":"gettingstarted/concepts/#uncertainty","text":"The uncertainty is expressed as standard deviation, and is propagated through most operations. Be careful here, however - all values are assumed to be independent. This can lead to grossly incorrect results . For example, we could set up a graph consisting of a node with the value 0\\pm1 0\\pm1 , feed it into a mathematical expression ( expr ) node as variable *a), and set the node's expression to a-a a-a : Figure: Incorrect uncertainty calculation. Click on image to expand. The result, 0\\pm\\sqrt{2} 0\\pm\\sqrt{2} , is a consequence of the subtraction assuming that all quantities are independent even when they are clearly the same quantity.","title":"Uncertainty"},{"location":"gettingstarted/concepts/#data-quality-bits","text":"Each pixel in an image has a set of bits describing its quality in the source data, which are propagated through all operations. Additionally, other DQ bits may be added as the data moves through the graph - for example, division by zero may occur, or an operation whose result is undefined for those values. While DQ bits are flexible, the following are probably stable: Bit Meaning NODATA no data present (typically for a particular pixel) NOUNCERTAINTY Pixel has no uncertainty (again typically for pixels) SAT Data is saturated DIVZERO A division by zero has occurred UNDEF Result of operation is undefined COMPLEX Data is real part of complex result ERROR Unspecified error There may also be user-defined or \"test\" bits for use in development. Move on to a First Tutorial Allender, Elyse J., Roger B. Stabbins, Matthew D. Gunn, Claire R. Cousins, and Andrew J. Coates. \"The ExoMars spectral tool (ExoSpec): An image analysis tool for ExoMars 2020 PanCam imagery.\" In Image and Signal Processing for Remote Sensing XXIV , vol. 10789, pp. 163-181. SPIE, 2018. link to PDF \u21a9","title":"Data quality bits"},{"location":"gettingstarted/connectors/","text":"Connector colours These are the current node connector colours - this list is subject to change. colour/pattern type notes red any type accepted input only dark blue image cyan region of interest dark green numeric data scalar or 1D vector crosshatch black variant output only, used in macros, currently experimental diagonal red none output only, output type is variable and node has not yet run gold test result used in unit tests data generic data e.g. spectra","title":"Connector colours"},{"location":"gettingstarted/connectors/#connector-colours","text":"These are the current node connector colours - this list is subject to change. colour/pattern type notes red any type accepted input only dark blue image cyan region of interest dark green numeric data scalar or 1D vector crosshatch black variant output only, used in macros, currently experimental diagonal red none output only, output type is variable and node has not yet run gold test result used in unit tests data generic data e.g. spectra","title":"Connector colours"},{"location":"gettingstarted/help/","text":"Getting help Nodes Inside the PCOT app there are three ways to get help on a node: Double-clicking on the little blue box in the top right corner of that node in the graph, Using the right-click context menu on a node, Using the right-click context menu on a button in the palette. The node help texts are also available in the automatically generated documentation . If this isn't enough, don't hesitate to contact the Aberystwyth team. Expressions Doing the above on an expr node will tell you what operations, functions and properties are available. This text is also available in the automatically generated documentation for this node . You can also get help on properties and functions by right clicking in the log box at the bottom and selecting the appropriate option. Inside the expression box in the expr node, you can right-click on most things and ask for help.","title":"Getting help"},{"location":"gettingstarted/help/#getting-help","text":"","title":"Getting help"},{"location":"gettingstarted/help/#nodes","text":"Inside the PCOT app there are three ways to get help on a node: Double-clicking on the little blue box in the top right corner of that node in the graph, Using the right-click context menu on a node, Using the right-click context menu on a button in the palette. The node help texts are also available in the automatically generated documentation . If this isn't enough, don't hesitate to contact the Aberystwyth team.","title":"Nodes"},{"location":"gettingstarted/help/#expressions","text":"Doing the above on an expr node will tell you what operations, functions and properties are available. This text is also available in the automatically generated documentation for this node . You can also get help on properties and functions by right clicking in the log box at the bottom and selecting the appropriate option. Inside the expression box in the expr node, you can right-click on most things and ask for help.","title":"Expressions"},{"location":"gettingstarted/inputs/","text":"Loading different image formats The examples in the tutorial use ENVI images, but other image formats are available: plain RGB \"multiband\" images (multiple monochrome PNG images, one per band) PDS4 products (work in progress) Loading an ENVI image To reiterate, this is how to load an image from an ENVI file: Click an input button to open an input, and select ENVI. Click on \"Select Directory\" and choose a directory with ENVI files. Double click on .hdr files in the main file view. This will load the file into the input. Note that ENVI is a \"two-file\" format - the .dat files contain the data, while .hdr files of the same name in the same directory contain information about the data. We only show the .hdr file in the file view, but the corresponding .dat file must be present too. ENVI files are a useful multispectral format, and we can provide scripts to convert raw image sets into ENVI if required. We currently only support images which are 32-bit floating point in the BSQ (band-sequential) format. Loading an RGB image To load RGB, open an input and click the RGB button. A dialog will appear which is very similar to the ENVI file dialog above , but showing ENVI header files instead of image files. Double-click on such a file to load its associated data, which is assumed to be in the same directory with the same name. RGB images don't have filters, so there is no wavelength information - instead, the channels are named R, G and B and these names can be used in expr nodes (e.g. a$R ). Loading a \"multiband\" image Sometimes data is provided as a set of monochrome PNG files, although this is clearly far from ideal. In this case we need to tell PCOT how to work out which filter is being used for each file. Again, we open the dialog by clicking on an input button and clicking the appropriate method - \"Multifile\" in this case. This will open the ENVI dialog, which is rather more complex than the RGB or ENVI dialogs: Figure: An open Multiband input. Click on image to expand. The procedure here is roughly this: Make sure the file names contain the the lens (left or right) as L or R, and the filter position number. Work out a regular expression which can find these in the file name. Hopefully you can just use the default, which assumes that the filename contains a string like LWAC01 for left lens, position 01; or RWAC10 for right lens, position 10. If your filenames don't have this format and it's too difficult to rename them, you'll have to write an RE yourself or find a handy IT person to help. Get the files into a single directory and open the input dialog as shown above. Determine which camera configuration produced the images (PANCAM or AUPE are supported but more can be added) and set the Camera option accordingly. Different setups use different filter sets, and will translate filter positions into different filter wavelengths and names. Click the \"Get Directory\" button. In the new dialog, select a directory containing the image files and and click \"Choose.\" A lot of files will appear in the Files box. Double-click images to preview them as a single channel. If they are dark, select an appropriate multiplier and double-click again. Click in image checkboxes to select them; selected images will be combined into a single multispectral image. Close the dialog when you have the selected images you want. It might be a good idea to create an input node to examine the resulting multispectral image. There is more detail on the regular expression syntax here . PDS4 products This is very much work in progress - please contact the developers if you need this functionality soon.","title":"Loading other image formats"},{"location":"gettingstarted/inputs/#loading-different-image-formats","text":"The examples in the tutorial use ENVI images, but other image formats are available: plain RGB \"multiband\" images (multiple monochrome PNG images, one per band) PDS4 products (work in progress)","title":"Loading different image formats"},{"location":"gettingstarted/inputs/#loading-an-envi-image","text":"To reiterate, this is how to load an image from an ENVI file: Click an input button to open an input, and select ENVI. Click on \"Select Directory\" and choose a directory with ENVI files. Double click on .hdr files in the main file view. This will load the file into the input. Note that ENVI is a \"two-file\" format - the .dat files contain the data, while .hdr files of the same name in the same directory contain information about the data. We only show the .hdr file in the file view, but the corresponding .dat file must be present too. ENVI files are a useful multispectral format, and we can provide scripts to convert raw image sets into ENVI if required. We currently only support images which are 32-bit floating point in the BSQ (band-sequential) format.","title":"Loading an ENVI image"},{"location":"gettingstarted/inputs/#loading-an-rgb-image","text":"To load RGB, open an input and click the RGB button. A dialog will appear which is very similar to the ENVI file dialog above , but showing ENVI header files instead of image files. Double-click on such a file to load its associated data, which is assumed to be in the same directory with the same name. RGB images don't have filters, so there is no wavelength information - instead, the channels are named R, G and B and these names can be used in expr nodes (e.g. a$R ).","title":"Loading an RGB image"},{"location":"gettingstarted/inputs/#loading-a-multiband-image","text":"Sometimes data is provided as a set of monochrome PNG files, although this is clearly far from ideal. In this case we need to tell PCOT how to work out which filter is being used for each file. Again, we open the dialog by clicking on an input button and clicking the appropriate method - \"Multifile\" in this case. This will open the ENVI dialog, which is rather more complex than the RGB or ENVI dialogs: Figure: An open Multiband input. Click on image to expand. The procedure here is roughly this: Make sure the file names contain the the lens (left or right) as L or R, and the filter position number. Work out a regular expression which can find these in the file name. Hopefully you can just use the default, which assumes that the filename contains a string like LWAC01 for left lens, position 01; or RWAC10 for right lens, position 10. If your filenames don't have this format and it's too difficult to rename them, you'll have to write an RE yourself or find a handy IT person to help. Get the files into a single directory and open the input dialog as shown above. Determine which camera configuration produced the images (PANCAM or AUPE are supported but more can be added) and set the Camera option accordingly. Different setups use different filter sets, and will translate filter positions into different filter wavelengths and names. Click the \"Get Directory\" button. In the new dialog, select a directory containing the image files and and click \"Choose.\" A lot of files will appear in the Files box. Double-click images to preview them as a single channel. If they are dark, select an appropriate multiplier and double-click again. Click in image checkboxes to select them; selected images will be combined into a single multispectral image. Close the dialog when you have the selected images you want. It might be a good idea to create an input node to examine the resulting multispectral image. There is more detail on the regular expression syntax here .","title":"Loading a \"multiband\" image"},{"location":"gettingstarted/inputs/#pds4-products","text":"This is very much work in progress - please contact the developers if you need this functionality soon.","title":"PDS4 products"},{"location":"gettingstarted/installrun/","text":"Installing and running PCOT PCOT is a Python program (and library) with a number of dependencies, notably numpy and PySide2 (the official Python interface to Qt). We find the best way to manage these is to use Anaconda and Poetry . Installation has been tested on Windows 10, MacOS and Ubuntu 20.04. Install Anaconda The first thing you will need to do is install Anaconda, which can be done from here: Windows: https://docs.anaconda.com/anaconda/install/windows/ Linux: https://docs.anaconda.com/anaconda/install/linux/ MacOS: https://docs.anaconda.com/anaconda/install/mac-os/ (untested) Obtain the software This can be done by either downloading the archive from Github and extracting it into a new directory, or cloning the repository. In both cases, the top level directory should be called PCOT (this isn't really mandatory but makes the instructions below simpler). The best way to download is this: Open an Anaconda shell window (see below) If you have an SSH key set up for GitHub, type this command into the shell ( changing the repository address if it is different ): git clone git@github.com:AU-ExoMars/PCOT.git Otherwise type this: git clone https://github.com/AU-ExoMars/PCOT.git You should now have a PCOT directory which will contain this file (as README.md) and quite a few others. Opening Anaconda's shell on different OSs Windows: Open the Anaconda Powershell Prompt application, which will have been installed when you installed Anaconda. This can be found in the Start menu under Anaconda3, or in the applications list in Anaconda Navigator (which is also in the Start menu in the same place). Linux and MacOS : just open a Bash shell Installing on Ubuntu / MacOS Assuming you have successfully installed Anaconda and cloned or downloaded PCOT as above: Open a bash shell cd to the PCOT directory (which contains this file). Run the command conda create -n pcot python=3.8 poetry . This will create an environment called pcot which uses Python 3.8 and the Poetry dependency and packaging manager. It may take some time. Activate the environment with conda activate pcot . Now run poetry install . This will set up all the packages PCOT is dependent on and install PCOT. You should now be able to run pcot to start the application. Installing on Windows Assuming you have successfully installed Anaconda and cloned or downloaded PCOT as above: Open the Anaconda PowerShell Prompt application from the Start Menu. cd to the PCOT directory (which contains this file). Run the command conda create -n pcot python=3.8 poetry . This will create an environment called pcot which uses Python 3.8 and the Poetry dependency and packaging manager. It may take some time. Activate the environment with conda activate pcot . Now run poetry install . This will set up all the packages PCOT is dependent on and install PCOT. You should now be able to run pcot to start the application. Running PCOT Once you have installed PCOT as above, you can run it by opening an Anaconda shell and entering the following commands: conda activate pcot pcot Running PCOT inside Pycharm These instructions may be useful if you want to run PCOT inside a debugger - for example, if you are testing a custom node. First set up the Conda environment and interpreter: Open PyCharm and open the PCOT directory as an existing project. Open File/Settings.. (Ctrl+Alt+S) Select Project:PCOT / Python Interpreter If the Python Interpreter is not already Python 3.8 with something like anaconda3/envs/pcot/bin/python Select the cogwheel to the right of the Python Interpreter dropdown and then select Add . Select Conda Environment . Select Existing Environment . Select the environment: it should be something like anaconda3/envs/pcot/bin/python . Select OK . Now set up the run configuration: Select Edit Configurations... (or it might be Add Configuration... ) from the configurations drop down in the menu bar Add a new configuration (the + symbol) and select Python Set Script Path to PCOT/src/pcot/__main__.py Make sure the interpreter is something like Project Default (Python 3.8 (pcot)) , i.e. the Python interpreter of the pcot environment. You should now be able to run and debug PCOT. Environment variables It's a good idea, but not mandatory, to set the environment variable PCOT_USER to a string of the form name <email> . For example, in Linux I have added the following to my .bashrc file: export PCOT_USER=\"Jim Finnis <jcf12@aber.ac.uk>\" This data is added to all saved PCOT graphs. If the environment variable is not set, the username returned by Python's getpass module is used (e.g. 'jcf12'). In case of problems There are a few things which can stop PCOT running - see issues .","title":"Installing and running"},{"location":"gettingstarted/installrun/#installing-and-running-pcot","text":"PCOT is a Python program (and library) with a number of dependencies, notably numpy and PySide2 (the official Python interface to Qt). We find the best way to manage these is to use Anaconda and Poetry . Installation has been tested on Windows 10, MacOS and Ubuntu 20.04.","title":"Installing and running PCOT"},{"location":"gettingstarted/installrun/#install-anaconda","text":"The first thing you will need to do is install Anaconda, which can be done from here: Windows: https://docs.anaconda.com/anaconda/install/windows/ Linux: https://docs.anaconda.com/anaconda/install/linux/ MacOS: https://docs.anaconda.com/anaconda/install/mac-os/ (untested)","title":"Install Anaconda"},{"location":"gettingstarted/installrun/#obtain-the-software","text":"This can be done by either downloading the archive from Github and extracting it into a new directory, or cloning the repository. In both cases, the top level directory should be called PCOT (this isn't really mandatory but makes the instructions below simpler). The best way to download is this: Open an Anaconda shell window (see below) If you have an SSH key set up for GitHub, type this command into the shell ( changing the repository address if it is different ): git clone git@github.com:AU-ExoMars/PCOT.git Otherwise type this: git clone https://github.com/AU-ExoMars/PCOT.git You should now have a PCOT directory which will contain this file (as README.md) and quite a few others.","title":"Obtain the software"},{"location":"gettingstarted/installrun/#opening-anacondas-shell-on-different-oss","text":"Windows: Open the Anaconda Powershell Prompt application, which will have been installed when you installed Anaconda. This can be found in the Start menu under Anaconda3, or in the applications list in Anaconda Navigator (which is also in the Start menu in the same place). Linux and MacOS : just open a Bash shell","title":"Opening Anaconda's shell on different OSs"},{"location":"gettingstarted/installrun/#installing-on-ubuntu-macos","text":"Assuming you have successfully installed Anaconda and cloned or downloaded PCOT as above: Open a bash shell cd to the PCOT directory (which contains this file). Run the command conda create -n pcot python=3.8 poetry . This will create an environment called pcot which uses Python 3.8 and the Poetry dependency and packaging manager. It may take some time. Activate the environment with conda activate pcot . Now run poetry install . This will set up all the packages PCOT is dependent on and install PCOT. You should now be able to run pcot to start the application.","title":"Installing on Ubuntu / MacOS"},{"location":"gettingstarted/installrun/#installing-on-windows","text":"Assuming you have successfully installed Anaconda and cloned or downloaded PCOT as above: Open the Anaconda PowerShell Prompt application from the Start Menu. cd to the PCOT directory (which contains this file). Run the command conda create -n pcot python=3.8 poetry . This will create an environment called pcot which uses Python 3.8 and the Poetry dependency and packaging manager. It may take some time. Activate the environment with conda activate pcot . Now run poetry install . This will set up all the packages PCOT is dependent on and install PCOT. You should now be able to run pcot to start the application.","title":"Installing on Windows"},{"location":"gettingstarted/installrun/#running-pcot","text":"Once you have installed PCOT as above, you can run it by opening an Anaconda shell and entering the following commands: conda activate pcot pcot","title":"Running PCOT"},{"location":"gettingstarted/installrun/#running-pcot-inside-pycharm","text":"These instructions may be useful if you want to run PCOT inside a debugger - for example, if you are testing a custom node. First set up the Conda environment and interpreter: Open PyCharm and open the PCOT directory as an existing project. Open File/Settings.. (Ctrl+Alt+S) Select Project:PCOT / Python Interpreter If the Python Interpreter is not already Python 3.8 with something like anaconda3/envs/pcot/bin/python Select the cogwheel to the right of the Python Interpreter dropdown and then select Add . Select Conda Environment . Select Existing Environment . Select the environment: it should be something like anaconda3/envs/pcot/bin/python . Select OK . Now set up the run configuration: Select Edit Configurations... (or it might be Add Configuration... ) from the configurations drop down in the menu bar Add a new configuration (the + symbol) and select Python Set Script Path to PCOT/src/pcot/__main__.py Make sure the interpreter is something like Project Default (Python 3.8 (pcot)) , i.e. the Python interpreter of the pcot environment. You should now be able to run and debug PCOT.","title":"Running PCOT inside Pycharm"},{"location":"gettingstarted/installrun/#environment-variables","text":"It's a good idea, but not mandatory, to set the environment variable PCOT_USER to a string of the form name <email> . For example, in Linux I have added the following to my .bashrc file: export PCOT_USER=\"Jim Finnis <jcf12@aber.ac.uk>\" This data is added to all saved PCOT graphs. If the environment variable is not set, the username returned by Python's getpass module is used (e.g. 'jcf12').","title":"Environment variables"},{"location":"gettingstarted/installrun/#in-case-of-problems","text":"There are a few things which can stop PCOT running - see issues .","title":"In case of problems"},{"location":"gettingstarted/issues/","text":"Common runtime issues Can't start Qt on Linux This sometimes happens: qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found. This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb. Try this: export QT_DEBUG_PLUGINS=1 pcot to run the program again, and look at the output. You might see errors like this (I've removed some stuff): QFactoryLoader::QFactoryLoader() checking directory path \"[...]envs/pcot/bin/platforms\" ... Cannot load library [...]/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory) QLibraryPrivate::loadPlugin failed on \"...[stuff removed].. (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)\" If that's the case, install the missing package: sudo apt install libxcb-xinerama0 That might help. Otherwise, send a message to us with the output from the QT_DEBUG_PLUGINS run and we will investigate. Conda fails on Windows I have once seen an error involving OpenSSH not being correctly installed on Windows when the conda create... command was run. This happened toward the end of the installation. To fix it, I just ran the command again - it installed OpenSSH correctly and I was able to proceed.","title":"Known issues"},{"location":"gettingstarted/issues/#common-runtime-issues","text":"","title":"Common runtime issues"},{"location":"gettingstarted/issues/#cant-start-qt-on-linux","text":"This sometimes happens: qt.qpa.plugin: Could not load the Qt platform plugin \"xcb\" in \"\" even though it was found. This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem. Available platform plugins are: eglfs, linuxfb, minimal, minimalegl, offscreen, vnc, wayland-egl, wayland, wayland-xcomposite-egl, wayland-xcomposite-glx, webgl, xcb. Try this: export QT_DEBUG_PLUGINS=1 pcot to run the program again, and look at the output. You might see errors like this (I've removed some stuff): QFactoryLoader::QFactoryLoader() checking directory path \"[...]envs/pcot/bin/platforms\" ... Cannot load library [...]/plugins/platforms/libqxcb.so: (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory) QLibraryPrivate::loadPlugin failed on \"...[stuff removed].. (libxcb-xinerama.so.0: cannot open shared object file: No such file or directory)\" If that's the case, install the missing package: sudo apt install libxcb-xinerama0 That might help. Otherwise, send a message to us with the output from the QT_DEBUG_PLUGINS run and we will investigate.","title":"Can't start Qt on Linux"},{"location":"gettingstarted/issues/#conda-fails-on-windows","text":"I have once seen an error involving OpenSSH not being correctly installed on Windows when the conda create... command was run. This happened toward the end of the installation. To fix it, I just ran the command again - it installed OpenSSH correctly and I was able to proceed.","title":"Conda fails on Windows"},{"location":"gettingstarted/tutorial/","text":"A first tutorial This page is a gentle introduction to PCOT's user interface elements, and will walk you through loading image data and performing some simple manipulations. There is also a video guide - it may be a little out of date, because development is progressing quickly but the basics will be the same. In this document, text written with a highlight like this is an instruction to follow. Be aware that this is an early version and there are no doubt a lot of serious problems! Also, the software is changing very quickly, so it may not look exactly as it does in these pages. Preparation PCOT is primarily designed to work with multispectral images. For this tutorial we have prepared an image and packaged it with the rest of the system, but you will need to uncompress it: it's an ENVI image and these can be rather large. It can be found in the exampleData directory as obj4-sand-subset.zip . Uncompress this image somewhere (perhaps into the same directory) producing two files: 1.hdr (the header file) and 1.dat (the data itself). This is a 256x256 image with taken using AUPE (Aberystwyth University Pancam Emulator), with 11 bands of data. It is a subset of a larger image used in testing, and is shown below as an RGB image: Figure: The example image (640nm, 550nm and 440nm bands). Click on image to expand. Now you have this image you can start PCOT . Setting up the camera directory Your data may come from a number of different cameras, and different cameras have different parameters and filter sets. PCOT needs to be able to find the files which describe these cameras. It does this when it starts up. When it can't find these files, PCOT will pop up a dialog box asking if you want to set a camera directory. You should select either the camera directory provided by your team, or the default camera directory - this will be the cameras directory inside the main PCOT directory. More cameras can be downloaded from the PCOT Cookbook site , which also contains useful \"recipes\" for doing common operations. Introduction to the user interface The image below shows the PCOT interface when the program has just been started, with text in red describing each part. Figure: The PCOT user interface. Click on image to expand. The window is divided into several areas: At the top, the input buttons each open one of PCOT's input windows. These describe how information is read into PCOT. Below this and to the left is the node area , which will be empty on startup. Double-clicking on a node in the graph (see below) will open a detailed view for that node. You can have several views open in this area looking at different nodes, and you can undock by double-clicking on the tab at the top of the view. To the right of the node area is the graph . This shows the nodes in the document and their connections. A new document always has an input 0 node, which brings input 0 into the graph. To the right of the graph is the palette , which consists of a number of sections which can be expanded or hidden. Each section a set of buttons, one for each node type. Drag a button onto the graph to create a new node of that type. At the bottom is the log area , the status bar , and a set of global controls . Working with graph nodes Each node is shown as a box with input connections on the top and output connections on the bottom. To illustrate this, consider the following graph you've seen before: Figure: R671_438 inset into source image RGB representation. Click on image to expand. Node boxes Looking at a pair of nodes in the graph: Figure: A pair of nodes. Click on image to expand. The inputs and outputs are coloured by type: blue is perhaps the most common and indicates an image. Red means any kind of data can be connected here, and generally only appears on inputs. Inputs and outputs can also have names (but don't always). Each node has a help box - double-clicking on this pops up help for the node. Each node has a name - usually this is the node's type, but in the case of the expr node it is the expression being calculated (see below). Nodes can have a \"display text\" in blue. These show extra data, such as the output of a calculation in an expr node or the annotation given to a region of interest. Currently nodes without a display text also show the number of times they have been calculated as a debugging aid. Creating new nodes This is done by clicking a node type button in the palette and dragging the button onto the graph. Selecting and opening graph nodes You can select a node in the graph by clicking on it, or by dragging a box which intersects the node. A selected node will be tinted blue. You can open a node for viewing or editing in the node area by double-clicking. If you double-click on the input 0 node, it will turn a dark cyan colour and a view of the node will appear on the left, along with a tab to select that view when multiple node views are open. You can close an open node by clicking on the cross in the node's tab. If you open the input 0 node in a new document you will see this: Figure: PCOT with an open but unused input node. Click on image to expand. The node is cyan because the current view's node is tinted green, and the node is already tinted blue because it is selected in the graph. There are two selection models: Multiple nodes can be selected in the graph; these are tinted blue. A single node's view can be open and currently being edited, this node is tinted green in the graph. Nodes which are both being edited and are selected are tinted cyan. The view for this particular node - input 0 - shows what external input is currently being read into PCOT on the input numbered 0. In this case it shows \"None\" because there is currently no input. \"Undocking\" a node's view Sometimes it is useful to see several node views at the same time. Double-clicking on the tab, where the name of the node type appears, will make the view open in a separate window. This can be done for several views, and the windows can be rearranged as you wish. Closing the window will \"redock\" the view so it appears in the node area as before. Constant and comment nodes These two nodes are special - the boxes in the graph have text fields which can be edited. For constant nodes, the value in the box will be the numeric output of the node. This node has no view, and double-clicking has no effect. For comment nodes, the value in the box is a text comment that can help other users navigate the graph. Once edited, the box can be resized by clicking and dragging its bottom-right corner. The text will flow to fit the box. Double-clicking on a comment node opens a view which provides an alternative way of editing the text, as well as changing the font size and colours. It's also the only way of getting blank lines into the text, since hitting the \"enter\" key will stop editing when you are editing the node text directly in its box. Figure: Comment and constant nodes. Click on image to expand. Loading an image The purpose of the input 0 node is to bring the an input into the graph for manipulation. As noted elsewhere , the graph is separate from the inputs. This makes it easier to run different graphs on the same inputs, or the same graph on different inputs. Clicking on the \"Input 0\" button at the top of the window will open a window to let us change the input, which looks like this: Figure: An input window, with the null input selected. Click on image to expand. Each input supports a number of input methods , only one of which can be active at a time. By default, the method is null , meaning that no data is loaded into that input. PCOT can support several types of multispectral data file: we will load a small ENVI image we uncompressed earlier. Click on ENVI , and the window will show that input's ENVI method settings and select the ENVI method as being active (see below). In the ENVI input window, Click on Select Directory to open a dialog Choose the directory where you stored the 1.hdr and 1.dat files you extracted earlier Double-click on the 1.hdr file You should see something like this: Figure: An open ENVI input. Click on image to expand. The Canvas The right-hand side is a common component in PCOT known as a canvas which will show the image selected. The canvas lets you modify how the RGB channels are mapped using the three combo boxes. Each box gives the band number, the input number and the wavelength. For example, the red channel is coming from band 2, which is from input 0, and has a wavelength of 640nm. If the RGB method is used to load an ordinary RGB image (e.g. from a PNG), the wavelength will not be shown. At the bottom of the image itself are three source indicators : these show what bands within which inputs were used to generate the canvas image. They should show something like [0:640][0:540][0:480] indicating that the red, green and blue channels come from the 640nm, 540nm and 480nm bands in input 0's image. The source indicator may get quite complex - it shows all inputs that have had an effect on each channel of the image. The indicators may be different because the source display is currently under development. The canvas is quite complicated because it does a lot of things, such as displaying spectra for each pixel, normalisation settings, and displaying data quality bits. More information can be found here , but for now: Click the spectrum button to open a spectrum view, and move the cursor over the image to show the spectrum at different pixels. You may need to resize the window, or drag the \"double bar\" which separates the image from the spectrum. Manipulating an image: obtaining a spectral parameter map Let's perform a simple manipulation on an image. Here we will generate a spectral parameter map from two of the image bands. This parameter is R671_438 , the ratio between the 671nm and 438nm bands, which indicates the presence of ferric minerals 1 . Start PCOT and load an image into input 0 as before , by clicking on the Input 0 button, selecting RGB and double-clicking on an image file. Double-click on the input 0 node in the graph - instances of this node bring input 0 into the graph. Open the maths section in the palette by clicking on it. Drag expr in the maths section onto the graph to create an expression evaluation node. Drag a connection from the output of input 0 to the a input of expr . Double-click on the expr node to open its view for editing - this will show an empty output because our mathematical expression is empty. We now have an input feeding into an expression evaluator, which we can edit. First, let's just see one band. Click in the expr view's expression box : this is the box which says \"Enter expression here...\". Enter the string a$671 This says \"select the band with centre wavelength 671nm from input a \". Press \"Run\" in the node's view . You should now see a monochrome image in the node's canvas: the result of the calculation, containing only that single band. Now change the expression to a$671/a$438 and press \"Run\" to see the result, which should be this: Figure: R671_438 parameter from an multispectral image. Click on image to expand. Note that the source indicators in the bottom left of the image are now displaying [0:438&0:671] [0:438&0:671] [0:438&0:671] This indicates that all three RGB channels shown in the canvas are getting their data from the 438 and 671 bands of input 0. Disconnecting nodes and node error states Disconnect the input node by dragging the arrowhead from the a box and releasing it in empty space. This will cause an error: Figure: A node in an error state. Click on image to expand. The EXPR is the kind of error - EXPR is an error in expression parsing/execution. More details can be seen in the log window, which in this case reads Error in expression: variable's input is not connected This is because there is no longer an image feeding into the expr node's input for variable a . Now reconnect the input node to the expr node to fix the error. Adding a region of interest It's a little hard to see what's going on, so we will add a region of interest. This will make the expr node treat the operation differently - only the area inside the rectangle will be norm(a$671/a$438) . Everywhere else in the image, the output will come from the left-hand side of the expression (the 671nm channel). The rules for ROIs are explained more fully on this page . Open the regions section of the palette and add a rect node between the input and expr nodes (the latter of which will be labelled with its expression). Nothing will change, because the rectangle has not been set. Edit the rect node and draw a rectangle by clicking and dragging on the canvas. Set the scale to 5 pixels and type \"671_438\" in the Annotation box , something like this: Figure: R671_438 with a rectangle. Click on image to expand. Now the expr node shows this - only the rectangle has the 671/438 parameter, while the rest of the image shows the 671nm band. To see this, click on the expr tab : Figure: R671_438 with a rectangle. Click on image to expand. The image is dark, because the values of the parameter shown in the rectangle are much greater than those of the 671nm band. It can be hard to adjust the rectangle when you can't see what's happening in the final image - try undocking the rect node and dragging it to a different part of the screen by double-clicking on the tab, so you can edit the rectangle while the expr output is visible. Dock it again by closing the undocked window . To make things more visible still, open the data section and add a gradient node after the expr node and view it , clicking making sure Show ROIs is enabled on its canvas (ROIs are typically retained on derived images): Figure: R671_438 with a rectangle and gradient. Click on image to expand. There's not much to see because of the nature of the image, unfortunately! Note that there is a \"legend\" at top left, and this can be edited and placed by clicking on the Legend button : Figure: R671_438 with a better gradient legend. Click on image to expand. The colour palette for the gradient, incidentally, is the \"viridis\" palette which is helpful for people with certain kinds of colour blindness 2 . Other presets are available by clicking on the Load Preset button, or you can edit the gradient by hand: double-click to add or remove a point, drag to move a point. It is possible to \"inset\" the gradient into the RGB representation used in the input node by passing that to the insetinto input of the gradient: Figure: R671_438 inset into source image RGB representation. Click on image to expand. If you switch to the Legend part of the gradient node and select Left margin for the Legend item , when you export the image with the canvas' Export Image button the legend will be in the margin: Figure: Exported R671_438 (svg). Click on image to expand. Allender, E. J., C. R. Cousins, M. D. Gunn, and C. M. Caudill. \"Multiscale and multispectral characterization of mineralogy with the ExoMars 2022 rover remote sensing payload.\" Earth and Space Science 7, no. 4 (2020): e2019EA000692. \u21a9 Simon Garnier, Noam Ross, Robert Rudis, Ant\u00f4nio P. Camargo, Marco Sciaini, and C\u00e9dric Scherer (2021). viridis(Lite) - Colorblind-Friendly Color Maps for R. viridis package version 0.6.2. \u21a9","title":"Tutorial"},{"location":"gettingstarted/tutorial/#a-first-tutorial","text":"This page is a gentle introduction to PCOT's user interface elements, and will walk you through loading image data and performing some simple manipulations. There is also a video guide - it may be a little out of date, because development is progressing quickly but the basics will be the same. In this document, text written with a highlight like this is an instruction to follow. Be aware that this is an early version and there are no doubt a lot of serious problems! Also, the software is changing very quickly, so it may not look exactly as it does in these pages.","title":"A first tutorial"},{"location":"gettingstarted/tutorial/#preparation","text":"PCOT is primarily designed to work with multispectral images. For this tutorial we have prepared an image and packaged it with the rest of the system, but you will need to uncompress it: it's an ENVI image and these can be rather large. It can be found in the exampleData directory as obj4-sand-subset.zip . Uncompress this image somewhere (perhaps into the same directory) producing two files: 1.hdr (the header file) and 1.dat (the data itself). This is a 256x256 image with taken using AUPE (Aberystwyth University Pancam Emulator), with 11 bands of data. It is a subset of a larger image used in testing, and is shown below as an RGB image: Figure: The example image (640nm, 550nm and 440nm bands). Click on image to expand. Now you have this image you can start PCOT .","title":"Preparation"},{"location":"gettingstarted/tutorial/#setting-up-the-camera-directory","text":"Your data may come from a number of different cameras, and different cameras have different parameters and filter sets. PCOT needs to be able to find the files which describe these cameras. It does this when it starts up. When it can't find these files, PCOT will pop up a dialog box asking if you want to set a camera directory. You should select either the camera directory provided by your team, or the default camera directory - this will be the cameras directory inside the main PCOT directory. More cameras can be downloaded from the PCOT Cookbook site , which also contains useful \"recipes\" for doing common operations.","title":"Setting up the camera directory"},{"location":"gettingstarted/tutorial/#introduction-to-the-user-interface","text":"The image below shows the PCOT interface when the program has just been started, with text in red describing each part. Figure: The PCOT user interface. Click on image to expand. The window is divided into several areas: At the top, the input buttons each open one of PCOT's input windows. These describe how information is read into PCOT. Below this and to the left is the node area , which will be empty on startup. Double-clicking on a node in the graph (see below) will open a detailed view for that node. You can have several views open in this area looking at different nodes, and you can undock by double-clicking on the tab at the top of the view. To the right of the node area is the graph . This shows the nodes in the document and their connections. A new document always has an input 0 node, which brings input 0 into the graph. To the right of the graph is the palette , which consists of a number of sections which can be expanded or hidden. Each section a set of buttons, one for each node type. Drag a button onto the graph to create a new node of that type. At the bottom is the log area , the status bar , and a set of global controls .","title":"Introduction to the user interface"},{"location":"gettingstarted/tutorial/#working-with-graph-nodes","text":"Each node is shown as a box with input connections on the top and output connections on the bottom. To illustrate this, consider the following graph you've seen before: Figure: R671_438 inset into source image RGB representation. Click on image to expand.","title":"Working with graph nodes"},{"location":"gettingstarted/tutorial/#node-boxes","text":"Looking at a pair of nodes in the graph: Figure: A pair of nodes. Click on image to expand. The inputs and outputs are coloured by type: blue is perhaps the most common and indicates an image. Red means any kind of data can be connected here, and generally only appears on inputs. Inputs and outputs can also have names (but don't always). Each node has a help box - double-clicking on this pops up help for the node. Each node has a name - usually this is the node's type, but in the case of the expr node it is the expression being calculated (see below). Nodes can have a \"display text\" in blue. These show extra data, such as the output of a calculation in an expr node or the annotation given to a region of interest. Currently nodes without a display text also show the number of times they have been calculated as a debugging aid.","title":"Node boxes"},{"location":"gettingstarted/tutorial/#creating-new-nodes","text":"This is done by clicking a node type button in the palette and dragging the button onto the graph.","title":"Creating new nodes"},{"location":"gettingstarted/tutorial/#selecting-and-opening-graph-nodes","text":"You can select a node in the graph by clicking on it, or by dragging a box which intersects the node. A selected node will be tinted blue. You can open a node for viewing or editing in the node area by double-clicking. If you double-click on the input 0 node, it will turn a dark cyan colour and a view of the node will appear on the left, along with a tab to select that view when multiple node views are open. You can close an open node by clicking on the cross in the node's tab. If you open the input 0 node in a new document you will see this: Figure: PCOT with an open but unused input node. Click on image to expand. The node is cyan because the current view's node is tinted green, and the node is already tinted blue because it is selected in the graph. There are two selection models: Multiple nodes can be selected in the graph; these are tinted blue. A single node's view can be open and currently being edited, this node is tinted green in the graph. Nodes which are both being edited and are selected are tinted cyan. The view for this particular node - input 0 - shows what external input is currently being read into PCOT on the input numbered 0. In this case it shows \"None\" because there is currently no input.","title":"Selecting and opening graph nodes"},{"location":"gettingstarted/tutorial/#undocking-a-nodes-view","text":"Sometimes it is useful to see several node views at the same time. Double-clicking on the tab, where the name of the node type appears, will make the view open in a separate window. This can be done for several views, and the windows can be rearranged as you wish. Closing the window will \"redock\" the view so it appears in the node area as before.","title":"\"Undocking\" a node's view"},{"location":"gettingstarted/tutorial/#constant-and-comment-nodes","text":"These two nodes are special - the boxes in the graph have text fields which can be edited. For constant nodes, the value in the box will be the numeric output of the node. This node has no view, and double-clicking has no effect. For comment nodes, the value in the box is a text comment that can help other users navigate the graph. Once edited, the box can be resized by clicking and dragging its bottom-right corner. The text will flow to fit the box. Double-clicking on a comment node opens a view which provides an alternative way of editing the text, as well as changing the font size and colours. It's also the only way of getting blank lines into the text, since hitting the \"enter\" key will stop editing when you are editing the node text directly in its box. Figure: Comment and constant nodes. Click on image to expand.","title":"Constant and comment nodes"},{"location":"gettingstarted/tutorial/#loading-an-image","text":"The purpose of the input 0 node is to bring the an input into the graph for manipulation. As noted elsewhere , the graph is separate from the inputs. This makes it easier to run different graphs on the same inputs, or the same graph on different inputs. Clicking on the \"Input 0\" button at the top of the window will open a window to let us change the input, which looks like this: Figure: An input window, with the null input selected. Click on image to expand. Each input supports a number of input methods , only one of which can be active at a time. By default, the method is null , meaning that no data is loaded into that input. PCOT can support several types of multispectral data file: we will load a small ENVI image we uncompressed earlier. Click on ENVI , and the window will show that input's ENVI method settings and select the ENVI method as being active (see below). In the ENVI input window, Click on Select Directory to open a dialog Choose the directory where you stored the 1.hdr and 1.dat files you extracted earlier Double-click on the 1.hdr file You should see something like this: Figure: An open ENVI input. Click on image to expand.","title":"Loading an image"},{"location":"gettingstarted/tutorial/#the-canvas","text":"The right-hand side is a common component in PCOT known as a canvas which will show the image selected. The canvas lets you modify how the RGB channels are mapped using the three combo boxes. Each box gives the band number, the input number and the wavelength. For example, the red channel is coming from band 2, which is from input 0, and has a wavelength of 640nm. If the RGB method is used to load an ordinary RGB image (e.g. from a PNG), the wavelength will not be shown. At the bottom of the image itself are three source indicators : these show what bands within which inputs were used to generate the canvas image. They should show something like [0:640][0:540][0:480] indicating that the red, green and blue channels come from the 640nm, 540nm and 480nm bands in input 0's image. The source indicator may get quite complex - it shows all inputs that have had an effect on each channel of the image. The indicators may be different because the source display is currently under development. The canvas is quite complicated because it does a lot of things, such as displaying spectra for each pixel, normalisation settings, and displaying data quality bits. More information can be found here , but for now: Click the spectrum button to open a spectrum view, and move the cursor over the image to show the spectrum at different pixels. You may need to resize the window, or drag the \"double bar\" which separates the image from the spectrum.","title":"The Canvas"},{"location":"gettingstarted/tutorial/#manipulating-an-image-obtaining-a-spectral-parameter-map","text":"Let's perform a simple manipulation on an image. Here we will generate a spectral parameter map from two of the image bands. This parameter is R671_438 , the ratio between the 671nm and 438nm bands, which indicates the presence of ferric minerals 1 . Start PCOT and load an image into input 0 as before , by clicking on the Input 0 button, selecting RGB and double-clicking on an image file. Double-click on the input 0 node in the graph - instances of this node bring input 0 into the graph. Open the maths section in the palette by clicking on it. Drag expr in the maths section onto the graph to create an expression evaluation node. Drag a connection from the output of input 0 to the a input of expr . Double-click on the expr node to open its view for editing - this will show an empty output because our mathematical expression is empty. We now have an input feeding into an expression evaluator, which we can edit. First, let's just see one band. Click in the expr view's expression box : this is the box which says \"Enter expression here...\". Enter the string a$671 This says \"select the band with centre wavelength 671nm from input a \". Press \"Run\" in the node's view . You should now see a monochrome image in the node's canvas: the result of the calculation, containing only that single band. Now change the expression to a$671/a$438 and press \"Run\" to see the result, which should be this: Figure: R671_438 parameter from an multispectral image. Click on image to expand. Note that the source indicators in the bottom left of the image are now displaying [0:438&0:671] [0:438&0:671] [0:438&0:671] This indicates that all three RGB channels shown in the canvas are getting their data from the 438 and 671 bands of input 0.","title":"Manipulating an image: obtaining a spectral parameter map"},{"location":"gettingstarted/tutorial/#disconnecting-nodes-and-node-error-states","text":"Disconnect the input node by dragging the arrowhead from the a box and releasing it in empty space. This will cause an error: Figure: A node in an error state. Click on image to expand. The EXPR is the kind of error - EXPR is an error in expression parsing/execution. More details can be seen in the log window, which in this case reads Error in expression: variable's input is not connected This is because there is no longer an image feeding into the expr node's input for variable a . Now reconnect the input node to the expr node to fix the error.","title":"Disconnecting nodes and node error states"},{"location":"gettingstarted/tutorial/#adding-a-region-of-interest","text":"It's a little hard to see what's going on, so we will add a region of interest. This will make the expr node treat the operation differently - only the area inside the rectangle will be norm(a$671/a$438) . Everywhere else in the image, the output will come from the left-hand side of the expression (the 671nm channel). The rules for ROIs are explained more fully on this page . Open the regions section of the palette and add a rect node between the input and expr nodes (the latter of which will be labelled with its expression). Nothing will change, because the rectangle has not been set. Edit the rect node and draw a rectangle by clicking and dragging on the canvas. Set the scale to 5 pixels and type \"671_438\" in the Annotation box , something like this: Figure: R671_438 with a rectangle. Click on image to expand. Now the expr node shows this - only the rectangle has the 671/438 parameter, while the rest of the image shows the 671nm band. To see this, click on the expr tab : Figure: R671_438 with a rectangle. Click on image to expand. The image is dark, because the values of the parameter shown in the rectangle are much greater than those of the 671nm band. It can be hard to adjust the rectangle when you can't see what's happening in the final image - try undocking the rect node and dragging it to a different part of the screen by double-clicking on the tab, so you can edit the rectangle while the expr output is visible. Dock it again by closing the undocked window . To make things more visible still, open the data section and add a gradient node after the expr node and view it , clicking making sure Show ROIs is enabled on its canvas (ROIs are typically retained on derived images): Figure: R671_438 with a rectangle and gradient. Click on image to expand. There's not much to see because of the nature of the image, unfortunately! Note that there is a \"legend\" at top left, and this can be edited and placed by clicking on the Legend button : Figure: R671_438 with a better gradient legend. Click on image to expand. The colour palette for the gradient, incidentally, is the \"viridis\" palette which is helpful for people with certain kinds of colour blindness 2 . Other presets are available by clicking on the Load Preset button, or you can edit the gradient by hand: double-click to add or remove a point, drag to move a point. It is possible to \"inset\" the gradient into the RGB representation used in the input node by passing that to the insetinto input of the gradient: Figure: R671_438 inset into source image RGB representation. Click on image to expand. If you switch to the Legend part of the gradient node and select Left margin for the Legend item , when you export the image with the canvas' Export Image button the legend will be in the margin: Figure: Exported R671_438 (svg). Click on image to expand. Allender, E. J., C. R. Cousins, M. D. Gunn, and C. M. Caudill. \"Multiscale and multispectral characterization of mineralogy with the ExoMars 2022 rover remote sensing payload.\" Earth and Space Science 7, no. 4 (2020): e2019EA000692. \u21a9 Simon Garnier, Noam Ross, Robert Rudis, Ant\u00f4nio P. Camargo, Marco Sciaini, and C\u00e9dric Scherer (2021). viridis(Lite) - Colorblind-Friendly Color Maps for R. viridis package version 0.6.2. \u21a9","title":"Adding a region of interest"},{"location":"gettingstarted/tutorial2/","text":"Tutorial part 2 Mathematical operations Performing operations on parts of an image Calculating a spectrum","title":"Tutorial part 2"},{"location":"gettingstarted/tutorial2/#tutorial-part-2","text":"","title":"Tutorial part 2"},{"location":"gettingstarted/tutorial2/#mathematical-operations","text":"","title":"Mathematical operations"},{"location":"gettingstarted/tutorial2/#performing-operations-on-parts-of-an-image","text":"","title":"Performing operations on parts of an image"},{"location":"gettingstarted/tutorial2/#calculating-a-spectrum","text":"","title":"Calculating a spectrum"},{"location":"userguide/","text":"Overview These pages are a reference guide to the PCOT application. If you want to read about using PCOT as a Python library, documentation is available in the Developer's Guide . At the moment, this consists of in-depth pages on particular aspects of PCOT. For introductory information see the Getting Started section, which will tell you how to install and run PCOT and give you a tutorial covering the basics. Contents Operating principles Global controls on the PCOT UI. The canvas and its optional views Special topics Updating PCOT Using the multifile input to read multispectral data from BMP, PNG etc. Using the expr node to perform mathematical operations Batch mode - controlling PCOT with parameter files for batch operations Camera data - how to get and create camera data files containing filter specifications etc. Autodocs Automatically generated documentation for nodes, expression functions and properties, and inputs/outputs in batch files.","title":"Overview"},{"location":"userguide/#overview","text":"These pages are a reference guide to the PCOT application. If you want to read about using PCOT as a Python library, documentation is available in the Developer's Guide . At the moment, this consists of in-depth pages on particular aspects of PCOT. For introductory information see the Getting Started section, which will tell you how to install and run PCOT and give you a tutorial covering the basics.","title":"Overview"},{"location":"userguide/#contents","text":"Operating principles Global controls on the PCOT UI. The canvas and its optional views","title":"Contents"},{"location":"userguide/#special-topics","text":"Updating PCOT Using the multifile input to read multispectral data from BMP, PNG etc. Using the expr node to perform mathematical operations Batch mode - controlling PCOT with parameter files for batch operations Camera data - how to get and create camera data files containing filter specifications etc.","title":"Special topics"},{"location":"userguide/#autodocs","text":"Automatically generated documentation for nodes, expression functions and properties, and inputs/outputs in batch files.","title":"Autodocs"},{"location":"userguide/cameradata/","text":"Camera data Each camera used with PCOT has different filters and other associated data. PCOT needs to know about these, and stores them in camera data files. These are in PCOT's archive format and so have the PARC extension. When you start PCOT for the first time you will need to tell it where to find these files. PCOT will load all the camera files from the provided directory, storing them under names given in the files themselves. Obtaining camera files Initial version of the files are stored in the PCOT repository with PCOT itself, in the cameras directory. For example, a file for PANCAM can be found in PCOT/cameras/pancam.parc . Currently these files hold very little calibration data, just the filter information. You should be able to download camera files containing full data for AUPE and PanCam from the PCOT Cookbook site. Data will be added to this server when it becomes available. The rest of this page describes how to make your own data files if you are working with a new camera. Creating camera files Camera data is generated from a YAML parameter file (and possibly extra data) and stored in a PARC file. To create a camera data file, run the pcot gencam command: pcot gencam cameraname.yaml cameraname.parc Format of the YAML file The camera parameter file should have the following form at minimum: name: AUPE # short name of the camera date: 2025-03-04 # date of the camera data in YYYY-MM-DD format (ISO 8601) author: Jim Finnis <jcf12@aber.ac.uk> # author of the camera data # short description of the camera, much less than 80 chars short: The left WAC on the Aberystwyth University PanCam Emulator # longer description of the camera description: | The Aberystwyth University PANCAM Emulator This dataset represents AUPE as it was on 4th March 2025. # Filters organised thus: filters: C01L: # filter name cwl: 640 # centre wavelength fwhm: 100 # full-width at half-maximum position: L03 # position in camera (e.g. L03 for left wheel, number 3) transmission: 1.0 # transmission ratio description: \"Red broadband\" # short phrase describing the filter C02L: cwl: 540 fwhm: 80 position: L02 transmission: 1.0 description: \"Green broadband\" (and so on) Flatfield data The gencam command will build flatfield data from a large number of images captured for each filter. These images should be stored in a directory for each filter, named for the filter name. For example: filters |-- C01L | |-- 001.png | |-- 002.png | |-- 003.png | |-- 004.png | `-- 005.png `-- C02L |-- 001.png |-- 002.png |-- 003.png |-- 004.png `-- 005.png The name of the directory should be the name or position of the filter according to the filters section of the file. Which is used depends on the key field (see below). The number and names of the files within the directories is unimportant, but they must be monochrome images of the same format and size. The files are processed as follows: All files are loaded in and processed into floating point data in the range [0,1] [0,1] . The files are scanned for saturated data (data equal to 1) and these pixels are masked out. The mean is found of each pixel, disregarding saturated pixels, and the results stored in a single image. The uncertainty is calculated as the std. dev. of the pixels used. If all bits for a pixel were saturated, the DQ SAT bit for that pixel is set and its value is set to zero. The result is not divided by the mean or processed further in any way - later this may be done when darkfields are implemented, but this can be done downstream for the moment. For flats to be included the YAML file should contain a flats section like this: flats: # enabled: yes # uncomment this line to avoid saving flatfield data (to save space) directory: foo/filters # the path to the directory, e.g. \"filters\" in example above extension: png # extension of image files (png or bin) bitdepth: 10 # how many bits are used in the data - this is 16bit data but only # the lower 10 bits are used - data will be multiplied up key: name # \"name\" or \"position\" - the filter directories can be named for either If you are creating the data from raw files - and this is typically the case - you will need to specify how those files should be loaded. This can be done with the rawloader subsection, which consists of a full specification for the raw loader (recommended) or a preset. You do not need to specify a preset if PNGs are used. To specify the raw loader fully use the following format: flats: directory: flats key: position # the subdirectories are named for the filter position extension: bin # loading raw files bitdepth: 10 rawloader: # u16, 1024x1024, 48-bit offset, bigendian, rotate 90 CCW # preset: pancamraw format: u16 # 16-bit unsigned integer data; others are f32, u8 width: 1024 height: 1024 bigendian: true offset: 48 # offset in bytes to the start of the data rot: 90 # rotate the data 90 degrees counter-clockwise horzflip: false # horizontal flip? No. vertflip: false # vertical flip? No. To use a preset instead, you can use the following format: flats: # ... other fields as above rawloader: preset: pancam-rawloader # use this multifile preset for loading PANCAM files where the preset has been created in the multifile input. However, It's best to specify the data fully rather than risk another user not having the preset when trying to reconstruct the data. See the multifile docs for more details on presets for loading binary files. Remapping filter names in flatfield data Sometimes the directory names are not the same as the filter names or positions. In this case you can use a directory_map dictionary inside the flats section, like this: flats: # ... other fields as above directory_map: # Each key below is a filter position, the value is the directory in which to find that filter. \"01\": \"01\" \"02\": \"02\" \"03\": \"some_other_dir\" # e.g. if filter 03 is in a different directory \"04\": \"yet_another_dir\" # e.g. if filter 04 is in a different directory # ... and so on. Note that every filter position must be listed in the directory_map dictionary, even if the value is the same as the key. Reflectance data This section is likely to change in the future as reflectance data becomes more complex! You can store the reflectance data for each filter in the camera data file for a given calibration target. To do this, you need to add a reflectance section to the YAML file, like this: reflectance: # PCT Colourimetry data from this directory: # ABU-Exomars - Colourimetry\\2023-11-21_colour_targets\\pct\\*-full-br\\*BS* images PCT: lwac_from_colourimetry_pct.csv # reflectance data from the Babelcolour ColourChecker dataset # https://babelcolor.com/colorchecker-2.htm#CCP2_data macbeth: lwac_from_babelcolour_colorchecker.csv The CSV files contain the reflectance per filter for each patch in the target, with the following columns: ROI : (region of interest) the name of the patch in the target filter : the name of the filter n : the mean of the reflectance for that patch in that filter u : the uncertainty in the reflectance for that patch in that filter Here are the first few lines of the lwac_from_colourimetry_pct.csv file: ROI,filter,n,u NG4,C03L,0.07009,0.0052 NG4,C02L,0.07165,0.00442 NG4,C01L,0.07126,0.00565 NG4,G0a,0.07174,0.0043 NG4,G0b,0.06581,0.00508 NG4,G0c,0.07276,0.00456 ... Listing camera data using PCOT Some of this information is duplicated in the multifile input docs You can find out which cameras are available and what data they have using the lscams subcommand: pcot lscams The basic command will just list the cameras, the filename they are stored in, and their short description. Given a camera name, it will only give data for that camera. There are more options: -f will list the filters -l (long) will list the full description of the camera and whether it has flats and reflectance data -F (file) will take a camera data filename rather than a camera name As with all commands you can get a help text with pcot lscams -h For example, to show filters and full details for AUPE_LEFT we could run pcot lscams -fl AUPE_LEFT Further work Later, more information will be added to the camera data files: Extra data (darkfields, BRDF data etc) Extra filter fields (aberration, etc)","title":"Camera data"},{"location":"userguide/cameradata/#camera-data","text":"Each camera used with PCOT has different filters and other associated data. PCOT needs to know about these, and stores them in camera data files. These are in PCOT's archive format and so have the PARC extension. When you start PCOT for the first time you will need to tell it where to find these files. PCOT will load all the camera files from the provided directory, storing them under names given in the files themselves.","title":"Camera data"},{"location":"userguide/cameradata/#obtaining-camera-files","text":"Initial version of the files are stored in the PCOT repository with PCOT itself, in the cameras directory. For example, a file for PANCAM can be found in PCOT/cameras/pancam.parc . Currently these files hold very little calibration data, just the filter information. You should be able to download camera files containing full data for AUPE and PanCam from the PCOT Cookbook site. Data will be added to this server when it becomes available. The rest of this page describes how to make your own data files if you are working with a new camera.","title":"Obtaining camera files"},{"location":"userguide/cameradata/#creating-camera-files","text":"Camera data is generated from a YAML parameter file (and possibly extra data) and stored in a PARC file. To create a camera data file, run the pcot gencam command: pcot gencam cameraname.yaml cameraname.parc","title":"Creating camera files"},{"location":"userguide/cameradata/#format-of-the-yaml-file","text":"The camera parameter file should have the following form at minimum: name: AUPE # short name of the camera date: 2025-03-04 # date of the camera data in YYYY-MM-DD format (ISO 8601) author: Jim Finnis <jcf12@aber.ac.uk> # author of the camera data # short description of the camera, much less than 80 chars short: The left WAC on the Aberystwyth University PanCam Emulator # longer description of the camera description: | The Aberystwyth University PANCAM Emulator This dataset represents AUPE as it was on 4th March 2025. # Filters organised thus: filters: C01L: # filter name cwl: 640 # centre wavelength fwhm: 100 # full-width at half-maximum position: L03 # position in camera (e.g. L03 for left wheel, number 3) transmission: 1.0 # transmission ratio description: \"Red broadband\" # short phrase describing the filter C02L: cwl: 540 fwhm: 80 position: L02 transmission: 1.0 description: \"Green broadband\" (and so on)","title":"Format of the YAML file"},{"location":"userguide/cameradata/#flatfield-data","text":"The gencam command will build flatfield data from a large number of images captured for each filter. These images should be stored in a directory for each filter, named for the filter name. For example: filters |-- C01L | |-- 001.png | |-- 002.png | |-- 003.png | |-- 004.png | `-- 005.png `-- C02L |-- 001.png |-- 002.png |-- 003.png |-- 004.png `-- 005.png The name of the directory should be the name or position of the filter according to the filters section of the file. Which is used depends on the key field (see below). The number and names of the files within the directories is unimportant, but they must be monochrome images of the same format and size. The files are processed as follows: All files are loaded in and processed into floating point data in the range [0,1] [0,1] . The files are scanned for saturated data (data equal to 1) and these pixels are masked out. The mean is found of each pixel, disregarding saturated pixels, and the results stored in a single image. The uncertainty is calculated as the std. dev. of the pixels used. If all bits for a pixel were saturated, the DQ SAT bit for that pixel is set and its value is set to zero. The result is not divided by the mean or processed further in any way - later this may be done when darkfields are implemented, but this can be done downstream for the moment. For flats to be included the YAML file should contain a flats section like this: flats: # enabled: yes # uncomment this line to avoid saving flatfield data (to save space) directory: foo/filters # the path to the directory, e.g. \"filters\" in example above extension: png # extension of image files (png or bin) bitdepth: 10 # how many bits are used in the data - this is 16bit data but only # the lower 10 bits are used - data will be multiplied up key: name # \"name\" or \"position\" - the filter directories can be named for either If you are creating the data from raw files - and this is typically the case - you will need to specify how those files should be loaded. This can be done with the rawloader subsection, which consists of a full specification for the raw loader (recommended) or a preset. You do not need to specify a preset if PNGs are used. To specify the raw loader fully use the following format: flats: directory: flats key: position # the subdirectories are named for the filter position extension: bin # loading raw files bitdepth: 10 rawloader: # u16, 1024x1024, 48-bit offset, bigendian, rotate 90 CCW # preset: pancamraw format: u16 # 16-bit unsigned integer data; others are f32, u8 width: 1024 height: 1024 bigendian: true offset: 48 # offset in bytes to the start of the data rot: 90 # rotate the data 90 degrees counter-clockwise horzflip: false # horizontal flip? No. vertflip: false # vertical flip? No. To use a preset instead, you can use the following format: flats: # ... other fields as above rawloader: preset: pancam-rawloader # use this multifile preset for loading PANCAM files where the preset has been created in the multifile input. However, It's best to specify the data fully rather than risk another user not having the preset when trying to reconstruct the data. See the multifile docs for more details on presets for loading binary files.","title":"Flatfield data"},{"location":"userguide/cameradata/#remapping-filter-names-in-flatfield-data","text":"Sometimes the directory names are not the same as the filter names or positions. In this case you can use a directory_map dictionary inside the flats section, like this: flats: # ... other fields as above directory_map: # Each key below is a filter position, the value is the directory in which to find that filter. \"01\": \"01\" \"02\": \"02\" \"03\": \"some_other_dir\" # e.g. if filter 03 is in a different directory \"04\": \"yet_another_dir\" # e.g. if filter 04 is in a different directory # ... and so on. Note that every filter position must be listed in the directory_map dictionary, even if the value is the same as the key.","title":"Remapping filter names in flatfield data"},{"location":"userguide/cameradata/#reflectance-data","text":"This section is likely to change in the future as reflectance data becomes more complex! You can store the reflectance data for each filter in the camera data file for a given calibration target. To do this, you need to add a reflectance section to the YAML file, like this: reflectance: # PCT Colourimetry data from this directory: # ABU-Exomars - Colourimetry\\2023-11-21_colour_targets\\pct\\*-full-br\\*BS* images PCT: lwac_from_colourimetry_pct.csv # reflectance data from the Babelcolour ColourChecker dataset # https://babelcolor.com/colorchecker-2.htm#CCP2_data macbeth: lwac_from_babelcolour_colorchecker.csv The CSV files contain the reflectance per filter for each patch in the target, with the following columns: ROI : (region of interest) the name of the patch in the target filter : the name of the filter n : the mean of the reflectance for that patch in that filter u : the uncertainty in the reflectance for that patch in that filter Here are the first few lines of the lwac_from_colourimetry_pct.csv file: ROI,filter,n,u NG4,C03L,0.07009,0.0052 NG4,C02L,0.07165,0.00442 NG4,C01L,0.07126,0.00565 NG4,G0a,0.07174,0.0043 NG4,G0b,0.06581,0.00508 NG4,G0c,0.07276,0.00456 ...","title":"Reflectance data"},{"location":"userguide/cameradata/#listing-camera-data-using-pcot","text":"Some of this information is duplicated in the multifile input docs You can find out which cameras are available and what data they have using the lscams subcommand: pcot lscams The basic command will just list the cameras, the filename they are stored in, and their short description. Given a camera name, it will only give data for that camera. There are more options: -f will list the filters -l (long) will list the full description of the camera and whether it has flats and reflectance data -F (file) will take a camera data filename rather than a camera name As with all commands you can get a help text with pcot lscams -h For example, to show filters and full details for AUPE_LEFT we could run pcot lscams -fl AUPE_LEFT","title":"Listing camera data using PCOT"},{"location":"userguide/cameradata/#further-work","text":"Later, more information will be added to the camera data files: Extra data (darkfields, BRDF data etc) Extra filter fields (aberration, etc)","title":"Further work"},{"location":"userguide/canvas/","text":"The Canvas and its optional views Most nodes and inputs use a canvas to display some bands of an image as RGB. This will take up a large proportion of their view - in some cases all of it. It is worth discussing in some detail. A canvas is shown below as the entire control area for an input node, which brings one of the four inputs into the graph. Figure: An open *input 0* node. Click on image to expand. You can pan the canvas using the scroll bars at the edge, and zoom with the mouse wheel. The button at bottom right will reset to show the entire image. To the left of the canvas image itself is the canvas control area. This has quite a lot in it, so is scrollable with collapsible sections, rather like the palette. At the top - and not collapsible - are three combo boxes which determine the image mapping : how the bands within the (probably) multispectral image map onto RGB channels for viewing. Each band in the combo box shows the input number (i.e. which of the four global inputs the data is sourced from), a colon, and typically the name, position or wavelength of the band. Exactly what is shown depends on the image being loaded and the Caption global control . The Guess RGB button tries to guess appropriate channels for the RGB canvas image. Data section This is the first of the collapsible sections. Show ROIs will mark any regions of interest the image has - these are added using nodes like rect (for a rectangle) and are carried forward through subsequent nodes (where appropriate), delimiting the area upon which calculations are performed. They also control the regions used for calculating spectra. Normally an ROI is only shown in the node which adds it. Show spectrum opens a side pane, and dragging the cursor across the image will plot the spectrum of the pixel under the cursor in this pane. If no filter wavelengths are available, a list of the values for each band is shown. The show spectrum pane looks like this: Figure: The \"show spectrum\" pane on an input node's canvas. Click on image to expand. The screenshot isn't showing the cursor, unfortunately, but the spectrum is for the pixel under the cursor. When a spectrum view is opened the image pane can be tiny - to fix this you can resize the PCOT window (or undocked node window), or drag the separator between the image and the spectrum (the two vertical bars). Each dot is shown with an approximation of its wavelength colour (using Dan Bruton's algorithm ) or black if the wavelength is not visible. Export image saves the RGB-mapped image as a PDF, SVG or PNG. If a gradient has been plotted using the gradient node, it may add a legend to the image. Cursor coordinates are shown next, followed by number of pixels and ROIs in the image and the image dimensions (width x height x bands). The hide DQ disables the data quality overlays and hides their sections - data quality can be slow to draw. Normalisation section Nominally, the canvas channels are 0-1 with 0 being zero intensity and 1 being full intensity (so RGB 1,1,1 is white). Values outside the 0-1 range are clamped. However, this can be changed. The norm box selects the normalisation range: to all bands means that the normalisation range of each band for each pixel is the minimum and maximum of all the bands as a whole. to RGB means that the normalisation range is taken from the channels mapped to RGB in the canvas. independent means that each band is normalised independently, to its own range. none means no normalisation is done. If to cropped area is on, the range used for normalisation is taken from the portion of the image visible in the canvas. Data quality layers Each image has a set of data quality bits and an uncertainty value associated with each pixel of each band. Viewing this can be challenging. We make three \"layers\" of DQ data available: each works the same way: SRC specifies which band we are viewing the DQ or uncertainty for. If max is specified and we are viewing uncertainty, the maximum uncertainty across all bands is used. If sum , then the sum of the uncertainties is used. If a DQ bit is being shown, the intersection of those bits across all bands is used for both max and sum . DATA specifies what data is being shown: NONE specifies that the DQ layer is inactive. BIT:name options specify a particular DQ bit. These are subject to change, but are likely to include nodata , nounc (no uncertainty data), sat (saturated). UNC specifies that the uncertainty data should be shown. UNC>THRESH specifies that the layer should be full intensity for pixels where the uncertainty is above a threshold (see below). UNC<THRESH specifies that the layer should be full intensity for pixels where the uncertainty is below a threshold (see below). COL specifies the colour for the DQ layer. transp indicates the transparency of the layer contrast may help improve visibility (it is a simple power function) thresh is the threshold for the threshold DATA modes additive indicates that the data quality layers should be added to the RGB image rather than blended.","title":"The Canvas"},{"location":"userguide/canvas/#the-canvas-and-its-optional-views","text":"Most nodes and inputs use a canvas to display some bands of an image as RGB. This will take up a large proportion of their view - in some cases all of it. It is worth discussing in some detail. A canvas is shown below as the entire control area for an input node, which brings one of the four inputs into the graph. Figure: An open *input 0* node. Click on image to expand. You can pan the canvas using the scroll bars at the edge, and zoom with the mouse wheel. The button at bottom right will reset to show the entire image. To the left of the canvas image itself is the canvas control area. This has quite a lot in it, so is scrollable with collapsible sections, rather like the palette. At the top - and not collapsible - are three combo boxes which determine the image mapping : how the bands within the (probably) multispectral image map onto RGB channels for viewing. Each band in the combo box shows the input number (i.e. which of the four global inputs the data is sourced from), a colon, and typically the name, position or wavelength of the band. Exactly what is shown depends on the image being loaded and the Caption global control . The Guess RGB button tries to guess appropriate channels for the RGB canvas image.","title":"The Canvas and its optional views"},{"location":"userguide/canvas/#data-section","text":"This is the first of the collapsible sections. Show ROIs will mark any regions of interest the image has - these are added using nodes like rect (for a rectangle) and are carried forward through subsequent nodes (where appropriate), delimiting the area upon which calculations are performed. They also control the regions used for calculating spectra. Normally an ROI is only shown in the node which adds it. Show spectrum opens a side pane, and dragging the cursor across the image will plot the spectrum of the pixel under the cursor in this pane. If no filter wavelengths are available, a list of the values for each band is shown. The show spectrum pane looks like this: Figure: The \"show spectrum\" pane on an input node's canvas. Click on image to expand. The screenshot isn't showing the cursor, unfortunately, but the spectrum is for the pixel under the cursor. When a spectrum view is opened the image pane can be tiny - to fix this you can resize the PCOT window (or undocked node window), or drag the separator between the image and the spectrum (the two vertical bars). Each dot is shown with an approximation of its wavelength colour (using Dan Bruton's algorithm ) or black if the wavelength is not visible. Export image saves the RGB-mapped image as a PDF, SVG or PNG. If a gradient has been plotted using the gradient node, it may add a legend to the image. Cursor coordinates are shown next, followed by number of pixels and ROIs in the image and the image dimensions (width x height x bands). The hide DQ disables the data quality overlays and hides their sections - data quality can be slow to draw.","title":"Data section"},{"location":"userguide/canvas/#normalisation-section","text":"Nominally, the canvas channels are 0-1 with 0 being zero intensity and 1 being full intensity (so RGB 1,1,1 is white). Values outside the 0-1 range are clamped. However, this can be changed. The norm box selects the normalisation range: to all bands means that the normalisation range of each band for each pixel is the minimum and maximum of all the bands as a whole. to RGB means that the normalisation range is taken from the channels mapped to RGB in the canvas. independent means that each band is normalised independently, to its own range. none means no normalisation is done. If to cropped area is on, the range used for normalisation is taken from the portion of the image visible in the canvas.","title":"Normalisation section"},{"location":"userguide/canvas/#data-quality-layers","text":"Each image has a set of data quality bits and an uncertainty value associated with each pixel of each band. Viewing this can be challenging. We make three \"layers\" of DQ data available: each works the same way: SRC specifies which band we are viewing the DQ or uncertainty for. If max is specified and we are viewing uncertainty, the maximum uncertainty across all bands is used. If sum , then the sum of the uncertainties is used. If a DQ bit is being shown, the intersection of those bits across all bands is used for both max and sum . DATA specifies what data is being shown: NONE specifies that the DQ layer is inactive. BIT:name options specify a particular DQ bit. These are subject to change, but are likely to include nodata , nounc (no uncertainty data), sat (saturated). UNC specifies that the uncertainty data should be shown. UNC>THRESH specifies that the layer should be full intensity for pixels where the uncertainty is above a threshold (see below). UNC<THRESH specifies that the layer should be full intensity for pixels where the uncertainty is below a threshold (see below). COL specifies the colour for the DQ layer. transp indicates the transparency of the layer contrast may help improve visibility (it is a simple power function) thresh is the threshold for the threshold DATA modes additive indicates that the data quality layers should be added to the RGB image rather than blended.","title":"Data quality layers"},{"location":"userguide/expr/","text":"The expr node The expr \"expression\" node performs mathematical operations on its four inputs, which (currently) must be images or scalar values. Constant scalars can also be used. The node can be found in the \"maths\" section of the palette. It has four inputs and one output, and these can be of any type - the output type will be determined when the expression runs. The expr node is unusual in that the node's box in the graph will show the expression being calculated: Figure: An example of a graph showing an expr node which is running a merge function, combining the channels of two images into a single image.. Click on image to expand. In this example, two images showing slightly different views of the same scene with different bands (one visible light, one infra-red) are registered using the manual register node. They are then merged together using the merge function. The box shows this function, and also IMG[12] indicating that the result is a 12-band image. Functions Merge is one of a large number of functions that expr supports, and more can be added using the plug-in mechanism . Full details of built-in functions can be found in the autodocs Properties Certain types of value have \"properties\" which can be extracted with the dot operator. For example, a.w will extract the width of the image in variable a a , and (a+b).n will return the number of pixels in the image which results from adding images a a and b b . There aren't many properties, but those that exist are listed in the autodocs . Autodocs The text below is drawn from the automatically generated documentation for the node, and is the authoritative documentation. expr Description Expression evaluator. The node box will show the text of the expression. The \"run\" button must be clicked to set the node to the new expression and perform it. The input can accept any type of data and the output type is determined when the node is run. The four inputs are assigned to the variables a, b, c, and d. They are typically (but not necessarily) images or numeric values. The standard operators +,/,*,- and ^ all have their usual meanings. When applied to images they work in a pixel-wise fashion, so if a is an image, 2*a will double the brightness. If b is also an image, a+b will add the two images, pixel by pixel. There are two non-standard operators: . for properties and $ for band extraction. These are described below. Image/numeric operators: operator description precedence (higher binds tighter) A + B add A to B (can act on ROIs) 10 A - B subtract A from B (can act on ROIs) 10 A / B divide A by B (can act on ROIs) 20 A * B multiply A by B (can act on ROIs) 20 A ^ B exponentiate A to the power B (can act on ROIs) 30 -A element-wise negation of A (can act on ROIs) 50 A.B property B of entity A (e.g. a.h is height of image a) 80 A$546 extract single band image of wavelength 546 100 A$_2 extract single band image from band 2 explicitly 100 A&B element-wise minimum of A and B (Zadeh's AND operator) 20 A|B element-wise maximum of A and B (Zadeh's OR operator) 20 !A element-wise 1-A (Zadeh's NOT operator) 50 All operators can act on images, 1D vectors and scalars with the exception of . and $ which have images on the left-hand side and identifiers or integers on the right-hand side. Those operators marked with (can act on ROIs) can also act on pairs of ROIs (regions of interest, see below). Binary operations on image pairs These act by performing the binary operation on the two underlying Numpy arrays. This means you may need to be careful about the ordering of the bands in the two images, because they will simply be operated on in the order they appear. For example, consider adding two images $a$ and $b$ with the same bands in a slightly different order: image a image b result of addition 480nm 480nm sum of 480nm bands 500nm 500nm sum of 500nm bands 610nm 670nm a 's 610nm band plus b 's 670nm band 670nm 610nm copy of previous band (addition being commutative) This probably isn't what you wanted. Note that this is obviously not an issue when an operation is being performed on bands in a single image. Binary operators on images with regions of interest If one of the two images has an ROI, the operation is only performed on that ROI; the remaining area of output is taken from the image without an ROI. If both images have an ROI an error will result - it is likely that this is a mistake on the user's part, and doing something more \"intelligent\" might conceal this. The desired result can be achieved using expr nodes on ROIs and an importroi node. Operations with vectors Some functions can generate vectors, such as mean for getting the means of the bands, and vec for generating vectors by hand. If an image is used in a binary operation with a vector on the other side, the vector must have the same number of elements as there are bands in the image. The operation will be performed on each band. Consider a 3-band image and the vector [2,3,4] . If we multiply them, the result will an image with the first band multiplied by 2, the second band multiplied by 3, and the third band multiplied by 4. Operators on ROIs themselves (as opposed to images with ROIs) operator description a+b union a*b intersection a-b difference You can source ROIs from the \"roi\" output of ROI nodes, and impose resulting ROIs on images with \"importroi\" node. Band extraction The notation $name or $wavelength takes an image on the left-hand side and extracts a single band, generating a new monochrome image. The right-hand side is either a filter name, a filter position, a wavelength or a band index preceded by \"_\". Depending on the camera, all these could be valid: expression meaning a$780 the 780nm band in image a a$_2 band 2 in the image a (a+b)$G0 the band named G0 in the image formed by adding images a and b ((a+b)/2)$780 the average of the 780nm bands of images a and b Be aware of caveats in the \"binary operations on image pairs\" section above: it may be better to extract the band before performing the operation, thus: old expression better expression (a+b)$G0 a$G0 + b$G0 ((a+b)/2)$780 (a$780+b$780)/2 Brackets Round brackets are used to group expressions as usual, but square brackets are used for indexing into a vector. For example, a[3] will extract the fourth element of the vector a . However, square brackets cannot (yet) create a vector. To do this, use the vec function - so vec(1,2,3)[1] will return 2. Band extraction can also be performed with vectors provided the vector elements are numeric (i.e. wavelengths): a $ vec(640,550,440) is valid. Properties Properties are indicated by the . operator, e.g. a.w to find an image's width. Help on functions and properties A list of functions can be obtained by right-clicking on either the log pane or function entry pane and selecting \"List all functions.\" Help on an individual function can be found by hovering over the name of a function, right-clicking and selecting \"Get help on 'somefunction'\". Similar actions are supported for properties. Uncertainties are assumed to be independent in all binary operations While uncertainty is propagated through operations (as population standard deviation) all quantities are assumed to be independent (calculating covariances is beyond the scope of this system). Be very careful here. For example, the uncertainty for the expression tan(a) will be calculated correctly, but if you try to use sin(a)/cos(a) the uncertainty will be incorrect because the nominator and denominator are not independent. Connections Inputs Index Name Type Desc 0 a any (none) 1 b any (none) 2 c any (none) 3 d any (none) Outputs Index Name Type Desc 0 (none) none (none) Parameters expr expr: string (default '') Expression to evaluate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"The \"expr\" node for mathematical expressions"},{"location":"userguide/expr/#the-expr-node","text":"The expr \"expression\" node performs mathematical operations on its four inputs, which (currently) must be images or scalar values. Constant scalars can also be used. The node can be found in the \"maths\" section of the palette. It has four inputs and one output, and these can be of any type - the output type will be determined when the expression runs. The expr node is unusual in that the node's box in the graph will show the expression being calculated: Figure: An example of a graph showing an expr node which is running a merge function, combining the channels of two images into a single image.. Click on image to expand. In this example, two images showing slightly different views of the same scene with different bands (one visible light, one infra-red) are registered using the manual register node. They are then merged together using the merge function. The box shows this function, and also IMG[12] indicating that the result is a 12-band image.","title":"The expr node"},{"location":"userguide/expr/#functions","text":"Merge is one of a large number of functions that expr supports, and more can be added using the plug-in mechanism . Full details of built-in functions can be found in the autodocs","title":"Functions"},{"location":"userguide/expr/#properties","text":"Certain types of value have \"properties\" which can be extracted with the dot operator. For example, a.w will extract the width of the image in variable a a , and (a+b).n will return the number of pixels in the image which results from adding images a a and b b . There aren't many properties, but those that exist are listed in the autodocs .","title":"Properties"},{"location":"userguide/expr/#autodocs","text":"The text below is drawn from the automatically generated documentation for the node, and is the authoritative documentation.","title":"Autodocs"},{"location":"userguide/expr/#expr","text":"","title":"expr"},{"location":"userguide/expr/#description","text":"Expression evaluator. The node box will show the text of the expression. The \"run\" button must be clicked to set the node to the new expression and perform it. The input can accept any type of data and the output type is determined when the node is run. The four inputs are assigned to the variables a, b, c, and d. They are typically (but not necessarily) images or numeric values. The standard operators +,/,*,- and ^ all have their usual meanings. When applied to images they work in a pixel-wise fashion, so if a is an image, 2*a will double the brightness. If b is also an image, a+b will add the two images, pixel by pixel. There are two non-standard operators: . for properties and $ for band extraction. These are described below.","title":"Description"},{"location":"userguide/expr/#connections","text":"","title":"Connections"},{"location":"userguide/expr/#inputs","text":"Index Name Type Desc 0 a any (none) 1 b any (none) 2 c any (none) 3 d any (none)","title":"Inputs"},{"location":"userguide/expr/#outputs","text":"Index Name Type Desc 0 (none) none (none)","title":"Outputs"},{"location":"userguide/expr/#parameters","text":"expr expr: string (default '') Expression to evaluate Automatically generated by generate_autodocs.py Date: 2025-06-12T17:28:29.051109","title":"Parameters"},{"location":"userguide/globalcontrols/","text":"Global controls These are at the bottom right of the PCOT main window, and currently consist of: Caption : select how different bands are labelled in the dropdown: Filter names - bands will be labelled by the name of the filter, e.g. \"C01L\" for the 640nm left-hand red broadband colour filter. Filter positions - the filter's position will be used, e.g. \"L07\" will be used for the C01L band, because it is the filter in position 7 on the left WAC. Wavelengths labels bands according to their centre wavelength, so the same band will be labelled \"640\" under this scheme. Autorun on change will cause each node to automatically perform its action when it is changed (either an input has changed or one of the controls in its tab). This will also cause all nodes \"downstream\" of it in the graph to change. It is sometimes useful to turn this off when changing a node will trigger a very slow action. Run all will cause all root nodes (nodes without inputs) in the graph to run, thus causing all their downstream nodes to run. Effectively, it runs all the nodes in the graph in the correct order.","title":"Global Controls"},{"location":"userguide/globalcontrols/#global-controls","text":"These are at the bottom right of the PCOT main window, and currently consist of: Caption : select how different bands are labelled in the dropdown: Filter names - bands will be labelled by the name of the filter, e.g. \"C01L\" for the 640nm left-hand red broadband colour filter. Filter positions - the filter's position will be used, e.g. \"L07\" will be used for the C01L band, because it is the filter in position 7 on the left WAC. Wavelengths labels bands according to their centre wavelength, so the same band will be labelled \"640\" under this scheme. Autorun on change will cause each node to automatically perform its action when it is changed (either an input has changed or one of the controls in its tab). This will also cause all nodes \"downstream\" of it in the graph to change. It is sometimes useful to turn this off when changing a node will trigger a very slow action. Run all will cause all root nodes (nodes without inputs) in the graph to run, thus causing all their downstream nodes to run. Effectively, it runs all the nodes in the graph in the correct order.","title":"Global controls"},{"location":"userguide/multifile/","text":"The Multifile input method Documentation for the other methods is forthcoming. The regex aspect of multifile is complicated, so documenting it was a priority. The multifile input method allows multiple monochrome files of \"standard\" types like PNG and BMP to be imported as multispectral images. It can also read raw binary files. Each file is flattened to greyscale in the process (if it is RGB) by finding the mean of the three channels for each pixel. Camera data files In order to analyse the images, PCOT needs to be able to find out which filter was used for each image and get information about it. It also needs access to calibration information, for example flat and dark field images, and reflectance data. This data is stored in PARC files - PCOT archives - and is generated by the pcot gencam command. If you want to know to make them yourself, read this documentation . However, PCOT already comes with some camera files. These can be found in the cameras directory inside the PCOT directory. They do not have flat/dark fields because that data is very large, but hopefully you will not need them - that stage of processing will take place upstream in the ROC. Other camera data files can be downloaded from the PCOT Cookbook . Finding out about camera data with lscams Some of this information is duplicated in the camera data docs You can find out about the camera data installed on your system with pcot lscams (list cameras). Alone, this will list the cameras with their short descriptions. You can get a list of the filters for a camera by using the command pcot lscams -f . Alone, this will list all cameras and their filters, but you can specify a camera: pcot lscams -f TRAINING_GEOLOGY will list all filters for the geology filter wheel on the training model, for example (if you have that camera data file installed). Each filter has a name and a position, as well as other data like centre wavelength, FWHM and so on. Filters may also have reflectance and flat/darkfield data - you can see if this is the case from the pcot lscams -l command: if a camera has flats, you'll see \"Has flats\" in the description; if it has reflectance data, you'll see a list of supported reflectance calibration targets. Setting a camera Once you know what camera was used, and you have installed a data file for that camera, you can set it in the multifile input using the Camera widget. Which image is which filter? Setting a file pattern The multifile input method lets you specify a camera, but PCOT still needs to be able to work out which filter was used to capture each band in the image. This is done by extracting the filter name or position from the filename using a regular expression (or regex ). If you have some experience with regular expressions (or access to someone with this experience), it will help immensely . Regular expressions describe patterns which texts might match. For example, the regex c[a-z]t will match any three-letter string starting with c and ending with t : it's c , followed by any character between a and z , followed by t . Beginner's guide Useful 'playground' to try out expressions The default pattern looks something like this: .*[LR]WAC(?P<pos>[0-9][0-9]).* This means: .* matches any number of any character, so there could be anything at the start of the filename. [LR] means \"either L or R \". WAC means we must then have the sequence of letters WAC (?P<pos>[0-9][0-9]) means we must now match two digits ( [0-9] ). We've put them in brackets and preceded them with ?P<pos> which means we should store the result under the name pos . The final .* means that there can now be any number of any character again - so there could be anything at the end of the filename. The idea is that a filename like /home/jim/files/DogBiscuitLWAC02Fish.jpg will be matched, and will result in 02 being stored as pos , which will then be used to look up the filter. You can set the default pattern from the command line with pcot setconfig Default.multifile_pattern \"your pattern here\" Named matches and how they are used Only one of the following should be true (e.g. you can't use name and n together): lens and n : if these are found, they are joined together to form a filter position which is looked up in the filter set (by the position \"Pos\" field). The idea is that lens indicates either the left or right camera and n identifies a filter. They're separate because many early files used names like LWAC02 or LWideAngle02 , in which the two elements were separate. In many cases you'll want to use the pos option below. pos : if this is found, it us used to match a filter position using the position field - as such, it's a simpler version of the lens / n combination name : if this is found, it is used to match a filter using the filter's name cwl : if this is found, it is used to match a filter using the CWL (wavelength) field If you need assistance, or this isn't flexible enough, contact us - or maybe use the assignfilters node. An alternative: assignfilters You can assign filters in a camera to bands in an image manually using the assignfilters node (in the \"utility\" group). This is fairly self-explanatory: feed in an image, select a camera, and assign the bands using the move up and down buttons. Reading raw binary files Data is often provided \"as is\" from the camera, in a raw binary format. Reading these files requires a little more information in advance: What the numeric format of the data is (e.g. 16-bit unsigned integer) How big the image is in pixels Whether there is a header at the start which should be skipped and how big it is (the \"offset\") Whether the image needs to be rotated and/or flipped Whether the data is \"big-endian\" or \"little-endian.\" These can be set by clicking the \"raw loader settings\" dialog. For images from AUPE, the settings are: 16-bit unsigned integer 1024x1024 48 byte offset Rotate 90 degrees Big-endian data Presets You can save and load these values - and most other settings for multifile input, such as the pattern and filter - using the \"Presets\" button. Presets are currently stored in your home user directory in a file called MFPresets.json . Users can easily copy this file from other users.","title":"Reading images from multiple files"},{"location":"userguide/multifile/#the-multifile-input-method","text":"Documentation for the other methods is forthcoming. The regex aspect of multifile is complicated, so documenting it was a priority. The multifile input method allows multiple monochrome files of \"standard\" types like PNG and BMP to be imported as multispectral images. It can also read raw binary files. Each file is flattened to greyscale in the process (if it is RGB) by finding the mean of the three channels for each pixel.","title":"The Multifile input method"},{"location":"userguide/multifile/#camera-data-files","text":"In order to analyse the images, PCOT needs to be able to find out which filter was used for each image and get information about it. It also needs access to calibration information, for example flat and dark field images, and reflectance data. This data is stored in PARC files - PCOT archives - and is generated by the pcot gencam command. If you want to know to make them yourself, read this documentation . However, PCOT already comes with some camera files. These can be found in the cameras directory inside the PCOT directory. They do not have flat/dark fields because that data is very large, but hopefully you will not need them - that stage of processing will take place upstream in the ROC. Other camera data files can be downloaded from the PCOT Cookbook .","title":"Camera data files"},{"location":"userguide/multifile/#finding-out-about-camera-data-with-lscams","text":"Some of this information is duplicated in the camera data docs You can find out about the camera data installed on your system with pcot lscams (list cameras). Alone, this will list the cameras with their short descriptions. You can get a list of the filters for a camera by using the command pcot lscams -f . Alone, this will list all cameras and their filters, but you can specify a camera: pcot lscams -f TRAINING_GEOLOGY will list all filters for the geology filter wheel on the training model, for example (if you have that camera data file installed). Each filter has a name and a position, as well as other data like centre wavelength, FWHM and so on. Filters may also have reflectance and flat/darkfield data - you can see if this is the case from the pcot lscams -l command: if a camera has flats, you'll see \"Has flats\" in the description; if it has reflectance data, you'll see a list of supported reflectance calibration targets.","title":"Finding out about camera data with lscams"},{"location":"userguide/multifile/#setting-a-camera","text":"Once you know what camera was used, and you have installed a data file for that camera, you can set it in the multifile input using the Camera widget.","title":"Setting a camera"},{"location":"userguide/multifile/#which-image-is-which-filter-setting-a-file-pattern","text":"The multifile input method lets you specify a camera, but PCOT still needs to be able to work out which filter was used to capture each band in the image. This is done by extracting the filter name or position from the filename using a regular expression (or regex ). If you have some experience with regular expressions (or access to someone with this experience), it will help immensely . Regular expressions describe patterns which texts might match. For example, the regex c[a-z]t will match any three-letter string starting with c and ending with t : it's c , followed by any character between a and z , followed by t . Beginner's guide Useful 'playground' to try out expressions The default pattern looks something like this: .*[LR]WAC(?P<pos>[0-9][0-9]).* This means: .* matches any number of any character, so there could be anything at the start of the filename. [LR] means \"either L or R \". WAC means we must then have the sequence of letters WAC (?P<pos>[0-9][0-9]) means we must now match two digits ( [0-9] ). We've put them in brackets and preceded them with ?P<pos> which means we should store the result under the name pos . The final .* means that there can now be any number of any character again - so there could be anything at the end of the filename. The idea is that a filename like /home/jim/files/DogBiscuitLWAC02Fish.jpg will be matched, and will result in 02 being stored as pos , which will then be used to look up the filter. You can set the default pattern from the command line with pcot setconfig Default.multifile_pattern \"your pattern here\"","title":"Which image is which filter? Setting a file pattern"},{"location":"userguide/multifile/#named-matches-and-how-they-are-used","text":"Only one of the following should be true (e.g. you can't use name and n together): lens and n : if these are found, they are joined together to form a filter position which is looked up in the filter set (by the position \"Pos\" field). The idea is that lens indicates either the left or right camera and n identifies a filter. They're separate because many early files used names like LWAC02 or LWideAngle02 , in which the two elements were separate. In many cases you'll want to use the pos option below. pos : if this is found, it us used to match a filter position using the position field - as such, it's a simpler version of the lens / n combination name : if this is found, it is used to match a filter using the filter's name cwl : if this is found, it is used to match a filter using the CWL (wavelength) field If you need assistance, or this isn't flexible enough, contact us - or maybe use the assignfilters node.","title":"Named matches and how they are used"},{"location":"userguide/multifile/#an-alternative-assignfilters","text":"You can assign filters in a camera to bands in an image manually using the assignfilters node (in the \"utility\" group). This is fairly self-explanatory: feed in an image, select a camera, and assign the bands using the move up and down buttons.","title":"An alternative: assignfilters"},{"location":"userguide/multifile/#reading-raw-binary-files","text":"Data is often provided \"as is\" from the camera, in a raw binary format. Reading these files requires a little more information in advance: What the numeric format of the data is (e.g. 16-bit unsigned integer) How big the image is in pixels Whether there is a header at the start which should be skipped and how big it is (the \"offset\") Whether the image needs to be rotated and/or flipped Whether the data is \"big-endian\" or \"little-endian.\" These can be set by clicking the \"raw loader settings\" dialog. For images from AUPE, the settings are: 16-bit unsigned integer 1024x1024 48 byte offset Rotate 90 degrees Big-endian data","title":"Reading raw binary files"},{"location":"userguide/multifile/#presets","text":"You can save and load these values - and most other settings for multifile input, such as the pattern and filter - using the \"Presets\" button. Presets are currently stored in your home user directory in a file called MFPresets.json . Users can easily copy this file from other users.","title":"Presets"},{"location":"userguide/principles/","text":"Operating principles PCOT nodes need to follow a set of rules to ensure that the information they process is handled consistently. This page describes these rules, most of which apply to image data. Source handling rules Each datum handled by PCOT has a \"source set\" describing where that datum ultimately comes from. Sources vary: in images, each band in an input image carries a source datum describing where it comes from. For a PDS4 source it could be a LIDVID, or it could simply be a filename (although ideally it should have some archive indexing data). Because data can be combined in various ways, each datum could have multiple sources. Image data in PCOT have a separate source set for each band. The rules for sources are simple: Every datum has a source set. This may be a null source if the datum is generated internally (consider the output from constant nodes). In the case of an image, this source set is actually a separate source set for each band, but may still be considered as a single source set for some operations (since the union of the band sets is available). If a datum, or band in a multi-band image, is constructed from data from more than once source set, the resulting datum or band consists of the union of the sets. As an example, consider the rather contrived example below. Here, each datum is marked as a black circle below which is a description of the the sources - there's more explanation of this below. image inputs are in yellow scalar inputs are in blue nodes are white rectangles expr nodes (which calculate mathematical expressions) are marked as such with the expression being the main text Figure: An example graph.. Click on image to expand. We have three inputs into the graph: Input 0 has an image with three bands centered on 640nm, 540nm and 440nm. Input 1 has a different image with four bands. Input 2 has a numeric value of 0.2 (perhaps a housekeeping datum of some kind). These data are then combined in various ways: Input 1 is converted to a single greyscale band and multiplied by input 2 (the scalar 0.2) The result of this operation is then added to the 540nm band of input 0. What do the sources look like for each datum? Datum A is an image, and as such it has one source set per band. Each source set consists of a single source, with details of input and filter wavelength. So here, the sources for A could be written as [ {0:640}, {0:540}, {0:440} ] That is, a list of three sets, each of which contains a single source which I've written in the form input:wavelength . Datum B is the extracted 540nm band of input A, so its sources are: [ {0:540} ] Datum C is another multiband image, this time from input 1: [ {1:780}, {1:640}, {1:540}, {1:440} ] Datum D is the greyscale conversion of Datum C above. This creates a single-band image, but that band combines all the bands of datum C. This results in a single source set containing all the bands: [ {1:780, 1:640, 1:540, 1:440} ] Datum E is the only non-image source. I will denote it here as just \"2\" indicating that it is input 2. It is worth noting here that sources may contain a lot of extra data describing exactly where they come from. For example, this input may come from PDS4, in which case at least the LIDVID will be included. But for now this is simply {2} Note that this is not shown as a list because it is not a multiband source. Datum F multiplies each band in datum D by the scalar E. This means that the source for E must be added to the source set for each band. There is only one band at this point because of the greyscale conversion: [ {1:780, 1:640, 1:540, 1:440, 2} ] Finally, we add the single-band images B and F together, resulting in another single-band image. This addition is done bandwise. There is only one band, so the result is: [ {1:780, 1:640, 1:540, 1:440, 2, 0:540} ] ROI rules Images may contain regions of interest. If this is the case, then any operation should only be performed on the region of interest if possible. However, the rest of the image should be passed through unchanged to provide context - it is always possible to use a croproi node before the operation if cropping is required. This rule has the following practical outcomes, in which images are denoted by capitals A, B, \\dots A, B, \\dots and scalars are denoted by lowercase x, y. \\dots x, y. \\dots So: In binary operations on an image and a scalar such as x+A x+A or A+x A+x , the operation is only performed on the region covered by the union of all ROIs on A A . Other parts of A A are left unchanged in the output, which carries the same ROIs. In binary operations on two images A+B A+B etc., there should be only one set of ROIs in operation. This means that either only one image has an ROI, or the sets of ROIs for both images are identical. Outside the ROI, the image pixels are taken from the left-hand side of the operator as shown in the figure below. Figure: Two images A and B, B has an ROI. In the result, the area covered by the ROI is A+B, the rest of the image is passed through from A (which has no ROI). Click on image to expand. Originally the rule was that the intersection of the unions of the ROIs would be processed, while the rest of the output would be from the left-hand side (in the case of image data). That was too complicated, and broke the \"principle of least astonishment.\" If the user has put different ROIs on both sides of their image they have probably made a mistake. However, it's quite possible for the same ROIs to be on both sides of an expression if they were derived from the same image. Quality data All values in PCOT - scalar and image - can have an uncertainty value and data quality (DQ) bits. In imagecubes, this applies to every pixel of every band. The uncertainty values are expressed as standard deviations. Operations need to combine these data in a sensible way. Note that the standard deviation used is the population standard deviation, not the sample standard deviation. This is currently implemented using the Value class. Consists of: image uncertainty map (float per band) image quality/info bits (uint16 per band) Uncertainty Uncertainty values are propagated through calculations in the expr node and elsewhere according to these rules: \\begin{align} \\sigma(a+b), \\quad \\sigma(a-b) &= \\sqrt{\\sigma^2 a + \\sigma^2 b}\\\\ \\sigma(ab) &= |ab| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}\\quad \\text{when all values positive}\\\\ \\sigma(a/b) &= \\left|\\frac{a}{b}\\right| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\frac{\\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}}{b^2}\\quad \\text{when all values positive}\\\\ \\sigma(a^b) &= \\sqrt{ (a^{b-1} b \\sigma a)^2 + (a^b \\log a \\sigma b)^2} \\\\ &= \\sqrt{a^{2b-2} ((a \\sigma b \\log a)^2 + (b \\sigma a)^2)}\\\\ \\sigma (a \\& b) &= \\min (\\sigma a, \\sigma b)\\\\ \\sigma (a | b) &= \\max (\\sigma a, \\sigma b) \\end{align} \\begin{align} \\sigma(a+b), \\quad \\sigma(a-b) &= \\sqrt{\\sigma^2 a + \\sigma^2 b}\\\\ \\sigma(ab) &= |ab| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}\\quad \\text{when all values positive}\\\\ \\sigma(a/b) &= \\left|\\frac{a}{b}\\right| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\frac{\\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}}{b^2}\\quad \\text{when all values positive}\\\\ \\sigma(a^b) &= \\sqrt{ (a^{b-1} b \\sigma a)^2 + (a^b \\log a \\sigma b)^2} \\\\ &= \\sqrt{a^{2b-2} ((a \\sigma b \\log a)^2 + (b \\sigma a)^2)}\\\\ \\sigma (a \\& b) &= \\min (\\sigma a, \\sigma b)\\\\ \\sigma (a | b) &= \\max (\\sigma a, \\sigma b) \\end{align} Remember that generally these rules only apply if the bands are independent. In reality there is always a covariance between them, but we have to ignore that. Not all images contain uncertainty data - it may be that the input image doesn't have this data, or that a function has been performed on the image which does not permit uncertainty data to be carried through (consider a decorrelation stretch, for example). This can be viewed by selecting the nounc (no uncertainty) bit for viewing in the canvas . DQ bits Each scalar, or band value for each pixel, has an associated set of bits which indicate error states, etc. Bits are currently: Bit name Meaning Effect on calculations (see notes below) NODATA There is no data here BAD SAT Pixel is saturated high in this band BAD DIVZERO The data is the result of a division by zero BAD UNDEF The data is the result of an undefined operation BAD COMPLEX The data is a complex number likely to be undefined (or just the real part) BAD ERROR There is an unspecified error in this data BAD ZERO This data is zero NOUNCERTAINTY There is no uncertainty data All bits are propagated into the data generated from the data they are attached to. BAD means that the data in these pixels should not be considered. While calculations will still be done on BAD data, the BAD bits will be propagated. In the case of nodes which generate a scalar from images, such as finding the mean or SD of a set of pixels (or similar operations in the spectrum node) the pixels marked BAD should be ignored. It is possible to set bits based on per-pixel conditions with the dqmod node. For example, convert all uncertainties greater than a given value into errors. It is possible to convert DQ bits into regions of interest using the roidq node, with the region being made up of pixels for which certain bits are absent or present. This can be done looking at all bands or just one. In general, when multiple image bands are combined (either from the same image or from different images) these are OR-ed together. This typically happens in a band-wise fashion because images are combined band-wise. Thus, when two images a a and b b are added, and the bits for channel i i of image a a are B_i(a) B_i(a) , \\[ B_i(a+b) = B_i(a) \\vee B_i(b)\\quad \\text{for all channels } i \\] However, some operations have a more complex flow of information. For example, a decorrelation stretch results in information from all bands being used in each band. In cases like this, the resulting bands are all ORed toether: \\[ B_i(\\text{decorr}(a)) = \\bigvee_i B_i(a) \\] UNIMPLEMENTED BIT OPERATIONS Nodes which perform a convolution operation or similar should propagate the error pixel to all affected pixels, leading to a blob of pixels in the output. I realise This isn't ideal ; another possibility could be to just zero the mask? But then we lose the error data. At the moment I don't believe we have any \"non-local\" behaviour where pixels affect regions of pixels in the output, so the point could be moot. Filter aberration UNIMPLEMENTED The filter wavelengths are only accurate for pixels in the centre of the image, due to the difference in the light path through the filter at different angles of incidence. Therefore: There will be a system in place to calculate the actual filter wavelength for a given pixel and use this in spectral plots (using the centre of the ROI for the spectrum node) A function should be available to generate the filter aberration value in expr - this would allow an \"image\" to be made of the aberration value which could be used in calculations It should be possible to set the ERROR bit for excessive aberration values Canvas information UNIMPLEMENTED The following should be visible in the canvas as optional overlays: Filter aberration as a heat map (default OFF)","title":"Operating principles"},{"location":"userguide/principles/#operating-principles","text":"PCOT nodes need to follow a set of rules to ensure that the information they process is handled consistently. This page describes these rules, most of which apply to image data.","title":"Operating principles"},{"location":"userguide/principles/#source-handling-rules","text":"Each datum handled by PCOT has a \"source set\" describing where that datum ultimately comes from. Sources vary: in images, each band in an input image carries a source datum describing where it comes from. For a PDS4 source it could be a LIDVID, or it could simply be a filename (although ideally it should have some archive indexing data). Because data can be combined in various ways, each datum could have multiple sources. Image data in PCOT have a separate source set for each band. The rules for sources are simple: Every datum has a source set. This may be a null source if the datum is generated internally (consider the output from constant nodes). In the case of an image, this source set is actually a separate source set for each band, but may still be considered as a single source set for some operations (since the union of the band sets is available). If a datum, or band in a multi-band image, is constructed from data from more than once source set, the resulting datum or band consists of the union of the sets. As an example, consider the rather contrived example below. Here, each datum is marked as a black circle below which is a description of the the sources - there's more explanation of this below. image inputs are in yellow scalar inputs are in blue nodes are white rectangles expr nodes (which calculate mathematical expressions) are marked as such with the expression being the main text Figure: An example graph.. Click on image to expand. We have three inputs into the graph: Input 0 has an image with three bands centered on 640nm, 540nm and 440nm. Input 1 has a different image with four bands. Input 2 has a numeric value of 0.2 (perhaps a housekeeping datum of some kind). These data are then combined in various ways: Input 1 is converted to a single greyscale band and multiplied by input 2 (the scalar 0.2) The result of this operation is then added to the 540nm band of input 0. What do the sources look like for each datum? Datum A is an image, and as such it has one source set per band. Each source set consists of a single source, with details of input and filter wavelength. So here, the sources for A could be written as [ {0:640}, {0:540}, {0:440} ] That is, a list of three sets, each of which contains a single source which I've written in the form input:wavelength . Datum B is the extracted 540nm band of input A, so its sources are: [ {0:540} ] Datum C is another multiband image, this time from input 1: [ {1:780}, {1:640}, {1:540}, {1:440} ] Datum D is the greyscale conversion of Datum C above. This creates a single-band image, but that band combines all the bands of datum C. This results in a single source set containing all the bands: [ {1:780, 1:640, 1:540, 1:440} ] Datum E is the only non-image source. I will denote it here as just \"2\" indicating that it is input 2. It is worth noting here that sources may contain a lot of extra data describing exactly where they come from. For example, this input may come from PDS4, in which case at least the LIDVID will be included. But for now this is simply {2} Note that this is not shown as a list because it is not a multiband source. Datum F multiplies each band in datum D by the scalar E. This means that the source for E must be added to the source set for each band. There is only one band at this point because of the greyscale conversion: [ {1:780, 1:640, 1:540, 1:440, 2} ] Finally, we add the single-band images B and F together, resulting in another single-band image. This addition is done bandwise. There is only one band, so the result is: [ {1:780, 1:640, 1:540, 1:440, 2, 0:540} ]","title":"Source handling rules"},{"location":"userguide/principles/#roi-rules","text":"Images may contain regions of interest. If this is the case, then any operation should only be performed on the region of interest if possible. However, the rest of the image should be passed through unchanged to provide context - it is always possible to use a croproi node before the operation if cropping is required. This rule has the following practical outcomes, in which images are denoted by capitals A, B, \\dots A, B, \\dots and scalars are denoted by lowercase x, y. \\dots x, y. \\dots So: In binary operations on an image and a scalar such as x+A x+A or A+x A+x , the operation is only performed on the region covered by the union of all ROIs on A A . Other parts of A A are left unchanged in the output, which carries the same ROIs. In binary operations on two images A+B A+B etc., there should be only one set of ROIs in operation. This means that either only one image has an ROI, or the sets of ROIs for both images are identical. Outside the ROI, the image pixels are taken from the left-hand side of the operator as shown in the figure below. Figure: Two images A and B, B has an ROI. In the result, the area covered by the ROI is A+B, the rest of the image is passed through from A (which has no ROI). Click on image to expand. Originally the rule was that the intersection of the unions of the ROIs would be processed, while the rest of the output would be from the left-hand side (in the case of image data). That was too complicated, and broke the \"principle of least astonishment.\" If the user has put different ROIs on both sides of their image they have probably made a mistake. However, it's quite possible for the same ROIs to be on both sides of an expression if they were derived from the same image.","title":"ROI rules"},{"location":"userguide/principles/#quality-data","text":"All values in PCOT - scalar and image - can have an uncertainty value and data quality (DQ) bits. In imagecubes, this applies to every pixel of every band. The uncertainty values are expressed as standard deviations. Operations need to combine these data in a sensible way. Note that the standard deviation used is the population standard deviation, not the sample standard deviation. This is currently implemented using the Value class. Consists of: image uncertainty map (float per band) image quality/info bits (uint16 per band)","title":"Quality data"},{"location":"userguide/principles/#uncertainty","text":"Uncertainty values are propagated through calculations in the expr node and elsewhere according to these rules: \\begin{align} \\sigma(a+b), \\quad \\sigma(a-b) &= \\sqrt{\\sigma^2 a + \\sigma^2 b}\\\\ \\sigma(ab) &= |ab| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}\\quad \\text{when all values positive}\\\\ \\sigma(a/b) &= \\left|\\frac{a}{b}\\right| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\frac{\\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}}{b^2}\\quad \\text{when all values positive}\\\\ \\sigma(a^b) &= \\sqrt{ (a^{b-1} b \\sigma a)^2 + (a^b \\log a \\sigma b)^2} \\\\ &= \\sqrt{a^{2b-2} ((a \\sigma b \\log a)^2 + (b \\sigma a)^2)}\\\\ \\sigma (a \\& b) &= \\min (\\sigma a, \\sigma b)\\\\ \\sigma (a | b) &= \\max (\\sigma a, \\sigma b) \\end{align} \\begin{align} \\sigma(a+b), \\quad \\sigma(a-b) &= \\sqrt{\\sigma^2 a + \\sigma^2 b}\\\\ \\sigma(ab) &= |ab| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}\\quad \\text{when all values positive}\\\\ \\sigma(a/b) &= \\left|\\frac{a}{b}\\right| \\sqrt{\\left(\\frac{\\sigma a}{a}\\right)^2+\\left(\\frac{\\sigma b}{b}\\right)^2} \\\\ &= \\frac{\\sqrt{a^2 \\sigma^2 b + b^2 \\sigma^2 a}}{b^2}\\quad \\text{when all values positive}\\\\ \\sigma(a^b) &= \\sqrt{ (a^{b-1} b \\sigma a)^2 + (a^b \\log a \\sigma b)^2} \\\\ &= \\sqrt{a^{2b-2} ((a \\sigma b \\log a)^2 + (b \\sigma a)^2)}\\\\ \\sigma (a \\& b) &= \\min (\\sigma a, \\sigma b)\\\\ \\sigma (a | b) &= \\max (\\sigma a, \\sigma b) \\end{align} Remember that generally these rules only apply if the bands are independent. In reality there is always a covariance between them, but we have to ignore that. Not all images contain uncertainty data - it may be that the input image doesn't have this data, or that a function has been performed on the image which does not permit uncertainty data to be carried through (consider a decorrelation stretch, for example). This can be viewed by selecting the nounc (no uncertainty) bit for viewing in the canvas .","title":"Uncertainty"},{"location":"userguide/principles/#dq-bits","text":"Each scalar, or band value for each pixel, has an associated set of bits which indicate error states, etc. Bits are currently: Bit name Meaning Effect on calculations (see notes below) NODATA There is no data here BAD SAT Pixel is saturated high in this band BAD DIVZERO The data is the result of a division by zero BAD UNDEF The data is the result of an undefined operation BAD COMPLEX The data is a complex number likely to be undefined (or just the real part) BAD ERROR There is an unspecified error in this data BAD ZERO This data is zero NOUNCERTAINTY There is no uncertainty data All bits are propagated into the data generated from the data they are attached to. BAD means that the data in these pixels should not be considered. While calculations will still be done on BAD data, the BAD bits will be propagated. In the case of nodes which generate a scalar from images, such as finding the mean or SD of a set of pixels (or similar operations in the spectrum node) the pixels marked BAD should be ignored. It is possible to set bits based on per-pixel conditions with the dqmod node. For example, convert all uncertainties greater than a given value into errors. It is possible to convert DQ bits into regions of interest using the roidq node, with the region being made up of pixels for which certain bits are absent or present. This can be done looking at all bands or just one. In general, when multiple image bands are combined (either from the same image or from different images) these are OR-ed together. This typically happens in a band-wise fashion because images are combined band-wise. Thus, when two images a a and b b are added, and the bits for channel i i of image a a are B_i(a) B_i(a) , \\[ B_i(a+b) = B_i(a) \\vee B_i(b)\\quad \\text{for all channels } i \\] However, some operations have a more complex flow of information. For example, a decorrelation stretch results in information from all bands being used in each band. In cases like this, the resulting bands are all ORed toether: \\[ B_i(\\text{decorr}(a)) = \\bigvee_i B_i(a) \\]","title":"DQ bits"},{"location":"userguide/principles/#unimplemented-bit-operations","text":"Nodes which perform a convolution operation or similar should propagate the error pixel to all affected pixels, leading to a blob of pixels in the output. I realise This isn't ideal ; another possibility could be to just zero the mask? But then we lose the error data. At the moment I don't believe we have any \"non-local\" behaviour where pixels affect regions of pixels in the output, so the point could be moot.","title":"UNIMPLEMENTED BIT OPERATIONS"},{"location":"userguide/principles/#filter-aberration-unimplemented","text":"The filter wavelengths are only accurate for pixels in the centre of the image, due to the difference in the light path through the filter at different angles of incidence. Therefore: There will be a system in place to calculate the actual filter wavelength for a given pixel and use this in spectral plots (using the centre of the ROI for the spectrum node) A function should be available to generate the filter aberration value in expr - this would allow an \"image\" to be made of the aberration value which could be used in calculations It should be possible to set the ERROR bit for excessive aberration values","title":"Filter aberration UNIMPLEMENTED"},{"location":"userguide/principles/#canvas-information-unimplemented","text":"The following should be visible in the canvas as optional overlays: Filter aberration as a heat map (default OFF)","title":"Canvas information UNIMPLEMENTED"},{"location":"userguide/uncs/","text":"Uncertainties \\begin{align} \\sigma(a+b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(a-b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(ab) &= \\sqrt{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}\\\\ \\sigma(a/b) &= \\sqrt{\\frac{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}{b^4}}\\\\ \\sigma(a^b) &= \\sqrt{a^{2b-2} (a^2 \\sigma_b^2 (\\ln{a})^2 + b^2 \\sigma_a^2)} \\end{align} \\begin{align} \\sigma(a+b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(a-b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(ab) &= \\sqrt{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}\\\\ \\sigma(a/b) &= \\sqrt{\\frac{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}{b^4}}\\\\ \\sigma(a^b) &= \\sqrt{a^{2b-2} (a^2 \\sigma_b^2 (\\ln{a})^2 + b^2 \\sigma_a^2)} \\end{align}","title":"Uncertainties"},{"location":"userguide/uncs/#uncertainties","text":"\\begin{align} \\sigma(a+b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(a-b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(ab) &= \\sqrt{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}\\\\ \\sigma(a/b) &= \\sqrt{\\frac{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}{b^4}}\\\\ \\sigma(a^b) &= \\sqrt{a^{2b-2} (a^2 \\sigma_b^2 (\\ln{a})^2 + b^2 \\sigma_a^2)} \\end{align} \\begin{align} \\sigma(a+b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(a-b) &= \\sqrt{\\sigma^2_a + \\sigma^2_b}\\\\ \\sigma(ab) &= \\sqrt{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}\\\\ \\sigma(a/b) &= \\sqrt{\\frac{a^2 \\sigma_b^2 + b^2 \\sigma_a^2}{b^4}}\\\\ \\sigma(a^b) &= \\sqrt{a^{2b-2} (a^2 \\sigma_b^2 (\\ln{a})^2 + b^2 \\sigma_a^2)} \\end{align}","title":"Uncertainties"},{"location":"userguide/update/","text":"Updating PCOT Updating an existing PCOT install should be fairly simple. Go into the PCOT root directory - this is the directory which contains PCOT on your machine, and is usually called PCOT . It should contain directories called src and tests . Type git pull - this should get the latest release Type poetry install - this should check for any updated packages and download them","title":"Updating PCOT"},{"location":"userguide/update/#updating-pcot","text":"Updating an existing PCOT install should be fairly simple. Go into the PCOT root directory - this is the directory which contains PCOT on your machine, and is usually called PCOT . It should contain directories called src and tests . Type git pull - this should get the latest release Type poetry install - this should check for any updated packages and download them","title":"Updating PCOT"},{"location":"userguide/batch/","text":"Batch mode Batch files - sometimes called parameter files - let you create a PCOT graph and then run it several times from the command line, modifying its inputs, outputs or nodes each time. For example, imagine that we have a graph like this to generate a false colour spectral parameter map across an entire image: The expr node is set to the parameter we want to generate, which in this case is a$670/(a$440+0.1) and we've renamed it back to expr for reasons which will become apparent ( expr nodes usually change their name to their expression). The gradient node is set to the defaults, but with the legend in the left margin rather than inside the image. When we run this on some data (assuming it has the two wavelengths we need), and right-click on the image inside the gradient node, we can save as a PDF. That PDF will look something like this: Figure: Example spectral parameter map. Click on image to expand. This is great, but we might want to run this on multiple images. We can do that using batch files. For example, this file will use the same settings for all the nodes and inputs, but replace the input filenames with those specified (we're using a multifile input here): inputs.0.multifile.directory = H:\\PancamData\\SamplesGeologyFilter inputs.0.multifile.filenames.+ = R01.bin .+ = R02.bin .+ = R03.bin .+ = R04.bin .+ = R05.bin .+ = R06.bin outputs.+.node=gradient .file=gradient.pdf .annotations=y .clobber=y The inputs will keep their settings (e.g. raw loader parameters and filter) but new files will be used. Running the graph with a batch file We can then run the graph with a batch file using the pcot batch subcommand: pcot batch mygraph.pcot mybatchfile.batch ... Any extra arguments will be set inside the Jinja2 templating engine used by the batch runner as var[0] , var[1] etc. If we want to do this several times, we can either write multiple batch files, or we can run the graph several times in one file, changing it each time. Here's an example that runs the graph twice: inputs.0.multifile.directory = H:\\PancamData\\SamplesGeologyFilter inputs.0.multifile.filenames.+ = R01.bin .+ = R02.bin .+ = R03.bin .+ = R04.bin .+ = R05.bin .+ = R06.bin outputs.+.node=gradient .file=gradient1.pdf .annotations=y .clobber=y run inputs.0.multifile.directory = H:\\PancamData\\Data2 expr.expr = a$440 gradient.preset = magma reset inputs.0.multifile.filenames inputs.0.multifile.filenames.+ = *1712*Training Model-R01*.bin inputs.0.multifile.filenames.+ = *1713*Training Model-R05*.bin outputs.0.file = gradient2.pdf run Here, we are running the graph once as above and then making some changes: changing the input directory changing the input to read different filenames, this time loaded using \"wildcards\" changing the expr node's expression to show just the 440nm channel changing the gradient's appearance with a preset changing the output to write to a different filename Write more on how the inputs work - intro here, more in params.md or elsewhere. Write more in general. Note autodocs . Finish. Parameter file format","title":"Batch mode"},{"location":"userguide/batch/#batch-mode","text":"Batch files - sometimes called parameter files - let you create a PCOT graph and then run it several times from the command line, modifying its inputs, outputs or nodes each time. For example, imagine that we have a graph like this to generate a false colour spectral parameter map across an entire image: The expr node is set to the parameter we want to generate, which in this case is a$670/(a$440+0.1) and we've renamed it back to expr for reasons which will become apparent ( expr nodes usually change their name to their expression). The gradient node is set to the defaults, but with the legend in the left margin rather than inside the image. When we run this on some data (assuming it has the two wavelengths we need), and right-click on the image inside the gradient node, we can save as a PDF. That PDF will look something like this: Figure: Example spectral parameter map. Click on image to expand. This is great, but we might want to run this on multiple images. We can do that using batch files. For example, this file will use the same settings for all the nodes and inputs, but replace the input filenames with those specified (we're using a multifile input here): inputs.0.multifile.directory = H:\\PancamData\\SamplesGeologyFilter inputs.0.multifile.filenames.+ = R01.bin .+ = R02.bin .+ = R03.bin .+ = R04.bin .+ = R05.bin .+ = R06.bin outputs.+.node=gradient .file=gradient.pdf .annotations=y .clobber=y The inputs will keep their settings (e.g. raw loader parameters and filter) but new files will be used.","title":"Batch mode"},{"location":"userguide/batch/#running-the-graph-with-a-batch-file","text":"We can then run the graph with a batch file using the pcot batch subcommand: pcot batch mygraph.pcot mybatchfile.batch ... Any extra arguments will be set inside the Jinja2 templating engine used by the batch runner as var[0] , var[1] etc. If we want to do this several times, we can either write multiple batch files, or we can run the graph several times in one file, changing it each time. Here's an example that runs the graph twice: inputs.0.multifile.directory = H:\\PancamData\\SamplesGeologyFilter inputs.0.multifile.filenames.+ = R01.bin .+ = R02.bin .+ = R03.bin .+ = R04.bin .+ = R05.bin .+ = R06.bin outputs.+.node=gradient .file=gradient1.pdf .annotations=y .clobber=y run inputs.0.multifile.directory = H:\\PancamData\\Data2 expr.expr = a$440 gradient.preset = magma reset inputs.0.multifile.filenames inputs.0.multifile.filenames.+ = *1712*Training Model-R01*.bin inputs.0.multifile.filenames.+ = *1713*Training Model-R05*.bin outputs.0.file = gradient2.pdf run Here, we are running the graph once as above and then making some changes: changing the input directory changing the input to read different filenames, this time loaded using \"wildcards\" changing the expr node's expression to show just the 440nm channel changing the gradient's appearance with a preset changing the output to write to a different filename Write more on how the inputs work - intro here, more in params.md or elsewhere. Write more in general. Note autodocs . Finish. Parameter file format","title":"Running the graph with a batch file"},{"location":"userguide/batch/params/","text":"Parameter file format The basics Parameter files are just lists of changes to be made to a structure. This structure contains information about the document's inputs and nodes, and how the system should output the resulting data once the document has run. Each line in the file changes an element in the structure. For example, inputs.0.rgb.file = mydata/foo.png tells the system to change input 0's RGB loading method that it should load the file mydata/foo.png . Only one loader can be active for each input, and this change will \"activate\" input 0's RGB loader. Another change could be: k.val = 2 which assumes that there is a node called k , whose parameter val we now set to 2 (this is probably a constant node). Lists Some of the elements in the parameter structure file are lists. We can add to lists by putting a + in the path. For example, there is a list of filenames to load inside each input's multifile loader. We can add elements to it like this: inputs.0.multifile.directory = mydata/multi inputs.0.multifile.filter_pattern = *Filter(?P<lens>L|R)(?P<n>[0-9][0-9]).* inputs.0.multifile.filenames.+ = FilterL02.png inputs.0.multifile.filenames.+ = TestFilterL01image.png inputs.0.multifile.filenames.+ = FilterR10.png This tells the multifile loader where the files are (the directory element), how to decode the names to get a filter (the filter_pattern element) and finally adds three filenames to the filename list. What the + element actually does is create a new item at the end of the list and start modifying that item. In the example above each item is just a string, but they could be structures. Adding a new structure to a list creates a default structure and adds it, and we can then immediately modify it. For example, we can add a new output like this: outputs.+.file = out.csv outputs.0.clobber = y outputs.0.node = spectrum The first line creates a new default output element in the outputs and sets its file to out.csv . The next two lines modify other values within that same output, which is the first output (numbered zero). We can write this more concisely using a relative path. Relative paths The left-hand side of each line is called the \"path\" because it describes how to get to each item we need to change. You'll notice from the above example that there can be a lot of repetition in paths as we change a lot of elements at the same level within the structure. So far we have seen absolute paths describing route in full, but if we start a path with a . - making a relative path - we are assumed to be at the same place as the last change. We can change the previous output example to this: outputs.+.file = out.csv .clobber = y .node = spectrum The first line creates a new output element and sets its file to \"out.csv\". The next two lines modify that output element, because the last thing we set was output.0.file , inside output.0 . We can also modify the input example the same way: inputs.0.multifile.directory = {globaldatadir}/multi .filter_pattern = *Filter(?P<lens>L|R)(?P<n>[0-9][0-9]).* .filenames.+ = FilterL02.png .+ = TestFilterL01image.png .+ = FilterR10.png The leading . means \"stay at the same level,\" which in this case is inputs.0.multifile . Path setters If we write a path without setting a value, it just sets the current path for the next items - this kind of line is called a path setter . We could also write the output example like this: outputs.+ .node = spectrum .file = out.csv .clobber = y The first line creates a new output and sets the path. The next three lines modify the structure at that path - the output just created. Going up levels with multiple dots In the example above we add an output and then modify it. We could add another output by adding an extra set of lines, like this: outputs.+.file = out.csv .clobber = y .node = spectrum outputs.+.file = out2.csv .node = spectrum2 But we can also do this: outputs.+.file = out.csv .clobber = y .node = spectrum ..+.file = out2.csv .node = spectrum2 That's because after the first output, our path is set to output.0. . Using two dots lets us go back up the path one level, to output. , so we can now add a new output. List notation for ordered parameters Some parameters have an implicit ordering. These are marked in the documentation as \"(ordered)\". For example, the circle node type autodocs contain this: croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius That means that while you can set the region of interest within a circle node with this: circle.croi.x = 100 .y = 100 .r = 20 you can also use a list, and the values will be assigned from the list with the order given in the documentation: circle.croi = [100,100,20] The 'run' directive - running multiple times If the file contains a line with just the word run (excluding comments), PCOT will run with the parameters as they stand at the current point in the file. That means you can write things like this: outputs.+.file = double.csv expr.expr = a*2 run outputs.0.file = triple.csv expr.expr = a*3 run outputs.0.file = quad.csv expr.expr = a*4 run Here we are running the same graph three times, with three different expressions in the node called expr . Each time we are modifying the first (and only) output filename. Note that we need to create output for the first run, and then we modify its filename for subsequent runs. Printing messages The print directive allows messages to be printed during the run: outputs.+.file = double.csv print doubling some data... expr.expr = a*2 run outputs.0.file = triple.csv print trebling some data... expr.expr = a*3 run This is particularly useful when using templates, as we will see below. Using Jinja2 templates Before they are run, each parameter file is processed using the Jinja2 templating engine , more typically used to create websites (as is obvious from its documentation). Here is an example of a file written to make use of this facility. It performs some manipulations on an image, writing the results to a single PARC (PCOT archive) file. It will generate four images. outputs.+.file = output.parc .annotations = n # annotations not supported in PARC .node = striproi(a,1) .name = main # the name the output will have in the PARC file .description = Primary output image # description in the PARC run # now we run a loop for values 1,2,3 using Jinja2 {% for i in range(1,4) %} # just one output, keep the same file and settings but append. outputs.0.append = y # we are now appending to the PARC .node = test{{i}} # get output from node1, node2 or node3. .name = testimg{{i}} # call it testimg1, testimg2 or testimg3 .description = Test image {{i}} print processing {{i}} # print a message run {% endfor %} # normally a \"run\" is silently appended to the end of a parameter file, # but this won't happen if the previous command was also a \"run\", which # it will be here. PCOT automatically sets up the following Jinja2 variables for you to use: variable name description {{docpath}} the path to the document (with backslashes replaced by forward slashes) {{docfile}} the name of the document file (i.e. the final part of the path) {{datetime}} the current date and time in ISO 8601 format {{date}} the current date in ISO 8601 format {{count}} the number of times the document has been run (useful in loops) {{vars}} an array of any extra arguments passed on the command line {{parampath}} the path to the parameter file (if one is used, it is \"NoFile\" otherwise) {{paramfile}} the name of the parameter file (if one is used, it is \"NoFile\" otherwise) We also set up some Jinja2 \"filters\" to manipulate filenames, etc.: filter action basename return the last part of a file path: \"foo/bar.png\" becomes \"bar.png\" dirname return the directory part of a file path: \"foo/bar.png\" becomes \"foo\" stripext remove extension: \"foo.bar\" becomes \"foo\" extension get extension: \"foo.bar\" becomes \".bar\" This lets us do things like this, which will add the parameter file name as a prefix to any text output, but strip off any directory elements: output.0.prefix = {{paramfile | basename}} Using outputs (and other lists) in a loop A common use case is writing multiple output files. You may be tempted to do something like this: {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.0.node = sink # set the output .clobber = y # can overwrite files .filename = output{{i}}.png # and change the filename {{ endfor }} That won't work, because output 0 doesn't exist yet. You may try to fix it like this: {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.+.node = sink # create new output! .clobber = y # can overwrite files .filename = output{{i}}.png # and change the filename {{ endfor }} but while that will run it will create a new output each time it goes around the loop! The right way to do it is to create the output and set it up before the loop, only changing the bits you need to: outputs.+.node = sink # create new output! .clobber = y # can overwrite files {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.0.filename = output{{i}}.png # and change the output filename {{ endfor }} Not yet written below here Inputs, outputs and nodes How inputs work is complicated. There's a structure that you fill in that modifies the inputs as they are in the document.","title":"Parameter file format"},{"location":"userguide/batch/params/#parameter-file-format","text":"","title":"Parameter file format"},{"location":"userguide/batch/params/#the-basics","text":"Parameter files are just lists of changes to be made to a structure. This structure contains information about the document's inputs and nodes, and how the system should output the resulting data once the document has run. Each line in the file changes an element in the structure. For example, inputs.0.rgb.file = mydata/foo.png tells the system to change input 0's RGB loading method that it should load the file mydata/foo.png . Only one loader can be active for each input, and this change will \"activate\" input 0's RGB loader. Another change could be: k.val = 2 which assumes that there is a node called k , whose parameter val we now set to 2 (this is probably a constant node).","title":"The basics"},{"location":"userguide/batch/params/#lists","text":"Some of the elements in the parameter structure file are lists. We can add to lists by putting a + in the path. For example, there is a list of filenames to load inside each input's multifile loader. We can add elements to it like this: inputs.0.multifile.directory = mydata/multi inputs.0.multifile.filter_pattern = *Filter(?P<lens>L|R)(?P<n>[0-9][0-9]).* inputs.0.multifile.filenames.+ = FilterL02.png inputs.0.multifile.filenames.+ = TestFilterL01image.png inputs.0.multifile.filenames.+ = FilterR10.png This tells the multifile loader where the files are (the directory element), how to decode the names to get a filter (the filter_pattern element) and finally adds three filenames to the filename list. What the + element actually does is create a new item at the end of the list and start modifying that item. In the example above each item is just a string, but they could be structures. Adding a new structure to a list creates a default structure and adds it, and we can then immediately modify it. For example, we can add a new output like this: outputs.+.file = out.csv outputs.0.clobber = y outputs.0.node = spectrum The first line creates a new default output element in the outputs and sets its file to out.csv . The next two lines modify other values within that same output, which is the first output (numbered zero). We can write this more concisely using a relative path.","title":"Lists"},{"location":"userguide/batch/params/#relative-paths","text":"The left-hand side of each line is called the \"path\" because it describes how to get to each item we need to change. You'll notice from the above example that there can be a lot of repetition in paths as we change a lot of elements at the same level within the structure. So far we have seen absolute paths describing route in full, but if we start a path with a . - making a relative path - we are assumed to be at the same place as the last change. We can change the previous output example to this: outputs.+.file = out.csv .clobber = y .node = spectrum The first line creates a new output element and sets its file to \"out.csv\". The next two lines modify that output element, because the last thing we set was output.0.file , inside output.0 . We can also modify the input example the same way: inputs.0.multifile.directory = {globaldatadir}/multi .filter_pattern = *Filter(?P<lens>L|R)(?P<n>[0-9][0-9]).* .filenames.+ = FilterL02.png .+ = TestFilterL01image.png .+ = FilterR10.png The leading . means \"stay at the same level,\" which in this case is inputs.0.multifile .","title":"Relative paths"},{"location":"userguide/batch/params/#path-setters","text":"If we write a path without setting a value, it just sets the current path for the next items - this kind of line is called a path setter . We could also write the output example like this: outputs.+ .node = spectrum .file = out.csv .clobber = y The first line creates a new output and sets the path. The next three lines modify the structure at that path - the output just created.","title":"Path setters"},{"location":"userguide/batch/params/#going-up-levels-with-multiple-dots","text":"In the example above we add an output and then modify it. We could add another output by adding an extra set of lines, like this: outputs.+.file = out.csv .clobber = y .node = spectrum outputs.+.file = out2.csv .node = spectrum2 But we can also do this: outputs.+.file = out.csv .clobber = y .node = spectrum ..+.file = out2.csv .node = spectrum2 That's because after the first output, our path is set to output.0. . Using two dots lets us go back up the path one level, to output. , so we can now add a new output.","title":"Going up levels with multiple dots"},{"location":"userguide/batch/params/#list-notation-for-ordered-parameters","text":"Some parameters have an implicit ordering. These are marked in the documentation as \"(ordered)\". For example, the circle node type autodocs contain this: croi circle definition (ordered) x: integer (default 0) Centre x coordinate y: integer (default 0) Centre y coordinate r: integer (default -1) Radius That means that while you can set the region of interest within a circle node with this: circle.croi.x = 100 .y = 100 .r = 20 you can also use a list, and the values will be assigned from the list with the order given in the documentation: circle.croi = [100,100,20]","title":"List notation for ordered parameters"},{"location":"userguide/batch/params/#the-run-directive-running-multiple-times","text":"If the file contains a line with just the word run (excluding comments), PCOT will run with the parameters as they stand at the current point in the file. That means you can write things like this: outputs.+.file = double.csv expr.expr = a*2 run outputs.0.file = triple.csv expr.expr = a*3 run outputs.0.file = quad.csv expr.expr = a*4 run Here we are running the same graph three times, with three different expressions in the node called expr . Each time we are modifying the first (and only) output filename. Note that we need to create output for the first run, and then we modify its filename for subsequent runs.","title":"The 'run' directive - running multiple times"},{"location":"userguide/batch/params/#printing-messages","text":"The print directive allows messages to be printed during the run: outputs.+.file = double.csv print doubling some data... expr.expr = a*2 run outputs.0.file = triple.csv print trebling some data... expr.expr = a*3 run This is particularly useful when using templates, as we will see below.","title":"Printing messages"},{"location":"userguide/batch/params/#using-jinja2-templates","text":"Before they are run, each parameter file is processed using the Jinja2 templating engine , more typically used to create websites (as is obvious from its documentation). Here is an example of a file written to make use of this facility. It performs some manipulations on an image, writing the results to a single PARC (PCOT archive) file. It will generate four images. outputs.+.file = output.parc .annotations = n # annotations not supported in PARC .node = striproi(a,1) .name = main # the name the output will have in the PARC file .description = Primary output image # description in the PARC run # now we run a loop for values 1,2,3 using Jinja2 {% for i in range(1,4) %} # just one output, keep the same file and settings but append. outputs.0.append = y # we are now appending to the PARC .node = test{{i}} # get output from node1, node2 or node3. .name = testimg{{i}} # call it testimg1, testimg2 or testimg3 .description = Test image {{i}} print processing {{i}} # print a message run {% endfor %} # normally a \"run\" is silently appended to the end of a parameter file, # but this won't happen if the previous command was also a \"run\", which # it will be here. PCOT automatically sets up the following Jinja2 variables for you to use: variable name description {{docpath}} the path to the document (with backslashes replaced by forward slashes) {{docfile}} the name of the document file (i.e. the final part of the path) {{datetime}} the current date and time in ISO 8601 format {{date}} the current date in ISO 8601 format {{count}} the number of times the document has been run (useful in loops) {{vars}} an array of any extra arguments passed on the command line {{parampath}} the path to the parameter file (if one is used, it is \"NoFile\" otherwise) {{paramfile}} the name of the parameter file (if one is used, it is \"NoFile\" otherwise) We also set up some Jinja2 \"filters\" to manipulate filenames, etc.: filter action basename return the last part of a file path: \"foo/bar.png\" becomes \"bar.png\" dirname return the directory part of a file path: \"foo/bar.png\" becomes \"foo\" stripext remove extension: \"foo.bar\" becomes \"foo\" extension get extension: \"foo.bar\" becomes \".bar\" This lets us do things like this, which will add the parameter file name as a prefix to any text output, but strip off any directory elements: output.0.prefix = {{paramfile | basename}}","title":"Using Jinja2 templates"},{"location":"userguide/batch/params/#using-outputs-and-other-lists-in-a-loop","text":"A common use case is writing multiple output files. You may be tempted to do something like this: {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.0.node = sink # set the output .clobber = y # can overwrite files .filename = output{{i}}.png # and change the filename {{ endfor }} That won't work, because output 0 doesn't exist yet. You may try to fix it like this: {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.+.node = sink # create new output! .clobber = y # can overwrite files .filename = output{{i}}.png # and change the filename {{ endfor }} but while that will run it will create a new output each time it goes around the loop! The right way to do it is to create the output and set it up before the loop, only changing the bits you need to: outputs.+.node = sink # create new output! .clobber = y # can overwrite files {{ for i in range(0,10) }} # loop over 10 items inputs.0.envi.filename = input{{i}}.hdr # change input 0 outputs.0.filename = output{{i}}.png # and change the output filename {{ endfor }} Not yet written below here","title":"Using outputs (and other lists) in a loop"},{"location":"userguide/batch/params/#inputs-outputs-and-nodes","text":"How inputs work is complicated. There's a structure that you fill in that modifies the inputs as they are in the document.","title":"Inputs, outputs and nodes"}]}